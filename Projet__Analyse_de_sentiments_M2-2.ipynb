{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9wJn51x55zx"
      },
      "source": [
        "# Projet : Analyse de sentiments dans les critiques de films"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fVFa-lN55zy"
      },
      "source": [
        "## Objectifs\n",
        "\n",
        "1. Implémenter une manière simple de représenter des données textuelles - Bag of words\n",
        "2. Implémenter un modèle d'apprentissage statistique basique - Bayésien Naïf\n",
        "3. Utiliser ces représentations et ce modèle pour une tâche d'analyse de sentiments\n",
        "4. Implémenter différentes manières d'obtenir des représentations denses des mêmes données\n",
        "5. Utiliser un modèle de régression logistique pour entraîner un classifieur sur ces nouvelles représentations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx6Bsetd55zz"
      },
      "source": [
        "## Dépendances nécessaires\n",
        "\n",
        "Pour commencer, on aura besoin des packages suivants:\n",
        "- The Machine Learning API Scikit-learn : http://scikit-learn.org/stable/install.html\n",
        "- The Natural Language Toolkit : http://www.nltk.org/install.html\n",
        "\n",
        "Les deux sont disponibles avec Anaconda: https://anaconda.org/anaconda/nltk et https://anaconda.org/anaconda/scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "socvB8me55zz"
      },
      "outputs": [],
      "source": [
        "import os.path as op\n",
        "import re \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import operator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yOFBZlT55z0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if not sys.warnoptions:\n",
        "    import warnings\n",
        "    warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opDOi1J455z0",
        "outputId": "af8dcdc6-9363-417b-943e-52a5a9815a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1pbz4Ip55z1",
        "outputId": "591c91bf-41a9-4497-ac47-14479281227d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "pip install --user -U nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "fZnU3Q4T55z1",
        "outputId": "236505d9-2a47-4aee-f588-3e9ef7603ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.10 are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install --user -U numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTwvcvQyjF7y",
        "outputId": "8b223ae7-6e68-4259-9280-0f27253ad22e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oan7I65-55z1"
      },
      "source": [
        "## Charger les données\n",
        "\n",
        "On récupère les données textuelles dans la variable *texts*\n",
        "\n",
        "On récupère les labels dans la variable $y$ qui en contient *len(texts)* : $0$ indique que la critique correspondante est négative tandis que $1$ qu'elle est positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGLLogYqTGSa",
        "outputId": "5e7c4d03-846a-4b8e-bbdf-1fec7cd0baae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "25120 documents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-df6e47ad8551>:27: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = np.ones(len(texts), dtype=np.int)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import os.path as op\n",
        "from glob import glob\n",
        "\n",
        "# Path to the folder containing the reviews\n",
        "folder_path = op.join('/content', 'gdrive', 'My Drive', 'data', 'imdb1')\n",
        "\n",
        "# Function to read a file given its path\n",
        "def read_file(file_path):\n",
        "    with open(file_path, encoding=\"utf8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "# Get the files from the path: ./data/imdb1/neg for negative reviews, and ./data/imdb1/pos for positive reviews\n",
        "filenames_neg = sorted(glob(op.join(folder_path, 'neg', '*.txt')))\n",
        "filenames_pos = sorted(glob(op.join(folder_path, 'pos', '*.txt')))\n",
        "\n",
        "# Read the reviews using the function and put them in two lists, that we concatenate\n",
        "texts_neg = list(map(read_file, filenames_neg))\n",
        "texts_pos = list(map(read_file, filenames_pos))\n",
        "texts = texts_neg + texts_pos\n",
        "\n",
        "# The first half of the elements of the list are string of negative reviews, and the second half positive ones\n",
        "# We create the labels, as an array of [1,len(texts)], filled with 1, and change the first half to 0\n",
        "import numpy as np\n",
        "y = np.ones(len(texts), dtype=np.int)\n",
        "y[:len(texts_neg)] = 0.\n",
        "\n",
        "print(\"%d documents\" % len(texts))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnj2tTv255z2",
        "outputId": "fd2bce26-9808-4da4-d97e-8ad2bfd67584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de documents: 2512\n"
          ]
        }
      ],
      "source": [
        "# This number of documents may be high for most computers: we can select a fraction of them (here, one in k)\n",
        "# Use an even number to keep the same number of positive and negative reviews\n",
        "k = 10\n",
        "texts_reduced = texts[0::k]\n",
        "y_reduced = y [0::k]\n",
        "\n",
        "print('Nombre de documents:', len(texts_reduced))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNpn2gK755z2"
      },
      "source": [
        "# Bayésien Naïf \n",
        "\n",
        "## Idée principale\n",
        "\n",
        "On dispose d'une critique étant en fait une liste de mots $s = (w_1, ..., w_N)$, et l'on cherche à trouver la classe associée $c$ - qui dans notre cas, peut-être $c = 0$ ou $c = 1$. L'objectif est donc de trouver pour chaque critique $s$ la classe $\\hat{c}$ maximisant la probabilité conditionelle **$P(c|s)$** : \n",
        "\n",
        "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\, P(c|s) = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)}$$\n",
        "\n",
        "**Hypothèse : P(s) est constante pour chaque classe** :\n",
        "\n",
        "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)} = \\underset{c}{\\mathrm{argmax}}\\,P(s|c)P(c)$$\n",
        "\n",
        "**Hypothèse naïve : les différentes variables (mots) d'une critique sont indépendantes entre elles** : \n",
        "\n",
        "$$P(s|c) = P(w_1, ..., w_N|c)=\\Pi_{i=1..N}P(w_i|c)$$\n",
        "\n",
        "On va donc pouvoir se servir des critiques annotées à notre disposition pour **estimer les probabilités $P(w|c)$ pour chaque mot $w$ étant donné les deux classes $c$**. Ces critiques vont nous permettre d'apprendre à évaluer la \"compatibilité\" entre les mots et classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyJYNb4C55z3"
      },
      "source": [
        "## Vue générale\n",
        "\n",
        "### Entraînement: Estimer les probabilités\n",
        "\n",
        "Pour chaque mot $w$ du vocabulaire $V$, $P(w|c)$ est le nombre d'occurences de $w$ dans une critique ayant pour classe $c$, divisé par le nombre total d'occurences dans $c$. Si on note $T(w,c)$ ce nombre d'occurences, on obtient:\n",
        "\n",
        "$$P(w|c) = \\text{Fréquence de }w\\text{ dans }c = \\frac{T(w,c)}{\\sum_{w' \\in V} T(w',c)}$$\n",
        "\n",
        "### Test: Calcul des scores\n",
        "\n",
        "Pour faciliter les calculs et éviter les erreurs d'*underflow* et d'approximation, on utilise le \"log-sum trick\", et on passe l'équation en log-probabilités : \n",
        "\n",
        "$$\\hat{c} =  \\underset{c}{\\mathrm{argmax}}\\, P(c|s) = \\underset{c}{\\mathrm{argmax}}\\, \\left[ \\mathrm{log}(P(c)) + \\sum_{i=1..N}log(P(w_i|c)) \\right]$$\n",
        "\n",
        "### Laplace smoothing (Lissage)\n",
        "\n",
        "Un mot qui n'apparaît pas dans un document a une probabilité nulle: cela va poser problème avec le logarithme. On garde donc une toute petite partie de la masse de probabilité qu'on redistribue avec le *Laplace smoothing*: \n",
        "\n",
        "$$P(w|c) = \\frac{T(w,c) + 1}{\\sum_{w' \\in V} T(w',c) + 1}$$\n",
        "\n",
        "Il existe d'autre méthodes de lissage, en général adaptées à d'autres applications plus complexes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6_8GhNz55z3"
      },
      "source": [
        "## Représentation adaptée des documents\n",
        "\n",
        "Notre modèle statistique, comme la plupart des modèles appliqués aux données textuelles, utilise les comptes d'occurences de mots dans un document. Ainsi, une manière très pratique de représenter un document est d'utiliser un vecteur \"Bag-of-Words\" (BoW), contenant les comptes de chaque mot (indifférement de leur ordre d'apparition) dans le document. \n",
        "\n",
        "Si on considère l'ensemble de tous les mots apparaissant dans nos $T$ documents d'apprentissage, que l'on note $V$ (Vocabulaire), on peut créer **un index**, qui est une bijection associant à chaque mot $w$ un entier, qui sera sa position dans $V$. \n",
        "\n",
        "Ainsi, pour un document extrait d'un ensemble de documents contenant $|V|$ mots différents, une représentation BoW sera un vecteur de taille $|V|$, dont la valeur à l'indice d'un mot $w$ sera son nombre d'occurences dans le document. \n",
        "\n",
        "On peut utiliser la classe **CountVectorizer** de scikit-learn pour mieux comprendre:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37BRH6Bh55z3"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fHQfmB055z3",
        "outputId": "64dc9b01-483c-482b-e23f-befd0932caf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['avenue' 'boulevard' 'city' 'down' 'ran' 'the' 'walk' 'walked']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 2, 0, 1, 0, 1],\n",
              "       [1, 0, 0, 1, 0, 1, 0, 1],\n",
              "       [0, 1, 0, 1, 1, 1, 0, 0],\n",
              "       [0, 0, 1, 1, 0, 1, 1, 0],\n",
              "       [1, 0, 0, 1, 0, 2, 1, 0]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus = ['I walked down down the boulevard',\n",
        "          'I walked down the avenue',\n",
        "          'I ran down the boulevard',\n",
        "          'I walk down the city',\n",
        "          'I walk down the the avenue']\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "Bow = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "Bow.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR51yXUM55z3"
      },
      "source": [
        "On affiche d'abord la liste contenant les mots ordonnés selon leur indice (On note que les mots de 2 caractères ou moins ne sont pas pris en compte)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHLNRXC155z4"
      },
      "source": [
        "## Détail: entraînement\n",
        "\n",
        "L'idée est d'extraire le nombre d'occurences $T(w,c)$ de chaque mot $w$ pour chaque classe $c$, ce qui permettra de calculer la matrice de probabilités conditionelles $\\pmb{P}$ telle que: $$\\pmb{P}_{w,c} = P(w|c)$$\n",
        "\n",
        "Notons que le nombre d'occurences $T(w,c)$ peut être obtenu facilement à partir des représentations BoW de l'ensemble des documents.\n",
        "\n",
        "### Procédure:\n",
        "<img src=\"algo_train.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
        "\n",
        "## Détail: test\n",
        "\n",
        "Nous connaissons maintenant les probabilités conditionelles données par la matrice $\\pmb{P}$. \n",
        "Il faut maintenant obtenir $P(s|c)$ pour le document courant. Cette quantité s'obtient à l'aide d'un calcul simple impliquant la représentation BoW du document et $\\pmb{P}$.\n",
        "\n",
        "### Procédure:\n",
        "<img src=\"algo_test.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-CohfrI55z4"
      },
      "source": [
        "## Preprocessing du texte: obtenir les représentations BoW\n",
        "\n",
        "D'abord, il faut transformer les critiques sous forme de strings en une liste de mots. La tactique la plus simple consiste à diviser le string suivant les espaces, avec la commande:\n",
        "```text.split()```\n",
        "\n",
        "Mais il faut aussi faire attention à enlever les caractères particuliers qui pourraient ne pas avoir été nettoyés (comme les balises HTML si on a obtenu les données à partir de pages web). Puisque l'on va compter les mots, il faudra construire une liste des mots apparaissant dans nos données. Dans notre cas, on aimerait réduire cette liste et l'uniformiser (ignorer les majuscules, la ponctuation, et les mots les plus courts). \n",
        "On va donc utiliser une fonction adaptée à nos besoins - mais c'est un travail qu'il n'est en général pas nécessaire de faire, puisqu'il existe de nombreux outils déjà adaptés à la plupart des cas de figures. \n",
        "Pour le nettoyage du texte, il existe de nombreux scripts, basés sur différents outils (expressions régulières, par exemple) qui permettent de préparer des données. La division du texte en mots et la gestion de la ponctuation est gérée lors d'une étape appellée *tokenization*; si besoin, un package python comme le NLTK contient de nombreux *tokenizers* différents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBUl2K7C55z4",
        "outputId": "3e231c01-c86e-42df-882b-f00f0c34a8e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhlRkCFe55z4"
      },
      "outputs": [],
      "source": [
        "# We might want to clean the file with various strategies:\n",
        "def clean_and_tokenize(text):\n",
        "    \"\"\"\n",
        "    Cleaning a document with:\n",
        "        - Lowercase        \n",
        "        - Removing numbers with regular expressions\n",
        "        - Removing punctuation with regular expressions\n",
        "        - Removing other artifacts\n",
        "    And separate the document into words by simply splitting at spaces\n",
        "    Params:\n",
        "        text (string): a sentence or a document\n",
        "    Returns:\n",
        "        tokens (list of strings): the list of tokens (word units) forming the document\n",
        "    \"\"\"        \n",
        "    # TODO - Lowercase\n",
        "    text = text.lower()\n",
        "    # TODO - Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # TODO - Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove small words (1 and statistique2 characters)\n",
        "    text = re.sub(r\"\\b\\w{1,2}\\b\", \"\", text)\n",
        "    # Remove HTML artifacts specific to the corpus we're going to work with\n",
        "    REPLACE_HTML = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "    text = REPLACE_HTML.sub(\" \", text)\n",
        "    \n",
        "    tokens = text.split()        \n",
        "    return tokens\n",
        "\n",
        "# Or we might want to use an already-implemented tool. The NLTK package has a lot of very useful text processing tools, among them various tokenizers\n",
        "# Careful, NLTK was the first well-documented NLP package, but it might be outdated for some uses. Check the documentation !\n",
        "# TODO : Utiliser la fonction clean_and_tokenize définie ci-haut et word_tokenize de nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "corpus_raw = \"I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeUlpYYq55z4",
        "outputId": "9c257358-6f51-46f0-821f-3ea95a43bbc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['walked', 'down', 'down', 'the', 'boulevard', 'walked', 'down', 'the', 'avenue', 'ran', 'down', 'the', 'boulevard', 'walk', 'down', 'the', 'city', 'walk', 'down', 'the', 'the', 'avenue']\n",
            "['avenue' 'boulevard' 'city' 'down' 'ran' 'the' 'walk' 'walked']\n"
          ]
        }
      ],
      "source": [
        "corpus_r = clean_and_tokenize(corpus_raw) \n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "Bow = vectorizer.fit_transform(corpus_r)\n",
        "print(corpus_r)\n",
        "print(vectorizer.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQY9JyvX55z5"
      },
      "source": [
        "Fonction **à compléter**. Elle prend en entrée une liste de document (chacun sous la forme d'un string) et renvoie, comme dans l'exemple utilisant ```CountVectorizer```:\n",
        "- Un vocabulaire qui associe à chaque mot rencontré un index\n",
        "- Une matrice, dont les lignes représentent les documents et les colonnes les mots indexés par le vocabulaire. En position $(i,j)$, on devra avoir le nombre d'occurence du mot $j$ dans le document $i$.\n",
        "\n",
        "Le vocabulaire, qui était sous la forme d'une *liste* dans l'exemple précédent, pourra être renvoyé sous forme de *dictionnaire* dont les clés sont les mots et les valeurs les indices. Puisque le vocabulaire recense les mots du corpus sans se soucier de leur nombre d'occurences, on pourra le constituer à l'aide d'un ensemble (```set``` en python). \n",
        "On pourra bien sur utiliser la fonction ```clean_and_tokenize``` pour transformer les strings en liste de mots. \n",
        "##### Quelques pointeurs pour les débutants en Python : \n",
        "\n",
        "- ```my_list.append(value)``` : put the variable ```value``` at the end of the list ```my_list```\n",
        "\n",
        "-  ```words = set()``` : create a set, which is a list of unique values \n",
        "\n",
        "- ```words.union(my_list)``` : extend the set ```words```\n",
        "\n",
        "- ```dict(zip(keys, values)))``` : create a dictionnary\n",
        "\n",
        "- ```for k, text in enumerate(texts)``` : syntax for a loop with the index, ```texts``` begin a list (of texts !)\n",
        "\n",
        "- ```len(my-list)``` : length of the list ```my_list```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgpfsVwv55z5"
      },
      "outputs": [],
      "source": [
        "def cleaned_text(text) :\n",
        "    \n",
        "    tokens = clean_and_tokenize(text)\n",
        "    text = ' '.join(tokens)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pZxHijEf55z5",
        "outputId": "62e860e5-b25b-4383-de6f-498a0d7d0ba4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'walked down down the boulevard'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_text(corpus[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y74QvvVepLi"
      },
      "outputs": [],
      "source": [
        "def count_words(texts):\n",
        "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    texts : list of str\n",
        "        The texts\n",
        "    Returns\n",
        "    -------\n",
        "    vocabulary : dict\n",
        "        A dictionary that points to an index in counts for each word.\n",
        "    counts : ndarray, shape (n_samples, n_features)\n",
        "        The counts of each word in each text.\n",
        "    \"\"\"\n",
        "    # List of cleaned texts\n",
        "    cleaned_texts = [cleaned_text(text) for text in texts]\n",
        "    \n",
        "    # BOW Vectorizer + Fit Transform\n",
        "    vectorizer = CountVectorizer()\n",
        "    bow = vectorizer.fit_transform(cleaned_texts)\n",
        "    \n",
        "    # Retrieve feature names, create vocabulary dictionary and counts array\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    vocabulary = {word: index for index, word in enumerate(feature_names)}\n",
        "    counts = bow.toarray()\n",
        "    \n",
        "    return vocabulary, counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVoW_DcG55z5",
        "outputId": "22693427-bb17-4ae7-ed27-eaa4b555e9de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'avenue': 0, 'boulevard': 1, 'city': 2, 'down': 3, 'ran': 4, 'the': 5, 'walk': 6, 'walked': 7}\n",
            "[[0 1 0 2 0 1 0 1]\n",
            " [1 0 0 1 0 1 0 1]\n",
            " [0 1 0 1 1 1 0 0]\n",
            " [0 0 1 1 0 1 1 0]\n",
            " [1 0 0 1 0 2 1 0]]\n"
          ]
        }
      ],
      "source": [
        "Voc, X = count_words(corpus)\n",
        "print(Voc)\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYvsmgbU55z6"
      },
      "source": [
        "## Bayésien Naïf \n",
        "\n",
        "Classe vide : fonctions **à compléter** : \n",
        "\n",
        "```python\n",
        "def fit(self, X, y)\n",
        "``` \n",
        "**Entraînement** : va apprendre un modèle statistique basés sur les représentations $X$ correspondant aux labels $y$.\n",
        "$X$ représente donc ici des représentations obtenues en sortie de count_words. On complète la fonction à l'aide de la procédure détaillée plus haut. Si il est possible de la suivre à la lettre, les représentations que l'on utilise nous permettre d'être bien plus efficace et d'éviter d'utiliser des boucles !\n",
        "\n",
        "\n",
        "Note: le lissage, effectué à la ligne $10$, ne se fait pas nécessairement avec un $1$, mais peut se faire avec une valeur positive $\\alpha$, qu'on peut implémenter comme argument de la classe ```NB```.\n",
        "\n",
        "```python\n",
        "def predict(self, X)\n",
        "```\n",
        "**Testing** : va renvoyer les labels prédits par le modèle pour d'autres représentations $X$.\n",
        "\n",
        "\n",
        "Pour faciliter la procédure, on prendra la moitié de la matrice $X$ obtenue plus haut pour entraîner le modèle, et l'autre moitié pour l'évaluer. **Important**: cette façon de procéder n'est pas réaliste: en général, on ne dispose que des données d'entraînement au moment de créer le vocabulaire et d'entraîner le modèle. Ainsi, il est possible que les données d'évaluation contiennent des mots *inconnus*. C'est quelque chose qu'on peut traiter facilement en dédiant un indice à tous les mots rencontrés qui ne sont pas contenus dans le vocabulaire - mais il existe de nombreuses méthodes plus complexes pour réussir à utiliser à bon escient ces mots que le modèle n'a pas rencontré à l'entraînement. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRay3AYH55z6"
      },
      "source": [
        "##### Quelques pointeurs pour les débutants en Python : \n",
        "\n",
        "Utiliser l'API Numpy pour travailler avec des tenseurs\n",
        "\n",
        "\n",
        "- ```X.shape``` : for a ```numpy.array```, return the dimension of the tensor\n",
        "\n",
        "- ```np.zeros((dim_1, dim_2,...))``` : create a tensor filled with zeros\n",
        "\n",
        "- ```np.sum(X, axis = n)``` : sum the tensor over the axis n\n",
        "\n",
        "- ```np.mean(X, axis = n)```\n",
        "\n",
        "- ```np.argmax(X, axis = n)```\n",
        "\n",
        "- ```np.log(X)```\n",
        "\n",
        "- ```np.dot(X_1, X_1)``` : Matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z0O3fhs55z6"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hoa7KT6fNk-"
      },
      "outputs": [],
      "source": [
        "class NB(BaseEstimator, ClassifierMixin):\n",
        "    # Les arguments de classe permettent l'héritage de classes de sklearn\n",
        "    def __init__(self, alpha=1.0):\n",
        "        # alpha est un paramètre pour le lissage: il correspond à la valeur ligne 10 de l'algorithme d'entraînement\n",
        "        # Dans l'algorithme d'entraînement, et comme valeur par défaut, on utilise alpha = 1\n",
        "        self.alpha = alpha\n",
        "        self.prior = np.zeros(0)\n",
        "        self.condprob = np.zeros(0)\n",
        "        self.V = 0\n",
        "        self.classes = 0\n",
        "        self.classname = np.zeros(0)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Taille des données\n",
        "        n, m = X.shape\n",
        "        \n",
        "        # Nombre de classes différentes\n",
        "        self.classes = np.unique(y)\n",
        "        self.classname = self.classes\n",
        "        \n",
        "        # Nombre de classes\n",
        "        nc = len(self.classes)\n",
        "        \n",
        "        # Initialisation de prior et condprob\n",
        "        self.prior = np.zeros(nc)\n",
        "        self.condprob = np.zeros((nc, m))\n",
        "        \n",
        "        # Compter les documents par classe\n",
        "        for i in range(nc):\n",
        "            self.prior[i] = np.sum(y == self.classes[i])\n",
        "        \n",
        "        # Calculer les probabilités a priori\n",
        "        self.prior /= n\n",
        "        \n",
        "        # Compter les occurrences de chaque mot par classe\n",
        "        for i in range(nc):\n",
        "            X_class = X[y == self.classes[i]]\n",
        "            self.condprob[i] = np.sum(X_class, axis=0)\n",
        "            \n",
        "        # Ajouter lissage de Laplace\n",
        "        self.condprob += self.alpha\n",
        "        self.condprob /= np.sum(self.condprob, axis=1)[:, np.newaxis]\n",
        "        \n",
        "        # Vocabulaire\n",
        "        self.V = X.shape[1]\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Nombre de documents\n",
        "        n, m = X.shape\n",
        "        \n",
        "        # Initialisation\n",
        "        nc = len(self.classes)\n",
        "        result = np.zeros(n)\n",
        "        \n",
        "        # Calculer les log-probabilités pour chaque classe\n",
        "        log_prob = np.zeros((n, nc))\n",
        "        for i in range(nc):\n",
        "            log_prob[:, i] = np.sum(np.log(self.condprob[i][np.newaxis, :]) * X, axis=1)\n",
        "        \n",
        "        # Ajouter les log-priorités\n",
        "        log_prob += np.log(self.prior[np.newaxis, :])\n",
        "        \n",
        "        # Trouver la classe ayant la plus haute probabilité\n",
        "        result = self.classes[np.argmax(log_prob, axis=1)]\n",
        "\n",
        "        return result\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return np.mean(self.predict(X) == y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmTnAxg_55z6"
      },
      "source": [
        "## Expérimentation\n",
        "\n",
        "On utilise la moitié des données pour l'entraînement, l'autre pour tester le modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuwTdbuS55z6"
      },
      "outputs": [],
      "source": [
        "voc, X = count_words(texts_reduced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7vCsPLn55z7",
        "outputId": "c82e8661-3188-44e6-e52e-235bd9e8668e",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7866242038216561\n"
          ]
        }
      ],
      "source": [
        "nb = NB()\n",
        "nb.fit(X[::2], y_reduced[::2])\n",
        "print(nb.score(X[1::2], y_reduced[1::2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aGE3WtX55z7"
      },
      "source": [
        "## Cross-validation \n",
        "\n",
        "Avec la fonction *cross_val_score* de scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdDRprYG55z7",
        "outputId": "44bcdc3b-e829-4e57-8bad-3da5b5a3728d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score de classification: 0.7774785549650305 (std 0.02101666125519929)\n"
          ]
        }
      ],
      "source": [
        "scores = cross_val_score(nb, X, y_reduced, cv=5)\n",
        "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ied8oWE055z7"
      },
      "source": [
        "## Evaluer les performances: \n",
        "\n",
        "**Quelles sont les points forts et les points faibles de ce système ? Comment y remédier ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laYe3KHo55z7"
      },
      "source": [
        "Les points forts : \n",
        "- Il crée une sorte de champs lexical de mots pour chaque catégorie\n",
        "- Facilement adaptable aux cas multi-categoriels\n",
        "- Peut également être utilisé dans le cadre de variables catégorielles \n",
        "\n",
        "\n",
        "Les points faibles :\n",
        "- Ne prend pas en compte le contexte et l'odre des mote \n",
        "    - Solution : Utiliser des passages en plusieurs mots (Bi-gram...) \n",
        "- NB est connu pour être un mauvais estimateur de probabilité \n",
        "    - Solution : Ne pas prendre très aux sérieux les probabilités prédites\n",
        "- NB posera problème si la taille de mots prise en compte est trop grande\n",
        "    - Solution : Réduire le Vocabulaire étudié sur la base de taille ou d'occurences du mot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4oxqAdo55z7"
      },
      "source": [
        "## Pour aller plus loin: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEkbp6lk55z7"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R-evZN755z8"
      },
      "source": [
        "## Scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2d1NMR755z8"
      },
      "source": [
        "### Améliorer les représentations\n",
        "\n",
        "On utilise la function \n",
        "```python\n",
        "CountVectorizer\n",
        "``` \n",
        "de scikit-learn pour constituer notre corpus. Elle nous permettra d'améliorer facilement nos représentations BoW.\n",
        "\n",
        "#### Tf-idf:\n",
        "\n",
        "Il s'agit du produit de la fréquence du terme (TF) et de sa fréquence inverse dans les documents (IDF).\n",
        "Cette méthode est habituellement utilisée pour extraire l'importance d'un terme $i$ dans un document $j$ relativement au reste du corpus, à partir d'une matrice d'occurences $mots \\times documents$. Ainsi, pour une matrice $\\mathbf{T}$ de $|V|$ termes et $D$ documents:\n",
        "$$\\text{TF}(T, w, d) = \\frac{T_{w,d}}{\\sum_{w'=1}^{|V|} T_{w',d}} $$\n",
        "\n",
        "$$\\text{IDF}(T, w) = \\log\\left(\\frac{D}{|\\{d : T_{w,d} > 0\\}|}\\right)$$\n",
        "\n",
        "$$\\text{TF-IDF}(T, w, d) = \\text{TF}(X, w, d) \\cdot \\text{IDF}(T, w)$$\n",
        "\n",
        "On peut l'adapter à notre cas en considérant que le contexte du deuxième mot est le document. Cependant, TF-IDF est généralement plus adaptée aux matrices peu denses, puisque cette mesure pénalisera les termes qui apparaissent dans une grande partie des documents. \n",
        "    \n",
        "#### Ne pas prendre en compte les mots trop fréquents:\n",
        "\n",
        "On peut utiliser l'option \n",
        "```python\n",
        "max_df=1.0\n",
        "```\n",
        "pour modifier la quantité de mots pris en compte. \n",
        "\n",
        "#### Essayer différentes granularités:\n",
        "\n",
        "Plutôt que de simplement compter les mots, on peut compter les séquences de mots - de taille limitée, bien sur. \n",
        "On appelle une séquence de $n$ mots un $n$-gram: essayons d'utiliser les 2 et 3-grams (bi- et trigrams).\n",
        "On peut aussi tenter d'utiliser les séquences de caractères à la place de séquences de mots.\n",
        "\n",
        "On s'intéressera aux options \n",
        "```python\n",
        "analyzer='word'\n",
        "```\n",
        "et \n",
        "```python\n",
        "ngram_range=(1, 2)\n",
        "```\n",
        "que l'on changera pour modifier la granularité. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE7EtptQZqXM",
        "outputId": "0a4b6591-9582-45e7-b1db-667163e8d41f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification score: %s (std %s) (0.7822538870363477, 0.02171467351437541)\n",
            "Classification score tf-idf: %s (std %s) (0.7734873626765305, 0.028913129424724272)\n",
            "Classification score sans mots fréquents: %s (std %s) (0.7734873626765305, 0.028913129424724272)\n",
            "Classification score bigram: %s (std %s) (0.7898061828233784, 0.023512164345416454)\n",
            "Classification score trigram: %s (std %s) (0.7272983612270598, 0.022410286409742002)\n",
            "Classification score char: %s (std %s) (0.7842387903653775, 0.023202886592023828)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "pipeline_base = Pipeline([\n",
        "    ('vect', CountVectorizer(analyzer='word', stop_words=None)),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_base, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score: %s (std %s)\",(np.mean(scores), np.std(scores)))\n",
        "\n",
        "\n",
        "pipeline_tf_idf = Pipeline([\n",
        "    ('vect', CountVectorizer(analyzer='word', stop_words=None)),\n",
        "    ('ifidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_tf_idf, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score tf-idf: %s (std %s)\",(np.mean(scores), np.std(scores)))\n",
        "\n",
        "pipeline_maxdf = Pipeline([\n",
        "    ('vect', CountVectorizer(analyzer='word', stop_words=None,max_df=1.0)),\n",
        "    ('ifidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB()),         \n",
        "])\n",
        "scores = cross_val_score(pipeline_maxdf, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score sans mots fréquents: %s (std %s)\",(np.mean(scores), np.std(scores)))\n",
        "\n",
        "pipeline_bigram = Pipeline([\n",
        "    ('vect', CountVectorizer(analyzer='word', stop_words=None, ngram_range=(2,2))),\n",
        "    ('ifidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_bigram, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score bigram: %s (std %s)\",(np.mean(scores), np.std(scores)))\n",
        "\n",
        "pipeline_trigram = Pipeline([\n",
        "    ('vect', CountVectorizer(analyzer='word', stop_words=None, ngram_range=(3,3))),\n",
        "    ('ifidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_trigram, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score trigram: %s (std %s)\",(np.mean(scores), np.std(scores)))\n",
        "\n",
        "pipeline_char = Pipeline([\n",
        "    ('vect', CountVectorizer(analyzer='char', stop_words=None, ngram_range=(2,6))),\n",
        "    ('ifidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_char, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score char: %s (std %s)\",(np.mean(scores), np.std(scores)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPvZll7U55z8"
      },
      "source": [
        "### Natural Language Toolkit (NLTK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWWMA2GT55z8"
      },
      "source": [
        "### Stemming \n",
        "\n",
        "Permet de revenir à la racine d'un mot: on peut ainsi grouper différents mots autour de la même racine, ce qui facilite la généralisation. Utiliser:\n",
        "```python\n",
        "from nltk import SnowballStemmer\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5n5IReH55z8"
      },
      "outputs": [],
      "source": [
        "from nltk import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqQnlobv55z8"
      },
      "source": [
        "#### Exemple d'utilisation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBVMEOHE55z9",
        "outputId": "e4e04676-7d70-4e3a-f0d2-b8c77813b2f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word : singers ; stemmed : singer\n",
            "word : cat ; stemmed : cat\n",
            "word : generalization ; stemmed : general\n",
            "word : philosophy ; stemmed : philosophi\n",
            "word : psychology ; stemmed : psycholog\n",
            "word : philosopher ; stemmed : philosoph\n"
          ]
        }
      ],
      "source": [
        "words = ['singers', 'cat', 'generalization', 'philosophy', 'psychology', 'philosopher']\n",
        "for word in words:\n",
        "    print('word : %s ; stemmed : %s' %(word, stemmer.stem(word)))#.decode('utf-8'))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXSmfkRh55z9"
      },
      "source": [
        "#### Transformation des données:\n",
        "\n",
        "Classe vide : function **à compléter** \n",
        "```python\n",
        "def stem(X)\n",
        "``` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuWU8wHka734"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "def stem(X): \n",
        "    X_stem = []\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    for document in X:\n",
        "        # On commence par appliquer une expression régulière pour supprimer les caractères spéciaux et les chiffres\n",
        "        document = re.sub(r'[^a-zA-Z ]', ' ', document)\n",
        "        # On transforme en minuscules\n",
        "        document = document.lower()\n",
        "        # On sépare les mots\n",
        "        words = document.split()\n",
        "        # On applique le stemming sur chaque mot\n",
        "        stemmed_words = [stemmer.stem(word) for word in words]\n",
        "        # On reconstitue le document\n",
        "        document_stem = ' '.join(stemmed_words)\n",
        "        X_stem.append(document_stem)\n",
        "    return X_stem\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYBA8pJY55z9",
        "outputId": "0654ed6b-b283-4fa9-b4e1-9928c5604440",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score de classification: 0.7810459949466548 (std 0.013107505401319918)\n"
          ]
        }
      ],
      "source": [
        "texts_stemmed = stem(texts_reduced)\n",
        "voc, X = count_words(texts_stemmed)\n",
        "nb = NB()\n",
        "\n",
        "scores = cross_val_score(nb, X, y_reduced, cv=5)\n",
        "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90P71ejB55z9"
      },
      "source": [
        "### Partie du discours\n",
        "\n",
        "Pour généraliser, on peut aussi utiliser les parties du discours (Part of Speech, POS) des mots, ce qui nous permettra  de filtrer l'information qui n'est potentiellement pas utile au modèle. On va récupérer les POS des mots à l'aide des fonctions:\n",
        "```python\n",
        "from nltk import pos_tag, word_tokenize\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tToHmPrZ55z9"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcUcnj9955z9"
      },
      "source": [
        "#### Exemple d'utilisation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NbDZIC655z-",
        "outputId": "73f08879-dfa0-45e8-f782-a84ceda3ed8f",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('I', 'PRP'), ('am', 'VBP'), ('Sam', 'NNP')]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "pos_tag(word_tokenize(('I am Sam')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewbsgmJS55z-",
        "outputId": "52161157-a710-4d0d-bbdd-d3a1abc850fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Story', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('man', 'NN'),\n",
              " ('who', 'WP'),\n",
              " ('has', 'VBZ'),\n",
              " ('unnatural', 'JJ'),\n",
              " ('feelings', 'NNS'),\n",
              " ('for', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('pig', 'NN'),\n",
              " ('.', '.'),\n",
              " ('Starts', 'VBZ'),\n",
              " ('out', 'RP'),\n",
              " ('with', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('opening', 'NN'),\n",
              " ('scene', 'NN'),\n",
              " ('that', 'WDT'),\n",
              " ('is', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('terrific', 'JJ'),\n",
              " ('example', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('absurd', 'JJ'),\n",
              " ('comedy', 'NN'),\n",
              " ('.', '.'),\n",
              " ('A', 'DT'),\n",
              " ('formal', 'JJ'),\n",
              " ('orchestra', 'NN'),\n",
              " ('audience', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('turned', 'VBN'),\n",
              " ('into', 'IN'),\n",
              " ('an', 'DT'),\n",
              " ('insane', 'NN'),\n",
              " (',', ','),\n",
              " ('violent', 'JJ'),\n",
              " ('mob', 'NN'),\n",
              " ('by', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('crazy', 'JJ'),\n",
              " ('chantings', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('it', 'PRP'),\n",
              " (\"'s\", 'VBZ'),\n",
              " ('singers', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('Unfortunately', 'RB'),\n",
              " ('it', 'PRP'),\n",
              " ('stays', 'VBZ'),\n",
              " ('absurd', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('WHOLE', 'NNP'),\n",
              " ('time', 'NN'),\n",
              " ('with', 'IN'),\n",
              " ('no', 'DT'),\n",
              " ('general', 'JJ'),\n",
              " ('narrative', 'JJ'),\n",
              " ('eventually', 'RB'),\n",
              " ('making', 'VBG'),\n",
              " ('it', 'PRP'),\n",
              " ('just', 'RB'),\n",
              " ('too', 'RB'),\n",
              " ('off', 'RP'),\n",
              " ('putting', 'VBG'),\n",
              " ('.', '.'),\n",
              " ('Even', 'RB'),\n",
              " ('those', 'DT'),\n",
              " ('from', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('era', 'NN'),\n",
              " ('should', 'MD'),\n",
              " ('be', 'VB'),\n",
              " ('turned', 'VBN'),\n",
              " ('off', 'RP'),\n",
              " ('.', '.'),\n",
              " ('The', 'DT'),\n",
              " ('cryptic', 'JJ'),\n",
              " ('dialogue', 'NN'),\n",
              " ('would', 'MD'),\n",
              " ('make', 'VB'),\n",
              " ('Shakespeare', 'NNP'),\n",
              " ('seem', 'VB'),\n",
              " ('easy', 'JJ'),\n",
              " ('to', 'TO'),\n",
              " ('a', 'DT'),\n",
              " ('third', 'JJ'),\n",
              " ('grader', 'NN'),\n",
              " ('.', '.'),\n",
              " ('On', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('technical', 'JJ'),\n",
              " ('level', 'NN'),\n",
              " ('it', 'PRP'),\n",
              " (\"'s\", 'VBZ'),\n",
              " ('better', 'JJR'),\n",
              " ('than', 'IN'),\n",
              " ('you', 'PRP'),\n",
              " ('might', 'MD'),\n",
              " ('think', 'VB'),\n",
              " ('with', 'IN'),\n",
              " ('some', 'DT'),\n",
              " ('good', 'JJ'),\n",
              " ('cinematography', 'NN'),\n",
              " ('by', 'IN'),\n",
              " ('future', 'JJ'),\n",
              " ('great', 'JJ'),\n",
              " ('Vilmos', 'NNP'),\n",
              " ('Zsigmond', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Future', 'NNP'),\n",
              " ('stars', 'VBZ'),\n",
              " ('Sally', 'NNP'),\n",
              " ('Kirkland', 'NNP'),\n",
              " ('and', 'CC'),\n",
              " ('Frederic', 'NNP'),\n",
              " ('Forrest', 'NNP'),\n",
              " ('can', 'MD'),\n",
              " ('be', 'VB'),\n",
              " ('seen', 'VBN'),\n",
              " ('briefly', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pos_tag(word_tokenize(texts_reduced[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRciSFDl55z-"
      },
      "source": [
        "Détails des significations des tags POS: https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9_vAlu255z-"
      },
      "source": [
        "####  Transformation des données:\n",
        "\n",
        "Classe vide : fonction **à compléter** \n",
        "```python\n",
        "def pos_tag_filter(X, good_tags=['NN', 'VB', 'ADJ', 'RB'])\n",
        "``` \n",
        "\n",
        "Ne garder que les noms, adverbes, verbes et adjectifs pour notre modèle. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEkJaiIpbrxw"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "def pos_tag_filter(X, good_tags=['NN', 'VB', 'ADJ', 'RB']):\n",
        "    X_pos = []\n",
        "    for text in X:\n",
        "        # Tokenize the text into words\n",
        "        words = word_tokenize(text)\n",
        "        # Apply Part-of-Speech (POS) tagging to the words\n",
        "        tagged_words = pos_tag(words)\n",
        "        # Keep only the words that have a POS tag in good_tags\n",
        "        filtered_words = [word for word, tag in tagged_words if tag in good_tags]\n",
        "        # Join the filtered words into a string and add it to the list\n",
        "        X_pos.append(' '.join(filtered_words))\n",
        "    return X_pos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if3HjBgi55z-",
        "outputId": "440da528-c054-49a0-be42-437aba74fe4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TIME 44.87489080429077\n",
            "Score de classification: 0.7480147006407769 (std 0.018029079998019285)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "texts_POS = pos_tag_filter(texts_reduced)\n",
        "voc, X = count_words(texts_POS)\n",
        "nb = NB()\n",
        "\n",
        "scores = cross_val_score(nb, X, y_reduced, cv=5)\n",
        "print(\"TIME\", time.time()-start)\n",
        "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JL9-IZg55z-"
      },
      "source": [
        "## Utilisation d'un classifieur plus complexe ?\n",
        "\n",
        "On peut utiliser les implémentations scikit-learn de classifieurs moins naïfs, comme la régression logistique ou les SVM. Quel en est l'inconvénient principal (imaginons que, plutôt qu'un modèle linéaire, on choisisse d'utiliser un réseau de neurones à plusieurs couches cachées ?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gxiFk3o55z_"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_LsnA--cIIY",
        "outputId": "0460485a-0a4e-4987-afed-999ba4375180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification score: 0.819260532422992 (std 0.010147391125280188)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "pipeline_logistic = Pipeline([\n",
        "    ('vect', CountVectorizer(analyzer='word', stop_words=None)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', LogisticRegression()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_logistic, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score: %s (std %s)\" % (np.mean(scores), np.std(scores)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ86sP9455z_"
      },
      "source": [
        "# Représentations denses\n",
        "\n",
        "##  Word Embeddings : Représentations distribuées via l'hypothèse distributionelle\n",
        "\n",
        "**But**: On va chercher à obtenir des représentations denses (comme vecteurs de nombres réels) de mots (et éventuellement de phrases). Ces représentations ont vocation à être distribuées: ce sont des représentations non-locales. On représente un objet comme une combinaison de *features*, par opposition à l'attribution d'un symbole dédié: voir le travail fondateur d'entre autres, Geoffrey Hinton, sur le sujet: [Distributed Representations](https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf).\n",
        "\n",
        "Le terme de représentation *distribuées* est très général, mais correspond à que l'on cherche à obtenir. L'enjeu est donc de pouvoir construire, automatiquement, de telles représentations.\n",
        "\n",
        "**Idée sous-jacente**: Elle est basée sur l'hypothèse distributionelle: les informations contextuelles suffisent à obtenir une représentation viable d'objets linguistiques.\n",
        " - *“For a large class of cases [...] the  meaning  of a word is  its  use in the  language.”* Wittgenstein (Philosophical Investigations, 43 - 1953)\n",
        " - *“You shall know a word by the company it keeps”*, Firth (\"A synopsis of linguistic theory 1930-1955.\" - 1957)\n",
        "\n",
        "Ainsi, on peut caractériser un mot par les mots qui l'accompagnent, via des comptes de co-occurences. Deux mots ayant un sens similaire auront une distribution contextuelle similaire et auront donc plus de chance d'apparaître dans des contextes similaires. Cette hypothèse peut servir de justification à l'application de statistiques à la sémantique (extraction d'information, analyse sémantique). Elle permet aussi une certaine forme de généralisation: on peut supposer que les informations que l'on a à propos d'un mot se généraliseront aux mots à la distribution similaire. \n",
        "\n",
        "**Motivation**: On cherche à obtenir des représentations distribuées pour pouvoir, de manière **efficace**:\n",
        "- Directement réaliser une analyse sémantique de surface.\n",
        "- S'en servir comme source d'informations pour d'autres modèles et applications liées au language, notamment pour l'analyse de sentiments. \n",
        "\n",
        "\n",
        "**Terminologie**: Attention à ne pas confondre l'idée de représentation *distribuée* et *distributionelle*. Le second indique en général (pour les mots) que la représentation a été obtenue strictement à partir de comptes de co-occurences, alors qu'on pourra utiliser des informations supplémentaires (labels de documents, tags de partie du discours, ...) pour construire des représentations distribuées. \n",
        "Les modèles qui permettent de construire ces représentations denses, sous forme de vecteurs, sont souvent appellés *vector spaces models*. On appelle aussi régulièrement ces représentations des *word embeddings*, car les mots sont embarqués (*embedded*) dans un espace vectoriel. En Français, on rencontre souvent le terme *plongements de mots* ou *plongements lexicaux*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHKQhXi-55z_"
      },
      "source": [
        "## Obtenir une représentation: comptes d'occurences et de co-occurences\n",
        "\n",
        "Selon le type de corpus dont on dispose, on pourra obtenir différents types d'informations distributionelles. Si l'on a accès à une collection de documents, on pourra ainsi choisir de compter le nombre d'occurence de chaque mot dans chacun des documents, pour obtenir une matrice $mots \\times documents$: c'est sur ce principe qu'est construit **Tf-Idf**. On va maintenant s'intéresser à un cas plus général: on dispose d'une grande quantité de données sous forme de texte, et on cherche à obtenir des représentations de mots sous forme de vecteurs de taille réduite, sans avoir besoin d'un découpage en documents ou catégories. \n",
        "\n",
        "Supposons qu'on dispose d'un corpus contenant $T$ mots différents. On va construire une matrice $\\mathbf{M}$ de taille $T \\times T$ qui contiendra le nombre de co-occurences entre les mots. Il y aura différents facteurs à considérer lors de la construction de cette matrice: \n",
        "- Comment définir le 'contexte' d'un mot, qui permettra de dire que les termes qu'il contient co-occurent avec ce mot ? On pourra choisir d'utiliser différentes échelles: le document, la phrase, le groupe nominal, ou tout simplement une fenêtre de $k$ mots, selon les informations que l'on cherche à capturer.\n",
        "*Encore une fois, si par exemple notre corpus est divisé en $D$ documents, on pourra même s'intéresser aux liens distributionnels entre mots et documents: chacun de ces $D$ documents agira comme un \"contexte\", et on construit une matrice d'occurences $\\mathbf{M}$ de dimension $T \\times D$, où $\\mathbf{M}_{t,d}$ contient le nombre d'occurences du mot $t$ dans le document $d$.* \n",
        "- Comment quantifier l'importance des comptes ? Par exemple, on pourra donner un poids décroissant à une co-occurence selon la distance entre les deux mots concernés ($\\frac{1}{d+1}$ pour une séparation par $d$ mots).\n",
        "- Faut-il garder tous les mots qui apparaissent dans le corpus ? En général, non. On verra que pour les grands corpus, le nombre de mots différents $T$ est énorme. Deuxièmement, même si le nombre de mots est raisonnable, on ne possèdera que très peu d'information distributionelle sur les mots les plus rares, et la représentation obtenue sera à priori de mauvaise qualité. Il faudra se poser la question de comment filtrer ces mots, et de comment traiter les mots qu'on choisit de ne pas représenter.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tAFaUfY55z_"
      },
      "source": [
        "#### Procédure\n",
        "\n",
        "Pour construire la matrice, on va dans un premier temps recueillir la liste des mots différents (ou le *vocabulaire* $V$) qui apparaissent dans le corpus sous forme de dictionaire {mots -> index}\n",
        "Puis, pour chaque terme $w$ du corpus,\n",
        "- On récupère l'index $i$ correspondant à l'aide de $V$\n",
        "- Pour chaque terme $w'$ du contexte de $w$, \n",
        "  + On récupère l'index $j$ correspondant à l'aide de $V$\n",
        "  + On incrémente $\\mathbf{M}_{i,j}$ par le poids correspondant, ou par $1$. \n",
        "  \n",
        "La procédure est très proche de celle qu'on a suivi au TP précédent, excepté qu'il faut maintenant compter les mots suivant leur apparition \n",
        "  \n",
        "#### Exemple\n",
        "\n",
        "On considère le corpus suivant: \n",
        "\n",
        "*I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.*\n",
        "\n",
        "On choisit de définir le contexte d'un mot comme la phrase à laquelle il appartient, et de ne pas utiliser de poids. \n",
        "On obtient la matrice suivante: \n",
        "\n",
        "|     *         | I | the | down | walked | boulevard | avenue | walk | ran | city |\n",
        "|---------------|---|-----|------|--------|-----------|--------|------|-----|------|\n",
        "| I             | 0 |      6 |    6 |   2 |         2 |      2 |   2 |    1 |    1 |\n",
        "| the           | 6 |      2 |    7 |   2 |         2 |      3 |   3 |    1 |    1 |\n",
        "| down          | 6 |      7 |    2 |   3 |         3 |      2 |   2 |    1 |    1 |\n",
        "| walked        | 2 |      2 |    3 |   0 |         1 |      1 |   0 |    0 |    0 |\n",
        "| boulevard     | 2 |      2 |    3 |   1 |         0 |      0 |   0 |    1 |    0 |\n",
        "| avenue        | 2 |      3 |    2 |   1 |         0 |      0 |   1 |    0 |    0 |\n",
        "| ran           | 2 |      3 |    2 |   0 |         0 |      1 |   0 |    0 |    1 |\n",
        "| walk          | 1 |      1 |    1 |   0 |         1 |      0 |   0 |    0 |    0 |\n",
        "| city          | 1 |      1 |    1 |   0 |         0 |      0 |   1 |    0 |    1 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-RQXiKc55z_"
      },
      "source": [
        "### Obtenir un Vocabulaire:\n",
        "\n",
        "Cette fois, on va implémenter séparément une fonction retournant le vocabulaire. Il faudra ici pouvoir contrôler sa taille, que ce soit en indiquant un nombre maximum de mots, ou un nombre minimum d'occurences pour qu'on prenne en compte les mots. On ajoute, à la fin, un mot \"inconnu\" qui remplacera tous les mots qui n'apparaissent pas dans notre vocabulaire 'limité'. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k3rIXJDcd-k"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "\n",
        "def word_count(str):\n",
        "    counts = dict()\n",
        "    words = str.split()\n",
        "\n",
        "    for word in words:\n",
        "        if word in counts:\n",
        "            counts[word] += 1\n",
        "        else:\n",
        "            counts[word] = 1\n",
        "\n",
        "    return counts\n",
        "\n",
        "\n",
        "def vocabulary(corpus, count_threshold=1, voc_threshold=0):\n",
        "    \"\"\"    \n",
        "    Function using word counts to build a vocabulary\n",
        "    Params:\n",
        "        corpus (list of list of strings): corpus of sentences\n",
        "        count_threshold (int): number of occurences necessary for a word to be included in the vocabulary\n",
        "        voc_threshold (int): maximum size of the vocabulary - 0 (default) indicates there is no max\n",
        "    Returns:\n",
        "        vocabulary (dictionary): keys: list of distinct words across the corpus\n",
        "                                 values: indexes corresponding to each word sorted by frequency \n",
        "        vocabulary_word_counts (dictionary): keys: list of distinct words across the corpus\n",
        "                                             values: word counts in the corpus\n",
        "    \"\"\"\n",
        "    corpus=' '.join(corpus)\n",
        "    word_counts = word_count(corpus)\n",
        "    word_counts = sorted(word_counts.items(), key=operator.itemgetter(1))\n",
        "    word_counts.reverse()\n",
        "    word_counts = dict(word_counts)\n",
        "    # Filter the dictionary : \n",
        "    # count_threshold :\n",
        "    Kept_words = [word for word in word_counts if word_counts[word]>count_threshold-1]\n",
        "    Occurrences = [word_counts[word] for word in Kept_words]\n",
        "\n",
        "    if voc_threshold>0 and len(Kept_words)>voc_threshold:\n",
        "        Kept_words = Kept_words[:voc_threshold]\n",
        "        Occurrences = Occurrences[:voc_threshold]\n",
        "\n",
        "    # Adding 'UNK' word with 0 occurences to Kept_words and Occurences\n",
        "    Kept_words.append('UNK')\n",
        "    Occurrences.append(0)\n",
        "\n",
        "    filtered_word_counts = dict(zip(Kept_words,Occurrences))\n",
        "\n",
        "    # Creating vocabulary_word_counts and Index\n",
        "    vocabulary_word_counts = filtered_word_counts\n",
        "    Index = list(range(len(filtered_word_counts)))\n",
        "    vocabulary = dict(zip(Kept_words,Index))\n",
        "\n",
        "    return vocabulary, vocabulary_word_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRHZYaTr550A",
        "outputId": "2f316efa-043b-472c-d1ff-51ebb612924b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'the': 0, 'down': 1, 'I': 2, 'UNK': 3}\n",
            "{'the': 6, 'down': 6, 'I': 5, 'UNK': 0}\n",
            "{'the': 0, 'down': 1, 'I': 2, 'walk': 3, 'avenue': 4, 'boulevard': 5, 'walked': 6, 'city': 7, 'ran': 8, 'UNK': 9}\n",
            "{'the': 6, 'down': 6, 'I': 5, 'walk': 2, 'avenue': 2, 'boulevard': 2, 'walked': 2, 'city': 1, 'ran': 1, 'UNK': 0}\n"
          ]
        }
      ],
      "source": [
        "# Example for testing:\n",
        "\n",
        "corpus = ['I walked down down the boulevard',\n",
        "          'I walked down the avenue',\n",
        "          'I ran down the boulevard',\n",
        "          'I walk down the city',\n",
        "          'I walk down the the avenue']\n",
        "\n",
        "voc, counts = vocabulary(corpus, count_threshold = 3)\n",
        "\n",
        "print(voc)\n",
        "print(counts)\n",
        "\n",
        "# We expect something like this:\n",
        "#  {'down': 0, 'the': 1, 'i': 2, 'UNK': 3}\n",
        "#  {'down': 6, 'the': 6, 'i': 5, 'UNK': 0}\n",
        "\n",
        "voc, counts = vocabulary(corpus)\n",
        "print(voc)\n",
        "print(counts)\n",
        "\n",
        "# We expect something like this:\n",
        "#  {'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n",
        "#  {'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XxEzDHy550A",
        "outputId": "2deab064-1ffa-4268-f9f7-ec01800776b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'the': 0, 'down': 1, 'I': 2, 'UNK': 3}\n",
            "{'the': 6, 'down': 6, 'I': 5, 'UNK': 0}\n",
            "{'the': 0, 'down': 1, 'I': 2, 'walk': 3, 'avenue': 4, 'boulevard': 5, 'walked': 6, 'city': 7, 'ran': 8, 'UNK': 9}\n",
            "{'the': 6, 'down': 6, 'I': 5, 'walk': 2, 'avenue': 2, 'boulevard': 2, 'walked': 2, 'city': 1, 'ran': 1, 'UNK': 0}\n"
          ]
        }
      ],
      "source": [
        "# Example for testing:\n",
        "\n",
        "corpus = ['I walked down down the boulevard',\n",
        "          'I walked down the avenue',\n",
        "          'I ran down the boulevard',\n",
        "          'I walk down the city',\n",
        "          'I walk down the the avenue']\n",
        "\n",
        "voc, counts = vocabulary(corpus, count_threshold = 3)\n",
        "\n",
        "print(voc)\n",
        "print(counts)\n",
        "\n",
        "# We expect something like this:\n",
        "#  {'down': 0, 'the': 1, 'i': 2, 'UNK': 3}\n",
        "#  {'down': 6, 'the': 6, 'i': 5, 'UNK': 0}\n",
        "\n",
        "voc, counts = vocabulary(corpus)\n",
        "print(voc)\n",
        "print(counts)\n",
        "\n",
        "# We expect something like this:\n",
        "#  {'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n",
        "#  {'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS6RRf0B550A"
      },
      "source": [
        "### Obtenir les co-occurences:\n",
        "\n",
        "La fonction prend en entrée le corpus (une liste de strings, correspondant aux documents, ou phrases) et un vocabulaire, ainsi que la taille de la fenêtre de contexte. On pourra aussi implémenter la solution la plus simple - que le contexte d'un mot soit le reste du document duquel il provient. \n",
        "Enfin, on pourra implémenter la possibilité de faire décroitre linéairement l'importance d'un mot du contexte à mesure qu'on s'éloigne du mot d'origine. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P4RazBI550B"
      },
      "outputs": [],
      "source": [
        "def co_occurence_matrix(corpus, vocabulary, window=0, distance_weighting=False):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "        corpus (list of list of strings): corpus of sentences\n",
        "        vocabulary (dictionary): words to use in the matrix\n",
        "        window (int): size of the context window; when 0, the context is the whole sentence\n",
        "        distance_weighting (bool): indicates if we use a weight depending on the distance between words for co-oc counts\n",
        "    Returns:\n",
        "        matrix (array of size (len(vocabulary), len(vocabulary))): the co-oc matrix, using the same ordering as the vocabulary given in input    \n",
        "    \"\"\" \n",
        "    l = len(vocabulary)\n",
        "    print(vocabulary)\n",
        "    M = np.zeros((l,l))\n",
        "    for sent in corpus:\n",
        "        # Obtenir la phrase:\n",
        "        sent = clean_and_tokenize(sent)\n",
        "        # Obtenir les indexs de la phrase grace au vocabulaire: \n",
        "\n",
        "        sent_idx = []\n",
        "        for word in sent : \n",
        "            if word in vocabulary:\n",
        "                sent_idx.append(vocabulary[word])\n",
        "            else:\n",
        "                sent_idx.append(-1)\n",
        "        # Parcourir les indexs de la phrase et ajouter 1 / dist(i,j) à M[i,j] si les mots d'index i et j apparaissent dans la même fenêtre. \n",
        "        for i, idx_i in enumerate(sent_idx):\n",
        "            # On vérifie que le mot est reconnu par le vocabulaire:\n",
        "            if idx_i > -1:\n",
        "                # Si on considère un contexte limité:\n",
        "                if window > 0:\n",
        "                    # On crée une liste qui contient les indexs de la fenêtre à gauche de l'index courant 'idx_i'\n",
        "                    l_ctx_idx = [sent_idx[k] for k in range(max(0,i-window),i)]\n",
        "                    # A compléter\n",
        "                # Si on considère que le contexte est la phrase entière:\n",
        "                else:\n",
        "                    # La liste qui contient le contexte à gauche du mot est plus facile à créer:\n",
        "                    sent_idx.pop(i)\n",
        "                    l_ctx_idx = sent_idx\n",
        "                    # A compléter\n",
        "                # On parcourt cette liste et on update M[i,j]:    \n",
        "                for j, idx_j in enumerate(l_ctx_idx):\n",
        "                    # ... en s'assurant que le mot correspondant à 'idx_j' est reconnu par le vocabulaire\n",
        "                    if idx_j > -1:\n",
        "                        # Calcul du poids:\n",
        "                        if distance_weighting:\n",
        "                            weight = 1.0 \n",
        "                            # A compléter\n",
        "                            lent = len(l_ctx_idx)\n",
        "                            weight = 1/(lent-j)\n",
        "                        else:\n",
        "                            weight = 1.0\n",
        "                        M[idx_i, idx_j] += weight * 1.0\n",
        "                        M[idx_j, idx_i] += weight * 1.0\n",
        "    return M  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhdhs70L550B",
        "outputId": "73149159-904c-4852-9dd0-3b4fc407dad4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'the': 0, 'down': 1, 'I': 2, 'walk': 3, 'avenue': 4, 'boulevard': 5, 'walked': 6, 'city': 7, 'ran': 8, 'UNK': 9}\n",
            "[[2. 5. 0. 3. 3. 2. 2. 1. 1. 0.]\n",
            " [5. 2. 0. 2. 1. 2. 3. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [3. 2. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
            " [3. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            " [2. 2. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            " [2. 3. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "corpus = ['I walked down down the boulevard',\n",
        "          'I walked down the avenue',\n",
        "          'I ran down the boulevard',\n",
        "          'I walk down the city',\n",
        "          'I walk down the the avenue']\n",
        "\n",
        "print(co_occurence_matrix(corpus, voc, 0, False))\n",
        "# Attention: les résultats sont différents de l'exemple plus haut car le vocabulaire n'est pas exactement le même: vérifiez par vous même."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZU7T1C_550B"
      },
      "source": [
        "#### Application à un vrai jeu de données\n",
        "\n",
        "On va chercher à obtenir ces comptes pour les données **imdb**.\n",
        "\n",
        "#### Etude rapide des données\n",
        "\n",
        "On voudrait ici, avant de procéder, avoir une idée de ce que contiennent ces critiques de films. On va donc obtenir le vocabulaire (entier) et représenter les fréquences des mots, dans l'ordre (attention, il faudra utiliser une échelle logarithmique): on devrait retrouver la loi de Zipf. Cela nous permettra d'avoir une idée de la taille du vocabulaire qu'on pourra choisir : il s'agit de réaliser un compromis entre les ressources nécessaires (taille des objets en mémoire) et quantité d'informations qu'on peut en tirer (les mots rares peuvent apporter beaucoup d'informations, mais il est difficile d'en apprendre de bonnes représentations,car ils sont rares !)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vD69OkRy550B",
        "outputId": "fa1ba1e0-c0e7-4496-e362-0c0be162a39f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABkUAAAHDCAYAAACaph0bAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZKklEQVR4nO3deXyV5Zk//uuchCRsCYuyo4B7QLFgQBSLtihqy1THrda2VKuOFrUdbafabws6nVar0/lZNXaf6mhtXaZabRV1UEQqKoKIiMUFVIosKpKwh+Q8vz8sKZEtgSQnJ3m/X6+84nme67mf60B4hHxy33cqSZIkAAAAAAAAWrl0thsAAAAAAABoDkIRAAAAAACgTRCKAAAAAAAAbYJQBAAAAAAAaBOEIgAAAAAAQJsgFAEAAAAAANoEoQgAAAAAANAmCEUAAAAAAIA2QSgCAAAAAAC0CUIRAADIYdOmTYtUKhXTpk3Ldiu0MalUKi655JJstwEAAA0iFAEAgF245557IpVKxf3337/NuaFDh0YqlYonn3xym3P77LNPHHXUUc3RYs5799134+qrr465c+dmuxUAAKAVE4oAAMAujB49OiIiZsyYUed4ZWVlzJ8/P/Lz8+Mvf/lLnXNLliyJJUuW1F7Lzr377rtxzTXXCEUAAIAmJRQBAIBd6NOnTwwcOHCbUGTmzJmRJEmcccYZ25zb8npPQ5EkSWLDhg17NAZNZ926ddluYadaen8AANDchCIAAFAPo0ePjhdffLFOQPGXv/wlBg8eHCeddFI8++yzkclk6pxLpVJx9NFHR0REdXV1fP/734/99tsvCgsLY8CAAfGd73wnNm3aVOc+AwYMiM9+9rPx6KOPxhFHHBHt27ePn//85xER8be//S1OOeWU6NixY/To0SP+9V//dZvrd2bp0qXx1a9+Nfr06ROFhYUxcODAuPjii6Oqqqq2ZtGiRXHGGWdEt27dokOHDnHkkUfGn//85zrj3HbbbZFKpeKtt96qc3x7+5sce+yxMWTIkFiwYEEcd9xx0aFDh+jbt29cf/31da4rKyuLiIhzzz03UqlUpFKpuO222yIi4vXXX4/TTjstevXqFUVFRdGvX7/4/Oc/HxUVFTt8r5dcckl06tQp1q9fv825s88+O3r16hU1NTW1xx555JE45phjomPHjtG5c+f4zGc+E6+88kqd677yla9Ep06d4s0334yTTz45OnfuHOecc069enzrrbfqvKetpVKpuPrqq2tfr1mzJr7xjW/EgAEDorCwMHr06BHHH398zJkzZ4fvNyLi6quvjlQqFQsWLIgvfOEL0bVr19pQbt68efGVr3wlBg0aFEVFRdGrV68477zz4oMPPtjuGG+88UZ85StfiS5dukRJSUmce+652/21/Lj/+I//iHQ6HTfffPMuawEAIBvys90AAADkgtGjR8cdd9wRzz33XBx77LER8VHwcdRRR8VRRx0VFRUVMX/+/DjssMNqzx188MHRvXv3iIg4//zz4/bbb4/TTz89rrjiinjuuefi2muvjVdffXWbvUoWLlwYZ599dvzLv/xLXHDBBXHQQQfFhg0b4tOf/nS88847cdlll0WfPn3ijjvuiCeeeKJe/b/77rsxYsSIWL16dVx44YVx8MEHx9KlS+O+++6L9evXR0FBQaxYsSKOOuqoWL9+fVx22WXRvXv3uP322+Of/umf4r777otTTz11t37tPvzwwzjxxBPjn//5n+PMM8+M++67L7797W/HoYceGieddFIccsgh8e///u8xadKkuPDCC+OYY46JiIijjjoqqqqqYty4cbFp06a49NJLo1evXrF06dL405/+FKtXr46SkpLt3vOss86K8vLy+POf/xxnnHFG7fH169fHQw89FF/5ylciLy8vIiLuuOOOmDBhQowbNy5+9KMfxfr16+OnP/1pbRA2YMCA2uurq6tj3LhxMXr06PjP//zP6NChw273uCMXXXRR3HfffXHJJZdEaWlpfPDBBzFjxox49dVXY9iwYbu8/owzzogDDjggfvjDH0aSJBER8fjjj8eiRYvi3HPPjV69esUrr7wSv/jFL+KVV16JZ599NlKpVJ0xzjzzzBg4cGBce+21MWfOnPjVr34VPXr0iB/96Ec7vO93v/vd+OEPfxg///nP44ILLmjQewYAgGaTAAAAu/TKK68kEZF8//vfT5IkSTZv3px07Ngxuf3225MkSZKePXsm5eXlSZIkSWVlZZKXl5dccMEFSZIkydy5c5OISM4///w6Y37zm99MIiJ54oknao/tu+++SUQkU6ZMqVN74403JhGR3HPPPbXH1q1bl+y///5JRCRPPvnkTvv/8pe/nKTT6WTWrFnbnMtkMkmSJMk3vvGNJCKSp59+uvbcmjVrkoEDByYDBgxIampqkiRJkt/85jdJRCSLFy+uM86TTz65TS9jxoxJIiL5n//5n9pjmzZtSnr16pWcdtpptcdmzZqVRETym9/8ps6YL774YhIRyb333rvT97e999S3b98690iSJLnnnnuSiEimT59e+/66dOlS+3u1xfLly5OSkpI6xydMmJBERHLllVc2uMfFixdv9/0lSZJERDJ58uTa1yUlJcnEiRPr+1ZrTZ48OYmI5Oyzz97m3Pr167c59rvf/a7Or8XWY5x33nl1ak899dSke/fu2/S9pc8rrrgiSafTyW233dbgvgEAoDlZPgsAAOrhkEMOie7du9fuFfLSSy/FunXr4qijjoqIj2Y1bNlsfebMmVFTU1O7dNHDDz8cERGXX355nTGvuOKKiIhtlqcaOHBgjBs3rs6xhx9+OHr37h2nn3567bEOHTrEhRdeuMveM5lMPPDAAzF+/Pg44ogjtjm/ZZbAww8/HCNGjKizD0qnTp3iwgsvjLfeeisWLFiwy3ttT6dOneKLX/xi7euCgoIYMWJELFq0aJfXbpll8eijj9Zr+aYtUqlUnHHGGfHwww/H2rVra4/ffffd0bdv39r3+Pjjj8fq1avj7LPPjvfff7/2Iy8vL0aOHBlPPvnkNmNffPHFjdLjjnTp0iWee+65ePfdd3fr+osuumibY+3bt6/9740bN8b7778fRx55ZETEdpfl+vgYxxxzTHzwwQdRWVlZ53iSJHHJJZfET37yk7jzzjtjwoQJu9UzAAA0F6EIAADUQyqViqOOOqp275C//OUv0aNHj9h///0jom4osuXzlm+8v/3225FOp2trt+jVq1d06dIl3n777TrHBw4cuM3933777dh///23WebooIMO2mXv7733XlRWVsaQIUN2Wvf2229vd7xDDjmk9vzu6Nev3zZ9d+3aNT788MNdXjtw4MC4/PLL41e/+lXstddeMW7cuCgvL9/pfiJbnHXWWbFhw4Z48MEHIyJi7dq18fDDD8cZZ5xR28/rr78eERGf+tSnYu+9967z8dhjj8XKlSvrjJmfnx/9+vVrtB635/rrr4/58+dH//79Y8SIEXH11VfXK0Daup+PW7VqVXz961+Pnj17Rvv27WPvvfeurdten/vss0+d1127do2I2Ob37H/+53+ivLw8br755jj77LPr3SMAAGSLUAQAAOpp9OjRUVFRES+//HLtfiJbHHXUUfH222/H0qVLY8aMGdGnT58YNGhQnes/HgzsyNY/1d8S7eh9bL1x+da27N3xccnf97vYlR//+Mcxb968+M53vhMbNmyIyy67LAYPHhx/+9vfdnrdkUceGQMGDIh77rknIiIeeuih2LBhQ5x11lm1NZlMJiI+2lfk8ccf3+bjj3/8Y50xCwsLI53e9p9Ru+qxIb9mZ555ZixatChuvvnm6NOnT9xwww0xePDgeOSRR3b6frfY3tfPmWeeGb/85S/joosuij/84Q/x2GOPxZQpU+r8Gmytvr9nRx99dPTs2TNuueWWWLVqVb36AwCAbBKKAABAPW2Z+TFjxoz4y1/+EkcffXTtueHDh0dhYWFMmzYtnnvuuTrn9t1338hkMrWzErZYsWJFrF69Ovbdd99d3nvfffeNN998c5tvSi9cuHCX1+69995RXFwc8+fP3+U9tjfeX//619rzEf+YNbB69eo6dbs7kyRi14HRoYceGt/97ndj+vTp8fTTT8fSpUvjZz/72S7HPfPMM2PKlClRWVkZd999dwwYMKB22aiIiP322y8iInr06BFjx47d5uPYY4+t93vYWY8N/TXr3bt3fO1rX4sHHnggFi9eHN27d48f/OAH9e5lax9++GFMnTo1rrzyyrjmmmvi1FNPjeOPP36b0G537L///vHYY4/Fu+++GyeeeGKsWbNmj8cEAICmJBQBAIB6OuKII6KoqCh++9vfxtKlS+vMFCksLIxhw4ZFeXl5rFu3rs6+HCeffHJERNx44411xvuv//qviIj4zGc+s8t7n3zyyfHuu+/GfffdV3ts/fr18Ytf/GKX16bT6TjllFPioYceihdeeGGb81uClpNPPjmef/75mDlzZu25devWxS9+8YsYMGBAlJaWRsQ/goTp06fX1tXU1NSrlx3p2LFjRGwbGlRWVkZ1dXWdY4ceemik0+nYtGnTLsc966yzYtOmTXH77bfHlClT4swzz6xzfty4cVFcXBw//OEPY/Pmzdtc/9577+3yHvXpsbi4OPbaa686v2YREbfeemud1zU1NdssZ9WjR4/o06dPvd7v9myZ9fHxQO3jX4+767DDDouHH344Xn311Rg/fnxs2LChUcYFAICmkJ/tBgAAIFcUFBREWVlZPP3001FYWBjDhw+vc/6oo46KH//4xxERdUKRoUOHxoQJE+IXv/hFrF69OsaMGRPPP/983H777XHKKafEcccdt8t7X3DBBXHLLbfEl7/85Zg9e3b07t077rjjjujQoUO9ev/hD38Yjz32WIwZMyYuvPDCOOSQQ2LZsmVx7733xowZM6JLly5x5ZVXxu9+97s46aST4rLLLotu3brF7bffHosXL47//d//rV02avDgwXHkkUfGVVddFatWrYpu3brF73//+22CgYbYb7/9okuXLvGzn/0sOnfuHB07doyRI0fGSy+9FJdcckmcccYZceCBB0Z1dXXccccdkZeXF6eddtouxx02bFjsv//+8f/+3/+LTZs21Vk6K+KjsOKnP/1pfOlLX4phw4bF5z//+dh7773jnXfeiT//+c9x9NFHxy233LLTezzxxBP16vH888+P6667Ls4///w44ogjYvr06fHaa6/VGWvNmjXRr1+/OP3002Po0KHRqVOn+L//+7+YNWtW7ddWQxUXF8cnP/nJuP7662Pz5s3Rt2/feOyxx2Lx4sW7Nd72HHnkkfHHP/4xTj755Dj99NPjgQceiHbt2jXa+AAA0FiEIgAA0ACjR4+Op59+una5rK0dffTR8eMf/zg6d+4cQ4cOrXPuV7/6VQwaNChuu+22uP/++6NXr15x1VVXxeTJk+t13w4dOsTUqVPj0ksvjZtvvjk6dOgQ55xzTpx00klx4okn7vL6vn37xnPPPRff+9734re//W1UVlZG375946STTqoNVnr27BnPPPNMfPvb346bb745Nm7cGIcddlg89NBD28xm+e1vfxv/8i//Etddd1106dIlvvrVr8Zxxx0Xxx9/fL3ez8e1a9cubr/99rjqqqvioosuiurq6vjNb34TY8aMiXHjxsVDDz0US5cujQ4dOsTQoUPjkUceqbMM1s6cddZZ8YMf/CD233//GDZs2Dbnv/CFL0SfPn3iuuuuixtuuCE2bdoUffv2jWOOOSbOPffcXY4/dOjQevU4adKkeO+99+K+++6Le+65J0466aR45JFHokePHrU1HTp0iK997Wvx2GOPxR/+8IfIZDKx//77x6233hoXX3xxvd7v9tx1111x6aWXRnl5eSRJEieccEI88sgj0adPn90e8+M+9alPxT333BOnnXZafOlLX4q77rpru/uvAABANqWS+u5uCAAAAAAAkMP82A4AAAAAANAmCEUAAAAAAIA2QSgCAAAAAAC0CUIRAAAAAACgTRCKAAAAAAAAbYJQBAAAAAAAaBPys91AQ2UymXj33Xejc+fOkUqlst0OAAAAAACQRUmSxJo1a6JPnz6RTu98LkjOhSLvvvtu9O/fP9ttAAAAAAAALciSJUuiX79+O63JuVCkc+fOEfHRmysuLs5yNwAAAAAAQDZVVlZG//79a/ODncm5UGTLklnFxcVCEQAAAAAAICKiXltu2GgdAAAAAABoE3ImFCkvL4/S0tIoKyvLdisAAAAAAEAOSiVJkmS7iYaorKyMkpKSqKiosHwWAAAAAAC0cQ3JDXJmpggAAAAAAMCeEIoAAAAAAABtglAEAAAAAABoE4QiAAAAAABAmyAUAQAAAAAA2gShCAAAAAAA0CYIRQAAAAAAgDYhP9sN0DhqMkk8v3hVrFyzMXp0LooRA7tFXjqV7bYAAAAAAKDFEIq0AlPmL4trHloQyyo21h7rXVIUk8eXxolDemexMwAAAAAAaDksn5XjpsxfFhffOadOIBIRsbxiY1x855yYMn9ZljoDAAAAAICWJWdCkfLy8igtLY2ysrJst9Ji1GSSuOahBZFs59yWY9c8tCBqMturAAAAAACAtiVnQpGJEyfGggULYtasWdlupcV4fvGqbWaIbC2JiGUVG+P5xauarykAAAAAAGihciYUYVsr1+w4ENmdOgAAAAAAaM2EIjmsR+eiRq0DAAAAAIDWTCiSw0YM7Ba9S4oitYPzqYjoXVIUIwZ2a862AAAAAACgRRKK5LC8dComjy+NiNgmGNnyevL40shL7yg2AQAAAACAtkMokuNOHNI7fvrFYdGrpO4SWb1KiuKnXxwWJw7pnaXOAAAAAACgZcnPdgPsuROH9I7jS3vF84tXxco1G6NH54+WzDJDBAAAAAAA/kEo0krkpVMxar/u2W4DAAAAAABaLMtnAQAAAAAAbYJQBAAAAAAAaBOEIgAAAAAAQJsgFAEAAAAAANoEoQgAAAAAANAmCEUAAAAAAIA2QSgCAAAAAAC0CUIRAAAAAACgTRCKAAAAAAAAbULOhCLl5eVRWloaZWVl2W4FAAAAAADIQakkSZJsN9EQlZWVUVJSEhUVFVFcXJztdgAAAAAAgCxqSG6QMzNFAAAAAAAA9oRQBAAAAAAAaBOEIgAAAAAAQJuQn+0GaBw1mSSeX7wqVq7ZGD06F8WIgd0iL53KdlsAAAAAANBiCEVagSnzl8XVD74Syys31R7rVVwYV//T4DhxSO8sdgYAAAAAAC2H5bNy3JT5y+KiO+fUCUQiIpZXboqL7pwTU+Yvy1JnAAAAAADQsghFclhNJokr//DyTmuu/MPLUZNJmqkjAAAAAABouYQiOezZNz+I1es377Rm9frN8eybHzRTRwAAAAAA0HIJRXLYzEXvN2odAAAAAAC0ZkKRnJZq5DoAAAAAAGi9hCI5bOTAbo1aBwAAAAAArZlQJIel0/WbAVLfOgAAAAAAaM2EIjns/bWb6lU39dUVTdwJAAAAAAC0fEKRHNajc1G96v44992oySRN3A0AAAAAALRsQpEcNmJgt+jWsd0u6z5YVxXPL17VDB0BAAAAAEDLJRTJYXnpVJx6eN961a5cs7GJuwEAAAAAgJZNKJLjxpb2qlddfZfaAgAAAACA1kookuNGDOwWvUuKIrWD86mI6F1SFCMGdmvOtgAAAAAAoMURiuS4vHQqJo8vjYjYJhjZ8nry+NLIS+8oNgEAAAAAgLZBKNIKnDikd/z0i8OiV0ndJbJ6lRTFT784LE4c0jtLnQEAAAAAQMuRn+0GaBwnDukdx5f2iucXr4qVazZGj84fLZllhggAAAAAAHwkK6HIgAEDori4ONLpdHTt2jWefPLJbLQBAAAAAAC0IVmbKfLMM89Ep06dsnX7VmfK/GVxzUMLYlnFxtpjvUuKYvL4UstnAQAAAABA2FOkVZgyf1lcfOecOoFIRMTyio1x8Z1zYsr8ZVnqDAAAAAAAWo4GhyLTp0+P8ePHR58+fSKVSsUDDzywTU15eXkMGDAgioqKYuTIkfH888/XOZ9KpWLMmDFRVlYWv/3tb3e7eSJqMklc89CCSLZzbsuxax5aEDWZ7VUAAAAAAEDb0eBQZN26dTF06NAoLy/f7vm77747Lr/88pg8eXLMmTMnhg4dGuPGjYuVK1fW1syYMSNmz54dDz74YPzwhz+MefPm7f47aOOeX7xqmxkiW0siYlnFxnh+8armawoAAAAAAFqgBociJ510UvzHf/xHnHrqqds9/1//9V9xwQUXxLnnnhulpaXxs5/9LDp06BD//d//XVvTt2/fiIjo3bt3nHzyyTFnzpwd3m/Tpk1RWVlZ54N/WLlmx4HI7tQBAAAAAEBr1ah7ilRVVcXs2bNj7Nix/7hBOh1jx46NmTNnRsRHM03WrFkTERFr166NJ554IgYPHrzDMa+99tooKSmp/ejfv39jtpzzenQuatQ6AAAAAABorRo1FHn//fejpqYmevbsWed4z549Y/ny5RERsWLFihg9enQMHTo0jjzyyPjyl78cZWVlOxzzqquuioqKitqPJUuWNGbLOW/EwG7Ru6QoUjs4n4qI3iVFMWJgt+ZsCwAAAAAAWpz85r7hoEGD4qWXXqp3fWFhYRQWFjZhR7ktL52KyeNL4+I7d7wE2eTxpZGX3lFsAgAAAAAAbUOjzhTZa6+9Ii8vL1asWFHn+IoVK6JXr16NeSu2cuKQ3nHhJwfGx3OPdCriwk8OjBOH9M5OYwAAAAAA0II0aihSUFAQw4cPj6lTp9Yey2QyMXXq1Bg1alRj3oqtTJm/LH4xfXFkkrrHkyTiF9MXx5T5y7LTGAAAAAAAtCANXj5r7dq18cYbb9S+Xrx4ccydOze6desW++yzT1x++eUxYcKEOOKII2LEiBFx4403xrp16+Lcc8/do0bLy8ujvLw8ampq9mic1qYmk8Q1Dy2IZDvnkvhoT5FrHloQx5f2soQWAAAAAABtWipJku19P32Hpk2bFscdd9w2xydMmBC33XZbRETccsstccMNN8Ty5cvj8MMPj5tuuilGjhzZKA1XVlZGSUlJVFRURHFxcaOMmctmvvlBnP3LZ3dZ97sLjoxR+3Vvho4AAAAAAKD5NCQ3aPBMkWOPPTZ2laNccsklcckllzR0aHbDyjUbG7UOAAAAAABaq0bdU4Tm16NzUaPWAQAAAABAayUUyXEjBnaL3iVFsaPdQlIR0bukKEYM7NacbQEAAAAAQIuTM6FIeXl5lJaWRllZWbZbaVHy0qmYPL40ImKbYGTL68njS22yDgAAAABAm9fgjdazzUbr2zdl/rK45qEFsaziH3uH9C4pisnjS+PEIb2z2BkAAAAAADSdJt1onZbpxCG94/jSXvH84lWxcs3G6NH5oyWzzBABAAAAAICP5MzyWexaXjoVIwZ2ix6di2Llmo3x/OJVUZPJqYlAAAAAAADQZMwUaUWmzF8WVz/4Siyv3FR7rFdxYVz9T4MtoQUAAAAAQJtnpkgrMWX+srjozjl1ApGIiOWVm+KiO+fElPnLstQZAAAAAAC0DDkTipSXl0dpaWmUlZVlu5UWpyaTxJV/eHmnNVf+4WVLaQEAAAAA0KblTCgyceLEWLBgQcyaNSvbrbQ4z775Qaxev3mnNavXb45n3/ygmToCAAAAAICWJ2dCEXZs5qL3G7UOAAAAAABaI6FIq5Bq5DoAAAAAAGh9hCKtwKj9ujdqHQAAAAAAtEZCkVbgyEHdo0NB3k5rOhbkxZGDhCIAAAAAALRdOROKlJeXR2lpaZSVlWW7lRapIH/nv5W7Og8AAAAAAK1dznynfOLEibFgwYKYNWtWtltpcZ5fvCpWr9+805oP12+O5xevaqaOAAAAAACg5cmZUIQdW7lmY6PWAQAAAABAayQUaQX26ljYqHUAAAAAANAaCUVag1T9yma9ZfksAAAAAADaLqFIK/D+2k31qrtt5ltRk0mauBsAAAAAAGiZhCKtQI/ORfWqW22zdQAAAAAA2jChSCswYmC36NK+Xb1qbbYOAAAAAEBblTOhSHl5eZSWlkZZWVm2W2lx8tKpOPfoAfWqre+sEgAAAAAAaG1SSZLk1CYTlZWVUVJSEhUVFVFcXJztdlqMqupMHPS9R2Jnv5vpVMRfv39SFOTnTBYGAAAAAAA71ZDcwHfHW4nZb3+400AkIiKTfFQHAAAAAABtkVCklajvXiH2FAEAAAAAoK0SirQS9d0rxJ4iAAAAAAC0VUKRVmL4vl0jVc86AAAAAABoi4QircSsxatiF1uKRETErU++0eS9AAAAAABASyQUaSVmLnq/XnW/mrEoajL1iU8AAAAAAKB1EYq0GvVZPCti7aaaeH7xqibuBQAAAAAAWh6hSCsxar/u9a5duWZjE3YCAAAAAAAtU86EIuXl5VFaWhplZWXZbqVFOnJQ9+hYmFev2h6di5q4GwAAAAAAaHlyJhSZOHFiLFiwIGbNmpXtVlqkvHQqbjjtsF3W9S4pihEDuzVDRwAAAAAA0LLkTCjCrp18WJ/4l08O3OH5VERMHl8aeen67T8CAAAAAACtiVCklbnq5NIYe0iP7Z4bW9ojThzSu5k7AgAAAACAlkEo0spc+/CC+L9XV2733OMLVsa1Dy9o5o4AAAAAAKBlEIq0IlXVmfjl04t3WvPLpxdHVXWmmToCAAAAAICWQyjSitwx863IJDuvySQf1QEAAAAAQFsjFGlF3l61vlHrAAAAAACgNRGKtCL7dutQr7rXl69p4k4AAAAAAKDlEYq0Il8aNaBedc+9tcq+IgAAAAAAtDlCkVakID8dowZ13WWdfUUAAAAAAGiLhCKtzAE9i+tVZ18RAAAAAADampwJRcrLy6O0tDTKysqy3UqL1r9r+0atAwAAAACA1iJnQpGJEyfGggULYtasWdlupUU7uFf9Zoq8+M7qpm0EAAAAAABamJwJRaif99duqlfdI/OX22wdAAAAAIA2RSjSyqxaV1WvuiRstg4AAAAAQNsiFGllunUqrHftU6+/14SdAAAAAABAyyIUaWV6FRfVu/bZRauiJpM0YTcAAAAAANByCEVamREDu0VxUV69aquqM/Hsmx80cUcAAAAAANAyCEVambx0Kr46elC962cuer8JuwEAAAAAgJZDKNIKXfKpA6IwP1XP6vrWAQAAAABAbhOKtEJ56VRcPGa/etWO2q97E3cDAAAAAAAtg1Cklbr00wdGx4Kd7y3StUO7OHKQUAQAAAAAgLZBKNJK5aVT8cUj99lpzREDukZe2vJZAAAAAAC0DUKRVqomk8TdL/xtpzWPL1gZD89b1kwdAQAAAABAdglFWqln3/wgVq/fvMu6y++ZGzWZpBk6AgAAAACA7BKKtFIzF71fr7qN1Zl45vX61QIAAAAAQC4TirRa9d8r5H9f3PkyWwAAAAAA0BoIRVqpUft1r3ft4vfXNWEnAAAAAADQMghFWqkjB3WPovz6zRZ5bcUa+4oAAAAAANDq5UwoUl5eHqWlpVFWVpbtVnJCXjoV/3n60HrVbticiWff/KCJOwIAAAAAgOzKmVBk4sSJsWDBgpg1a1a2W8kZnz28bwzYq0O9auu7MTsAAAAAAOSqnAlF2D0nDu5Vr7rXVqxt4k4AAAAAACC7hCKt3NqN1fWqm/HG+/YVAQAAAACgVROKtHKpVP02W19fVRPPL17VxN0AAAAAAED2CEVauQHd67enSETE4wuWN2EnAAAAAACQXUKRVu5LowbUu/au596xhBYAAAAAAK2WUKSVK8hPx5EDu9ardmN1Jp55/f0m7ggAAAAAALJDKNIGHNiruN61981Z0oSdAAAAAABA9ghF2oB9u9V/X5Glqzc2YScAAAAAAJA9QpE2oCH7ivTr2r7pGgEAAAAAgCwSirQBBfnpOHlIz3rVVlVnmrgbAAAAAADIDqFIG3HzF4ZHXmrXdVNeWS4YAQAAAACgVRKKtBF56VSc+om+u6zLJBF3zHyr6RsCAAAAAIBmJhRpQzoU5ter7u1V65u4EwAAAAAAaH5CkTZk324d6lW3flN1E3cCAAAAAADNTyjShnxh5L71qpvxxgdRk0mauBsAAAAAAGheQpE2ZO6S1fWqW165MZ5fvKppmwEAAAAAgGYmFGlDVq7Z2CS1AAAAAACQC4QibUiPzkX1rt2rY2ETdgIAAAAAAM1PKNKGjBjYLbq0z69XbSaxpwgAAAAAAK2LUKQNyUunYtSgbvWq/c/H/trE3QAAAAAAQPMSirQxhe3qN1Pkpb9Vxv2z/9bE3QAAAAAAQPMRirQxfbu2r3ftv977UlzwP7OasBsAAAAAAGg+WQtF1q9fH/vuu29885vfzFYLbdJR++3VoPrHF6yMH/x5QRN1AwAAAAAAzSdrocgPfvCDOPLII7N1+zbryEHdIy/VsGt++fTiqKrONE1DAAAAAADQTLISirz++uvx17/+NU466aRs3L5Ny0un4nND+zT4us/e9HQTdAMAAAAAAM2nwaHI9OnTY/z48dGnT59IpVLxwAMPbFNTXl4eAwYMiKKiohg5cmQ8//zzdc5/85vfjGuvvXa3m2bPXHf60AZf89rKtbGhqqYJugEAAAAAgObR4FBk3bp1MXTo0CgvL9/u+bvvvjsuv/zymDx5csyZMyeGDh0a48aNi5UrV0ZExB//+Mc48MAD48ADD9yzztltBfnpOPfofRp83YV32HQdAAAAAIDclUqSJNnti1OpuP/+++OUU06pPTZy5MgoKyuLW265JSIiMplM9O/fPy699NK48sor46qrroo777wz8vLyYu3atbF58+a44oorYtKkSdu9x6ZNm2LTpk21rysrK6N///5RUVERxcXFu9s6ETH6usfjb6ur6l2fl4547T9Ojrx0AzclAQAAAACAJlJZWRklJSX1yg0adU+RqqqqmD17dowdO/YfN0inY+zYsTFz5syIiLj22mtjyZIl8dZbb8V//ud/xgUXXLDDQGRLfUlJSe1H//79G7PlNm3GlcdH947t6l1fk4l49s0PmrAjAAAAAABoOo0airz//vtRU1MTPXv2rHO8Z8+esXz58t0a86qrroqKiorajyVLljRGq/zd7O+dEPt0Lap3/X8+9tcm7AYAAAAAAJpOfjZv/pWvfGWXNYWFhVFYWNj0zbRhT37rU7Hfdx6uV+2LSyqiqjoTBfmNmqcBAAAAAECTa9TvbO+1116Rl5cXK1asqHN8xYoV0atXr8a8FY0oL52K04b1qXf97c8sbsJuAAAAAACgaTRqKFJQUBDDhw+PqVOn1h7LZDIxderUGDVq1B6NXV5eHqWlpVFWVranbbIdow/oUe/aX0xf1ISdAAAAAABA02jw8llr166NN954o/b14sWLY+7cudGtW7fYZ5994vLLL48JEybEEUccESNGjIgbb7wx1q1bF+eee+4eNTpx4sSYOHFi7S7yNK5exfXfV+S9tVWxoaom2hfkNWFHAAAAAADQuBocirzwwgtx3HHH1b6+/PLLIyJiwoQJcdttt8VZZ50V7733XkyaNCmWL18ehx9+eEyZMmWbzddpWUYM7BYd26Vj3eZMvep/+PCC+P4phzZxVwAAAAAA0HhSSZIk2W6iIbbMFKmoqIji4uJst9Oq3Pj4wrhx6hu7LoyIju3S8cr3T2rijgAAAAAAYOcakhs06p4i5LZLP31g5KXqV7tucybufv6dpm0IAAAAAAAaUc6EIjZab3p56VT8f2cMrXf9t//wcoy/eXoTdgQAAAAAAI3H8lls46DvPhybquv/ZXFY3+J48NJjmrAjAAAAAADYPstnsUc+ecDeDaqft7Qy/jh3aRN1AwAAAAAAjUMowjb+v7M+0eBrrrjnpajJ5NSkIwAAAAAA2hihCNvoVJQfnQvzGnRNdSaJZ15/v4k6AgAAAACAPScUYbtmXjW2wdfc/OTrTdAJAAAAAAA0jpwJRcrLy6O0tDTKysqy3Uqb0KkoP4b06dSga+a8s9oSWgAAAAAAtFipJEly6rvYDdlFnj13xPcfi/fXba53/W+/OjKOPmCvJuwIAAAAAAD+oSG5Qc7MFCE7XvjeCTHmgO71rv/Lm+81YTcAAAAAALD7hCLs0u1fPTJ6dCqoV+3/PLO4ibsBAAAAAIDdIxShXo7av35LYq2tSmL0df/XxN0AAAAAAEDDCUWol75d29e79m+rN8Unr3+iCbsBAAAAAICGE4pQL0ft17DN099ZtSH+/aFXmqgbAAAAAABouJwJRcrLy6O0tDTKysqy3UqbdOSg7tEuL9Wga/77L2/FhqqaJuoIAAAAAAAaJpUkSZLtJhqisrIySkpKoqKiIoqLi7PdTpvyX4/+NW568s0GX3frFz4RJx/Wpwk6AgAAAACgrWtIbpAzM0XIvq8ff9BufcF87a4X49qHFzR6PwAAAAAA0BBCEeotL52Kmz5/+G5d+/Ppi+PhecsatyEAAAAAAGgAoQgN8tnD+8bxpT1269pv3ftS1GRyarU2AAAAAABaEaEIDfbLL5fFyUN6Nvi6dZtr4tK7ZjdBRwAAAAAAsGtCEXbLzV8YHh3aNfzL5+H5K+L822c1QUcAAAAAALBzQhF2S146Ff95xtDduvb/Xl0Z1zw0v5E7AgAAAACAncuZUKS8vDxKS0ujrKws263wdycf1if+5ZMDd+va3/zl7Zj8oGAEAAAAAIDmk0qSJKd2vq6srIySkpKoqKiI4uLibLdDRDw8b1l87a45u3XtkD6d4k+XjWnkjgAAAAAAaCsakhvkzEwRWq6TD+sdr/77ibt17fx318b4m59u5I4AAAAAAGBbQhEaRfuCvDj36H1269qXl1bG9/+0oJE7AgAAAACAuoQiNJrJ4w+N/l0Ld+vaX89YHFXVmUbuCAAAAAAA/kEoQqN6+ttjo1+X3QtGRl/7f43cDQAAAAAA/INQhEY348qxcUivjg2+buW6zTHsmilN0BEAAAAAAAhFaCKPfOPY3ZoxsmpDTQz7/mNN0BEAAAAAAG2dUIQmM+PKsdG/a1GDr1u1bnN89uanm6AjAAAAAADaspwJRcrLy6O0tDTKysqy3QoN8PS3Px2H9y9u8HXzl1bGH+cubYKOAAAAAABoq1JJkiTZbqIhKisro6SkJCoqKqK4uOHfbKf51WSSOPi7D8fmTMOvffXfT4z2BXmN3xQAAAAAAK1CQ3KDnJkpQu7KS6fi/zvz8N269pBJU+Jf7ng+ajI5ld0BAAAAANACCUVoFp89vG+MPWTv3br20Vfei/2/83BMmb+skbsCAAAAAKAtEYrQbH41YUR8+uDdC0aSiLjozjmCEQAAAAAAdptQhGb166+MiK+OHrjb13/tzjmW0gIAAAAAYLcIRWh23/tsaVx67H67dW0mIo78wWON2xAAAAAAAG2CUISs+MYJB0VR3u5d+9666jj5J081bkMAAAAAALR6QhGyIi+dihvPHrbb1y9YtjZOvnFa4zUEAAAAAECrJxQha04c0jt+9sU9CEaWr4vDJj8SVdWZRuwKAAAAAIDWSihCVp04pHe8+cOTo0engt26vnJTJg787iMx+cF5jdwZAAAAAACtjVCErMtLp+L57x4fPz596G6PcfszS+LA7/w5NlTVNGJnAAAAAAC0JkIRWozTjugXt3z+E7t9fVUm4pBJU+LU8qejJpM0YmcAAAAAALQGOROKlJeXR2lpaZSVlWW7FZrQZw/vExccM2CPxnhxSWXs952HY8r8ZY3TFAAAAAAArUIqSZKc+pH6ysrKKCkpiYqKiiguLs52OzSRax6aH7/5y9t7PM7PvjgsThzSuxE6AgAAAACgJWpIbpAzM0VoWyaPHxKfOmivPR7nojvnRFV1phE6AgAAAAAg1wlFaLH++9yRcVjfPZ8NdOB3H7GUFgAAAAAAQhFatgcvPSa+OnrgHo9z0Z1zBCMAAAAAAG2cUIQW73ufLY3X/uOk6NI+f4/GuejOOXHrk29YTgsAAAAAoI2y0To55f45S+Nf75m7x+OcOKRHlH/hiMhLp/a8KQAAAAAAssZG67Rapw7rG2/+8OQY0L3DHo0zZf7K2O87D8eDc/7WSJ0BAAAAANDSCUXIOXnpVEz71nHx/505dI/Huuyel2LMj/4vajI5NWEKAAAAAIDdIBQhZ506rF/cdObhezzO2x9uiv2+83D8eMpfhSMAAAAAAK2YUISc9k/D+sbxpT0aZaybp71pSS0AAAAAgFZMKELO++WXy+Krowc02niX3fNSnHzjk402HgAAAAAALYNQhFbhe58dHDef/YlGG2/B8vVx8P/7s+W0AAAAAABaEaEIrcb4oX3iZ18c1mjjbawJy2kBAAAAALQiQhFalROH9I43f3hyDOzeodHGtJwWAAAAAEDrIBSh1clLp+LJbx0X868eF/27FDXKmAuWr48DvvPnqKrONMp4AAAAAAA0P6EIrVanovx4+spPx2v/cVLsv/eezxzZnIk48LuPxLUPL2iE7gAAAAAAaG45E4qUl5dHaWlplJWVZbsVckxBfjr+74rj4tV/PzHapVN7PN7Ppy+O7/9pfiN0BgAAAABAc0olSZJku4mGqKysjJKSkqioqIji4uJst0MOOvc3z8eTC9/b43G+NGqf+P7nDm2EjgAAAAAA2F0NyQ2EIrRJG6pqYuyPp8XSio17NE6HdqmY/b1x0b4gr5E6AwAAAACgIYQiUE8bqmpi8KQpsafbp3fv2C6e+tanolNRfqP0BQAAAABA/TQkN8iZPUWgKbQvyItF130muncs2KNxPli3OYZc/Wjsf9Wf49Yn34iq6j2NWQAAAAAAaGxCEYiI2d87Pj518N57PE51EnH9owvjwO8+EhfdOStqMjk1EQsAAAAAoFWzfBZsZUNVTQyZNCVqGnHMU4f2iR+dMTQK8mWQAAAAAACNzZ4isIdG/eCxWLZmc6OOWZifilM/0S8mjx9sY3YAAAAAgEYiFIFGMOz7j8WqdY0bjGyx317t45FvHGv2CAAAAADAHrLROjSCOd87Ifp3LWqSsd98f0Mc+N1H4oyfzbApOwAAAABAMzFTBHbhmgdfid8881aT3qNjYV5MPHb/OP+YQWaPAAAAAAA0gOWzoJE9PG9ZXPK7OZFphj8tHdql43OH941J9h4BAAAAANgloQg0gZpMEjc+tjBumfZmNNcfmlRE7N+jY5w2rF+cN9osEgAAAACAjxOKQBOqySQxY+F78fV7XozVG6qb9d7jBu8dt55TFnnpVLPeFwAAAACgpRKKQDPZUFUTnyt/Ol5bsa5Z7zvxk4Pi8hMPFo4AAAAAAG2eUASaWVV1Jk7+yVPxxnvrm/W+B/boGP/vM6Ux+oC9BSQAAAAAQJskFIEs2VBVE6eWz4i/rljb7Pe+9Nj94hsnHCQcAQAAAADaFKEIZFlVdSZ+PePNuHfWO7Hog43Neu/R+3WPX04oi/YFec16XwAAAACAbBCKQAtz/5yl8a/3zG3We+alIsYcuHfcdPaw6FSU36z3BgAAAABoLkIRaIFqMklMvHN2TFmwotnvnYqIIwd1i4vG7Gf/EQAAAACgVRGKQAu2ZWmt22YsjhVrN2elhyF9iuP3F44ygwQAAAAAyHlCEcgRNZkkpr+6Mr774MuxtGJTs9+/Y7tU3PrFI8weAQAAAAByllAEclBNJokZC9+LW6e9FrPfqYjqZv6T2a1Duzj/mEFx/jGDoiA/3bw3BwAAAADYTUIRaAWqqjNxzq9mxqy3Vjf7vbu0z4+LxuwX540WkAAAAAAALVuLDkVWr14dY8eOjerq6qiuro6vf/3rccEFF9T7eqEIbU02w5GIiLIBXeK3548SjgAAAAAALVKLDkVqampi06ZN0aFDh1i3bl0MGTIkXnjhhejevXu9rheK0FZVVWfil0+/ET+Z+kZUNffaWhHRoV06Pnd435g0fnC0L8hr9vsDAAAAAGxPiw5FtrZq1aoYNmxYvPDCC7HXXnvV6xqhCERsqKqJsT+eFksrNmbl/ulUxMiB3eKiMfvZpB0AAAAAyKqG5AYNXg9n+vTpMX78+OjTp0+kUql44IEHtqkpLy+PAQMGRFFRUYwcOTKef/75OudXr14dQ4cOjX79+sW3vvWtegciwEfaF+TFX676dLz67yfG0ft1a/b7Z5KImYtWxYTfzIr9vvNwHPzdR+LsX8yMpxaujJpMTm1TBAAAAAC0IQ0ORdatWxdDhw6N8vLy7Z6/++674/LLL4/JkyfHnDlzYujQoTFu3LhYuXJlbU2XLl3ipZdeisWLF8ddd90VK1as2P13AG1Y+4K8+O0Fo+LNH54cv/nSEXFIz46RjTkbG6szdUKSQ773SJz3m+dj7cbqLHQDAAAAALB9e7R8ViqVivvvvz9OOeWU2mMjR46MsrKyuOWWWyIiIpPJRP/+/ePSSy+NK6+8cpsxvva1r8WnPvWpOP3007d7j02bNsWmTZtqX1dWVkb//v0tnwU7saGqJiY9OC/un/1uZGH7kW30Ki6M4w7qYT8SAAAAAKDRNWT5rPzGvHFVVVXMnj07rrrqqtpj6XQ6xo4dGzNnzoyIiBUrVkSHDh2ic+fOUVFREdOnT4+LL754h2Nee+21cc011zRmm9DqtS/IixtO/0TccPonYkNVTVz90Mtx3wtLoyZLAcnyyk3xu1lL4nezlkRERCoi8tOp6FFcGOeM3DfOP2ZQFOQ3eOIaAAAAAECDNGoo8v7770dNTU307NmzzvGePXvGX//614iIePvtt+PCCy+MJEkiSZK49NJL49BDD93hmFdddVVcfvnlta+3zBQB6qd9QV786LTD40enHR5rN1bHmOufiA/Wb85qT0lEbM4ksXT1xrj+0YVx/aMLIxURXTq0i3GDe8VkM0oAAAAAgCbQqKFIfYwYMSLmzp1b7/rCwsIoLCxsuoagDelUlB+zJ51QO3vk4ZeWxZqqTLbbioiPgpIP12+O389aEr/fakZJYbt09O/aPk4b1i/OG21GCQAAAACw+xo1FNlrr70iLy9vm43TV6xYEb169WrMWwF7YOvZIzWZJKa/ujKu+N+5sWp9y9oYPYmIjZsz8frKdXHdlIVx3ZSFERFRlJ+K7p0svQUAAAAANEyjfiexoKAghg8fHlOnTq09lslkYurUqTFq1KjGvBXQSPLSqThucM+YM2lcvPYfJ8WIAV2y3dIubaz+x9JbB373kRh01Z/j6Oumxq1PvhFV1S1j5gsAAAAA0PI0eKbI2rVr44033qh9vXjx4pg7d25069Yt9tlnn7j88stjwoQJccQRR8SIESPixhtvjHXr1sW55567R42Wl5dHeXl51NTU7NE4wI4V5KfjnouOjqrqTPzy6Tfil9MXxeoNLf/PXCaJOvuTRHy09FY6HdGpMD+G79M1bjp7WHQqavYVAwEAAACAFiSVJEnSkAumTZsWxx133DbHJ0yYELfddltERNxyyy1xww03xPLly+Pwww+Pm266KUaOHNkoDVdWVkZJSUlUVFREcXFxo4wJ7FhNJokZC9+LW6e9FrPerohcnoeRjogzy/rbyB0AAAAAWpGG5AYNDkWyTSgC2bVlk/apryyP99e3/FkkO9OruDCOO6hHTBKSAAAAAEDOEooAzWZDVU1MenBePPry8qjclMvzSCLapSM6FbWLcYN7mU0CAAAAADlCKAJkzZaQ5E9zl8WG6px6vOxQu7xUdO9YYFYJAAAAALRArTIU2Xqj9ddee00oAjlg6/1I5i2tiI3VETnxwNmFdCpi5MBucdGY/WL0AXtHXjqV7ZYAAAAAoM1qlaHIFmaKQO6rySQx/dWVcf1jr8ailetiU049hbaVioiOhXlxaN8SQQkAAAAANDOhCJBz1m6sjsvueiGeW/RBrKvOdjd7Li8VsXdnG7kDAAAAQFMTigCtQlV1Jn494824d9Y7seTDjbE5t/dxj7xURFG7vBgxsFvcfPaw6FSUn+2WAAAAACDnCUWAVmvrpbcWrlgXOZ6TRCo+2melXTqiU1G7GDe4V0w2swQAAAAA6q1VhiI2Wge25+OzSaozrWMz94iPZpYUtxeUAAAAAMDOtMpQZAszRYD62lBVE5MenBf/+8K7OT+jZGt5EdGuXTr6d20fpw3rF+eNHhQF+elstwUAAAAAWSEUAfiYtRur49LfzooZb6yKzTn11Ku/vHREp8L8GL5P17jJniUAAAAAtBFCEYCdqMkkMWPhe3HrtNdi3tKK2Fjdepbc+rgte5ZEWI4LAAAAgNZJKAKwG9ZurI7L7nohnlv0QayrznY3zaNTYV4c2rckLhqzX4w+YO/IS6ey3RIAAAAANIhQBKCRbD2rZNbbFa1qb5LtSUdE5/aW4AIAAAAgdwhFAJrIhqqauPqhl2PqK8tj1fqaVh+SbNEuHdGpyNJbAAAAALQ8rTIUKS8vj/Ly8qipqYnXXntNKAK0GG01KLH0FgAAAAAtQasMRbYwUwRo6drSRu4fV9QuHf27to/ThvWL80YPioL8dLZbAgAAAKCVE4oAtEAbqmpi0oPz4tGXl8eaTZlIIiIVrT8wSUdEXl4quncsiOMO6hGTLL8FAAAAQCMSigDkmLUbq+Oyu16I5xZ9EOuqs91N80inItq3S8e+3TvGN084OMYcZAkuAAAAABpOKALQCmwdlKz/e1CSUw/s3ZSOiA72KwEAAACgnoQiAK1YVXUmfj3jzbh31jvxzqqNUZ1TT/Hdl4qIdDqiU2F+DN+na9x09rDoVJSf7bYAAAAAyDKhCEAb1Fb3LElFRGF+Krp3KoxzRu4b5x9jg3cAAACAtkQoAsA22tq+JcISAAAAgLahVYYi5eXlUV5eHjU1NfHaa68JRQD20Iaqmrj6oZdj6ivL48P1NVGT7YaayZbZM1s+t0tHdCpqF+MG94rJ4wdH+4K87DYIAAAAQIO0ylBkCzNFAJpOW93cfXvMNAEAAADIDUIRABpVW1t6a1fSEVGQn45Be3eMb55wcIw5aO/IS6ey3RYAAABAmyQUAaBJbW9TdyKK2qWjf9f2cdqwfnHeaDNLAAAAAJqDUASAZldVnYlfz3gz7p31Tiz5cGNUZ9ru0ls7Yv8SAAAAgMYnFAGgxajJJDH91ZVx/WOvxqKV66IqEZZsLS8d0akwP4bv0zVuOntYdCrKz3ZLAAAAADlFKAJAi1dVnYlfPv1G3DnzrXivcnPYqqSuVHwUHm39OZWKaN8uHft2t5cJAAAAwBZCEQBy1oaqmrj6oZdj6ivL48P1NZEJM0t2Ji8VsXfnwjjuoB4xyZJcAAAAQBskFAGgVdqywfuUectiTVVO/e+rWZlVAgAAALQlrTIUKS8vj/Ly8qipqYnXXntNKAJARNizpKEEJgAAAEBr0ypDkS3MFAGgPraEJT96dEG8uXJ9bP778Y/v1cG20hGRl5eK7h0LLMsFAAAAtHhCEQCop7Ubq+Oyu16I5xZ9EOvs9r5TW4KkdumITkXtYtzgXjFZYAIAAABkmVAEAPbQ9pblijC7ZEc+PgMnPxVRVJAXh/YtiYvG7BejD7BEFwAAANA0hCIA0ISqqjPxy6ffiDtnvhXvVW4OE0wa5uMBSjoiCvLTMWhve5wAAAAADScUAYBmVlWdiV/PeDPunfVOLPlwY1RnzCppDB8PUFIRkZ9ORY/iwjhn5L5x/jGDoiA/ndUeAQAAgOwSigBAC7T1/iXrq4Umjc0MFAAAAGibhCIAkCM2VNXEpAfnxZR5y2JN1T/+l/zxb/DTuLb+9Y2IKMxPRfdOZp8AAABALhKKAEArtGUvkzueWRwr1lQLS5qYTeMBAAAgNwhFAKANqMkkMWPhe3HrtNdi3tKK2GhJrmaXiohUKqJ9u3Ts290yXQAAAJANQhEAaOM2VNXE1Q+9HFNfWR4frq+JTAhMsmXrPU7y8lLRvWNBHHdQj5g0fnC0L8jLcncAAACQ+4QiAMAObVmG686Zb8V7lZujOtsNUevje8ls+dwuHdGpqF2MG9wrJgtTAAAAoA6hCACw2yzLlRs+HpyYiQIAAEBb1SpDkfLy8igvL4+ampp47bXXhCIAkAU7Ckw+/g16WpaP//7YPB4AAIDWpFWGIluYKQIALd+GqpqY9OC8ePTl5bFmU2a7wYkApWXa+vfHJvIAAADkAqEIAJAzPr7HSc3fjwtQWq6P/77kRUS7duno37V9nDasX5w3elAU5Kez2iMAAABth1AEAGiVajJJTH91ZVz/2KuxaOW6qPr732IEKC3XzmYI5aUiitvbQB4AAIA9IxQBANq8+sxAoWWyeTwAAAANIRQBAKinqupM/HrGm3HvrHdiyYcbozojOMkF9dmjxr4oAAAAbYNQBACgkexs03hyX34qoqggLw7tWxIXjdkvRh8gNAEAAMg1QhEAgGby8WW6qj92XoCSm+x9AgAAkDuEIgAALUxNJokZC9+LW6e9FvOWVsTG6u0v92QmSu7Y1dJdW85ZwgsAAKBpCUUAAFqJHYUpApTWY3v7o0REFOanonunwjhn5L5x/jGDoiA/nbUeAQAAWjKhCABAG1OTSWL6qyvj+sdejUUr10XV3/+GJ0BpXeq7wXw6HdGpMD+G79M1bjp7WHQqym/mTgEAAJqPUAQAgB3a2ebxgpPWa1fLfAlSAACAXCUUAQBgj23ZRP6OZxbHyjXVkfn7cQFK27SjWSp5qYiidnkxYmC3uFmYAgAAZIFQBACAZrMlPLlz5lvxXuXmqPn78e19A12Q0jbUd5kve6gAAACNQSgCAECLt3ZjdVx21wvx3KIPYn31R8cEJ2zPNrNTIqJdu3T079o+ThvWL84bLUQBAIC2TCgCAEDOa8jm8YIUttje14b9UgAAoHUTigAAQGy7tFd1thuiRWnoMl9bPqdSEe3bpWPf7h3jmyccHGMO2jvy0qkAAACyo1WGIuXl5VFeXh41NTXx2muvCUUAANhjG6pq4uqHXo6pryyPD9fXRCbMPGH37Shk2VnYkp+KKCrIi0P7lsRFY/aL0QcIWAAAoKFaZSiyhZkiAAA0p13tfbKzb3jDnthRuJKOiLy8VHTvWBDHHdQjJo0fHO0L8rLYKQAAZJdQBAAAWohdLeFlXxQak1kqAAC0RUIRAADIYVXVmfj1jDfj3lnvxJIPN0Z1xgbzZEd9vubapSM6FbWLcYN7xWSzVgAAyAKhCAAAtGEbqmpi0oPz4tGXl8eaTRnLfJEV9dm4Pi8i2rVLR/+u7eO0Yf3ivNGDoiA/3cydAgCQ64QiAABAg2wvSNmaWSo0N3uoAABQX0IRAACgWexsVorN6Gku9fmas4cKAEDrJRQBAAByRk0miemvrozrH3s1Fq1cF5s+9i8Us1NoSvVZ5mvrz+l0RKfC/Bi+T9e46exh0akov1n7BQBgW0IRAACg1dp6I/p3Vm2M6q3+RSNAIVsaMkNqy2ezVwAAGodQBAAA4O/WbqyOy+56IZ5b9EGsr/7oWEOX+RKy0Nx29fWZZ8YKAEAtoQgAAEATqarOxC+ffiPueGZxrFxTHZm/H9/Zkkv2VKE5NCTg2/pzfjoVPYoL45yR+8b5xwyKgvx0M3YNALDnhCIAAAA5YENVTVz90Msx9ZXlsWp9TW3AElG/cAWaUkNClnRE5OWlonvHgjjuoB4xafzgaF+Q16z9AgBtl1AEAACglfr4xvRVf/8XnVkqtFT1nUWViojC/FR072TWCgDQMEIRAAAAdmrrWSof/n2WSn2XXIJsashSde3SEZ2K2sW4wb1istkrANBqCUUAAABoFrvayH5rwhVaiobsu2JpMABo+YQiAAAAtHhV1Zn49Yw3495Z78Q7qzZG9Vb/OhWckAt2d3P7JCLyUhFF7fJixMBucfPZw6JTUX7zNQ4ArYxQBAAAgFahJpPEjIXvxa3TXot5SytiY7Vlvmi9GrIv0M5qUxFR2C4d/bu2j9OG9YvzRtufBYDWTSgCAAAAW9nRHioN+aYz5LrdDVkiIvLSEZ0K82P4Pl3jJjNbAGhhhCIAAADQBHa1h0pDfsIfcl19vt7zUxFFBXlxaN+SuGjMfjH6gL0jL51q7lYBaOWEIgAAANDCbaiqiUkPzotHX14eazZldhquCFJozXYnVGyXjuhU1C7GDe4Vk8cPjvYFec3ZMgAtjFAEAAAAWrH6BCpbE7LQljQkXNxVbToiCvLTMWjvjvHNEw6OMQeZ6QLQEglFAAAAgF2qySQx/dWVcf1jr8aileui6u/fIbAUGOyakAWg5RCKAAAAAM2mJpPEjIXvxa3TXouX/lYRG2v+cc6SYFA/ezKj5eO16XREp8L8GL5P17jp7GHRqSi/SXsHyDahCAAAAJBTdjZrZWtCFthzu7PEmH1cgJasRYciS5YsiS996UuxcuXKyM/Pj+9973txxhln1Pt6oQgAAABQH2s3Vsdld70Qzy36INZXf3TMvivQdBpjP5cd1aRSEe3bpWPf7pYeA7bVokORZcuWxYoVK+Lwww+P5cuXx/Dhw+O1116Ljh071ut6oQgAAADQXBpr3xVhCzStPQlk0hGRl5eK7h0L4riDesQkM2Eg57ToUOTjhg4dGn/605+if//+9aoXigAAAAC5bkNVTUx6cF48+vLyWLMps8c/XS9sgaaxp38uIyIK81PRvVNhnDNy3zj/mEFRkJ9ult6hLWnSUGT69Olxww03xOzZs2PZsmVx//33xymnnFKnpry8PG644YZYvnx5DB06NG6++eYYMWLENmPNnj07JkyYEPPnz6/3/YUiAAAAADtWVZ2JX894M+6d9U4s+XBjVGeELNDSNHYI2tCaCGENrUtDcoP8hg6+bt26GDp0aJx33nnxz//8z9ucv/vuu+Pyyy+Pn/3sZzFy5Mi48cYbY9y4cbFw4cLo0aNHbd2qVaviy1/+cvzyl79saAsAAAAA7EBBfjouPvaAuPjYAxptzPrMbNmaUAV2LtnB5/rUNKR2ZzUbq5NYunpjXP/owrj+0YV1ancnkMmLiHbt0tG/a/s4bVi/OG+0oIWWaY+Wz0qlUtvMFBk5cmSUlZXFLbfcEhERmUwm+vfvH5deemlceeWVERGxadOmOP744+OCCy6IL33pSzu9x6ZNm2LTpk21rysrK6N///5migAAAAC0cBuqauLqh16Oqa8sjw/X10Qm9vwn3IUtkHt2d9ZLOh3RqTA/hu/TNW46e1h0Kmrwz/jTRjTpTJGdqaqqitmzZ8dVV11VeyydTsfYsWNj5syZERGRJEl85StfiU996lO7DEQiIq699tq45pprGrNNAAAAAJpB+4K8+NFph0ec1rjjrt1YHZfd9UI8t+iDWFdd91xjLDkkfIHGtbuzXmoyERUbquOJhe/FkKsf3aa2MZcYa5eXiu4dC+K4g3rEpPGDo31BXgPeIbmkUWeKvPvuu9G3b9945plnYtSoUbV1//Zv/xZPPfVUPPfcczFjxoz45Cc/GYcddljt+TvuuCMOPfTQ7d7DTBEAAAAAsqGqOhO/fPqNuHPmW/Fe5eao+ftxIQu0DU2954v9XRpP1maK1Mfo0aMjk8nUu76wsDAKCwubsCMAAAAA2FZBfjomHndgTDzuwEYZrz4hi7AFWo6m3vNlZ/u7bO/PeToiOhTmxaF9S+KiMfvF6AP2jrz0lmiF+mrUUGSvvfaKvLy8WLFiRZ3jK1asiF69ejXmrQAAAAAgpzR2yLIjG6pqYtKD8+LRl5fHmk2ZRvkJd6B5bS9kyUTE2k01MXPRqpi5aFUU5qfjJ58/PE4c0jsLHeauRg1FCgoKYvjw4TF16tTaJbUymUxMnTo1Lrnkksa8FQAAAACwHe0L8uKG0z8RN5zeeGNuvY/L+r/v49IUywltrwbYvk3Vmbjozjnxsy8OE4w0QINDkbVr18Ybb7xR+3rx4sUxd+7c6NatW+yzzz5x+eWXx4QJE+KII46IESNGxI033hjr1q2Lc889d48aLS8vj/Ly8qipqdl1MQAAAADQaDoV5cd/n3dk1u7/8aXHqj92fk8CGch1Vz+4II4v7WUprXpq8Ebr06ZNi+OOO26b4xMmTIjbbrstIiJuueWWuOGGG2L58uVx+OGHx0033RQjR45slIYbsmEKAAAAAMDO1GSSmLHwvbh12mvx0t8qYuNWP5PdWLNeoKn97oIjY9R+3bPdRtY0JDdocCiSbUIRAAAAACAXVFVn4tcz3ox7Z70TSz7cGNWZPV9izHJkbM9PPn94fO7wvtluI2sakhs06p4iAAAAAAB8pCA/HRcfe0BcfOwB2W6ljppMEtNfXRnXP/ZqLFq5LjZ9LC3ZnUCG7OrRuSjbLeQMoQgAAAAAQBuSl07FcYN7xnGDezbamFvPinln1cao3iopEbI0rV7FRTFiYLdst5EzciYUsdE6AAAAAEDL1BSzYjZU1cSkB+fFoy8vjzWbMk2+xFiuuvqfSm2y3gD2FAEAAAAAoM2qySQxY+F7ceu012Le0orYWN38e77sjsL8dPzk84fHiUN6N8Jouc2eIgAAAAAAUA956VSMOaRHjDmkR7Pfe+tlx5Z8uDGqMzsOUNIR0aEwLw7tWxIXjdkvRh+wtxkiu8FMEQAAAAAAIGc1JDdIN1NPAAAAAAAAWSUUAQAAAAAA2oScCUXKy8ujtLQ0ysrKst0KAAAAAACQg+wpAgAAAAAA5Cx7igAAAAAAAHyMUAQAAAAAAGgThCIAAAAAAECbIBQBAAAAAADahJwJRcrLy6O0tDTKysqy3QoAAAAAAJCDUkmSJNluoiEqKiqiS5cusWTJkl3uIg8AAAAAALRulZWV0b9//1i9enWUlJTstDa/mXpqNGvWrImIiP79+2e5EwAAAAAAoKVYs2bNLkORnJspkslk4t13343OnTtHKpXKdjstypY0zCwaoCl4xgBNzXMGaGqeM0BT85wBmprnzPYlSRJr1qyJPn36RDq9811Dcm6mSDqdjn79+mW7jRatuLjYHwigyXjGAE3NcwZoap4zQFPznAGamufMtnY1Q2SLnNloHQAAAAAAYE8IRQAAAAAAgDZBKNKKFBYWxuTJk6OwsDDbrQCtkGcM0NQ8Z4Cm5jkDNDXPGaCpec7suZzbaB0AAAAAAGB3mCkCAAAAAAC0CUIRAAAAAACgTRCKAAAAAAAAbYJQBAAAAAAAaBOEIq1EeXl5DBgwIIqKimLkyJHx/PPPZ7sloAWaPn16jB8/Pvr06ROpVCoeeOCBOueTJIlJkyZF7969o3379jF27Nh4/fXX69SsWrUqzjnnnCguLo4uXbrEV7/61Vi7dm2dmnnz5sUxxxwTRUVF0b9//7j++uub+q0BLcS1114bZWVl0blz5+jRo0eccsopsXDhwjo1GzdujIkTJ0b37t2jU6dOcdppp8WKFSvq1Lzzzjvxmc98Jjp06BA9evSIb33rW1FdXV2nZtq0aTFs2LAoLCyM/fffP2677bamfntAC/DTn/40DjvssCguLo7i4uIYNWpUPPLII7XnPWOAxnTddddFKpWKb3zjG7XHPGeAPXH11VdHKpWq83HwwQfXnveMaXpCkVbg7rvvjssvvzwmT54cc+bMiaFDh8a4ceNi5cqV2W4NaGHWrVsXQ4cOjfLy8u2ev/766+Omm26Kn/3sZ/Hcc89Fx44dY9y4cbFx48bamnPOOSdeeeWVePzxx+NPf/pTTJ8+PS688MLa85WVlXHCCSfEvvvuG7Nnz44bbrghrr766vjFL37R5O8PyL6nnnoqJk6cGM8++2w8/vjjsXnz5jjhhBNi3bp1tTX/+q//Gg899FDce++98dRTT8W7774b//zP/1x7vqamJj7zmc9EVVVVPPPMM3H77bfHbbfdFpMmTaqtWbx4cXzmM5+J4447LubOnRvf+MY34vzzz49HH320Wd8v0Pz69esX1113XcyePTteeOGF+NSnPhWf+9zn4pVXXokIzxig8cyaNSt+/vOfx2GHHVbnuOcMsKcGDx4cy5Ytq/2YMWNG7TnPmGaQkPNGjBiRTJw4sfZ1TU1N0qdPn+Taa6/NYldASxcRyf3331/7OpPJJL169UpuuOGG2mOrV69OCgsLk9/97ndJkiTJggULkohIZs2aVVvzyCOPJKlUKlm6dGmSJEly6623Jl27dk02bdpUW/Ptb387Oeigg5r4HQEt0cqVK5OISJ566qkkST56rrRr1y659957a2teffXVJCKSmTNnJkmSJA8//HCSTqeT5cuX19b89Kc/TYqLi2ufLf/2b/+WDB48uM69zjrrrGTcuHFN/ZaAFqhr167Jr371K88YoNGsWbMmOeCAA5LHH388GTNmTPL1r389SRJ/lwH23OTJk5OhQ4du95xnTPMwUyTHVVVVxezZs2Ps2LG1x9LpdIwdOzZmzpyZxc6AXLN48eJYvnx5nedJSUlJjBw5svZ5MnPmzOjSpUscccQRtTVjx46NdDodzz33XG3NJz/5ySgoKKitGTduXCxcuDA+/PDDZno3QEtRUVERERHdunWLiIjZs2fH5s2b6zxrDj744Nhnn33qPGsOPfTQ6NmzZ23NuHHjorKysvYnwWfOnFlnjC01/v4DbUtNTU38/ve/j3Xr1sWoUaM8Y4BGM3HixPjMZz6zzbPAcwZoDK+//nr06dMnBg0aFOecc0688847EeEZ01yEIjnu/fffj5qamjp/CCIievbsGcuXL89SV0Au2vLM2NnzZPny5dGjR4865/Pz86Nbt251arY3xtb3ANqGTCYT3/jGN+Loo4+OIUOGRMRHz4GCgoLo0qVLndqPP2t29RzZUU1lZWVs2LChKd4O0IK8/PLL0alTpygsLIyLLroo7r///igtLfWMARrF73//+5gzZ05ce+2125zznAH21MiRI+O2226LKVOmxE9/+tNYvHhxHHPMMbFmzRrPmGaSn+0GAABonSZOnBjz58+vsz4uQGM46KCDYu7cuVFRURH33XdfTJgwIZ566qlstwW0AkuWLImvf/3r8fjjj0dRUVG22wFaoZNOOqn2vw877LAYOXJk7LvvvnHPPfdE+/bts9hZ22GmSI7ba6+9Ii8vL1asWFHn+IoVK6JXr15Z6grIRVueGTt7nvTq1StWrlxZ53x1dXWsWrWqTs32xtj6HkDrd8kll8Sf/vSnePLJJ6Nfv361x3v16hVVVVWxevXqOvUff9bs6jmyo5ri4mL/kIA2oKCgIPbff/8YPnx4XHvttTF06ND4yU9+4hkD7LHZs2fHypUrY9iwYZGfnx/5+fnx1FNPxU033RT5+fnRs2dPzxmgUXXp0iUOPPDAeOONN/xdppkIRXJcQUFBDB8+PKZOnVp7LJPJxNSpU2PUqFFZ7AzINQMHDoxevXrVeZ5UVlbGc889V/s8GTVqVKxevTpmz55dW/PEE09EJpOJkSNH1tZMnz49Nm/eXFvz+OOPx0EHHRRdu3ZtpncDZEuSJHHJJZfE/fffH0888UQMHDiwzvnhw4dHu3bt6jxrFi5cGO+8806dZ83LL79cJ4R9/PHHo7i4OEpLS2trth5jS42//0DblMlkYtOmTZ4xwB779Kc/HS+//HLMnTu39uOII46Ic845p/a/PWeAxrR27dp48803o3fv3v4u01yyvdM7e+73v/99UlhYmNx2223JggULkgsvvDDp0qVLsnz58my3BrQwa9asSV588cXkxRdfTCIi+a//+q/kxRdfTN5+++0kSZLkuuuuS7p06ZL88Y9/TObNm5d87nOfSwYOHJhs2LChdowTTzwx+cQnPpE899xzyYwZM5IDDjggOfvss2vPr169OunZs2fypS99KZk/f37y+9//PunQoUPy85//vNnfL9D8Lr744qSkpCSZNm1asmzZstqP9evX19ZcdNFFyT777JM88cQTyQsvvJCMGjUqGTVqVO356urqZMiQIckJJ5yQzJ07N5kyZUqy9957J1dddVVtzaJFi5IOHTok3/rWt5JXX301KS8vT/Ly8pIpU6Y06/sFmt+VV16ZPPXUU8nixYuTefPmJVdeeWWSSqWSxx57LEkSzxig8Y0ZMyb5+te/XvvacwbYE1dccUUybdq0ZPHixclf/vKXZOzYsclee+2VrFy5MkkSz5jmIBRpJW6++eZkn332SQoKCpIRI0Ykzz77bLZbAlqgJ598MomIbT4mTJiQJEmSZDKZ5Hvf+17Ss2fPpLCwMPn0pz+dLFy4sM4YH3zwQXL22WcnnTp1SoqLi5Nzzz03WbNmTZ2al156KRk9enRSWFiY9O3bN7nuuuua6y0CWba9Z0xEJL/5zW9qazZs2JB87WtfS7p27Zp06NAhOfXUU5Nly5bVGeett95KTjrppKR9+/bJXnvtlVxxxRXJ5s2b69Q8+eSTyeGHH54UFBQkgwYNqnMPoPU677zzkn333TcpKChI9t577+TTn/50bSCSJJ4xQOP7eCjiOQPsibPOOivp3bt3UlBQkPTt2zc566yzkjfeeKP2vGdM00slSZJkZ44KAAAAAABA87GnCAAAAAAA0CYIRQAAAAAAgDZBKAIAAAAAALQJQhEAAAAAAKBNEIoAAAAAAABtglAEAAAAAABoE4QiAAAAAABAmyAUAQAAAAAA2gShCAAAAAAA0CYIRQAAAAAAgDZBKAIAAAAAALQJQhEAAAAAAKBN+P8B2Ctt/d9tNJcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 2000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 280618\n",
            "Part of the corpus by taking the \"x\" most frequent words:\n",
            "5000 : 0.82\n",
            "10000 : 0.86\n",
            "15000 : 0.89\n",
            "20000 : 0.90\n",
            "25000 : 0.91\n",
            "30000 : 0.92\n",
            "35000 : 0.93\n",
            "40000 : 0.94\n",
            "45000 : 0.94\n",
            "50000 : 0.94\n",
            "55000 : 0.95\n",
            "60000 : 0.95\n",
            "65000 : 0.95\n",
            "70000 : 0.96\n",
            "75000 : 0.96\n",
            "80000 : 0.96\n",
            "85000 : 0.96\n",
            "90000 : 0.96\n",
            "95000 : 0.97\n",
            "100000 : 0.97\n",
            "105000 : 0.97\n",
            "110000 : 0.97\n",
            "115000 : 0.97\n",
            "120000 : 0.97\n",
            "125000 : 0.97\n",
            "130000 : 0.97\n",
            "135000 : 0.98\n",
            "140000 : 0.98\n",
            "145000 : 0.98\n",
            "150000 : 0.98\n",
            "155000 : 0.98\n",
            "160000 : 0.98\n",
            "165000 : 0.98\n",
            "170000 : 0.98\n",
            "175000 : 0.98\n",
            "180000 : 0.98\n",
            "185000 : 0.98\n",
            "190000 : 0.98\n",
            "195000 : 0.99\n",
            "200000 : 0.99\n",
            "205000 : 0.99\n",
            "210000 : 0.99\n",
            "215000 : 0.99\n",
            "220000 : 0.99\n",
            "225000 : 0.99\n",
            "230000 : 0.99\n",
            "235000 : 0.99\n",
            "240000 : 0.99\n",
            "245000 : 0.99\n",
            "250000 : 0.99\n",
            "255000 : 1.00\n",
            "260000 : 1.00\n",
            "265000 : 1.00\n",
            "270000 : 1.00\n",
            "275000 : 1.00\n",
            "280000 : 1.00\n"
          ]
        }
      ],
      "source": [
        "corpus = texts\n",
        "vocab, word_counts = vocabulary(corpus)\n",
        "rank_counts = {w:[vocab[w], word_counts[w]] for w in vocab}\n",
        "rank_counts_array = np.array(list(rank_counts.values()))\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.title('Word counts versus rank')\n",
        "plt.scatter(rank_counts_array[:5000,0], rank_counts_array[:5000,1])\n",
        "plt.yscale('log')\n",
        "plt.show()\n",
        "\n",
        "print('Vocabulary size: %i' % len(vocab))\n",
        "print('Part of the corpus by taking the \"x\" most frequent words:')\n",
        "#for i in range(5000, len(vocab), 5000):\n",
        "for i in range(5000, len(vocab), 5000):\n",
        "    print('%i : %.2f' % (i, np.sum(rank_counts_array[:i, 1]) / np.sum(rank_counts_array[:,1]) ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLclV57m550B"
      },
      "source": [
        "Résultat de l'analyse: on peut se contenter d'un vocabulaire de 10000, voire 5000 mots - c'est important, car cela va déterminer la taille des objets que l'on va manipuler. On va maintenant recréer la matrice de co-occurence avec différents paramètres. Cela peut-être long: si cela pose problème, travaillez avec un vocabulaire plus réduit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69co2dsH550B",
        "outputId": "ec9993b3-3ea5-4e0a-90de-b48e67d81478"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'the': 0, 'a': 1, 'and': 2, 'of': 3, 'to': 4, 'is': 5, 'in': 6, 'I': 7, 'that': 8, 'this': 9, 'it': 10, '/><br': 11, 'was': 12, 'as': 13, 'with': 14, 'for': 15, 'but': 16, 'The': 17, 'on': 18, 'movie': 19, 'are': 20, 'his': 21, 'film': 22, 'have': 23, 'not': 24, 'be': 25, 'you': 26, 'he': 27, 'by': 28, 'at': 29, 'one': 30, 'an': 31, 'from': 32, 'who': 33, 'like': 34, 'all': 35, 'they': 36, 'has': 37, 'so': 38, 'just': 39, 'about': 40, 'or': 41, 'her': 42, 'out': 43, 'some': 44, 'very': 45, 'more': 46, 'This': 47, 'would': 48, 'what': 49, 'when': 50, 'good': 51, 'only': 52, 'their': 53, 'It': 54, 'if': 55, 'had': 56, 'really': 57, \"it's\": 58, 'even': 59, 'which': 60, 'up': 61, 'can': 62, 'were': 63, 'my': 64, 'see': 65, 'no': 66, 'she': 67, 'than': 68, '-': 69, 'been': 70, 'there': 71, 'into': 72, 'get': 73, 'will': 74, 'story': 75, 'much': 76, 'because': 77, 'other': 78, 'most': 79, 'we': 80, 'time': 81, 'me': 82, 'make': 83, 'could': 84, 'also': 85, 'do': 86, 'how': 87, 'people': 88, 'first': 89, 'its': 90, 'any': 91, '/>The': 92, 'great': 93, \"don't\": 94, 'made': 95, 'think': 96, 'bad': 97, 'him': 98, 'being': 99, 'many': 100, 'never': 101, 'then': 102, 'But': 103, 'two': 104, '<br': 105, 'too': 106, 'little': 107, 'after': 108, 'where': 109, 'way': 110, 'And': 111, 'it.': 112, 'them': 113, 'well': 114, 'your': 115, 'watch': 116, 'does': 117, 'seen': 118, 'movie.': 119, 'know': 120, 'character': 121, \"It's\": 122, 'did': 123, 'characters': 124, 'movies': 125, 'best': 126, 'love': 127, 'ever': 128, 'over': 129, 'still': 130, 'A': 131, 'In': 132, 'should': 133, 'films': 134, 'plot': 135, 'such': 136, 'acting': 137, 'these': 138, 'i': 139, 'off': 140, 'show': 141, 'film.': 142, 'He': 143, 'better': 144, 'say': 145, \"doesn't\": 146, 'through': 147, 'go': 148, 'those': 149, 'If': 150, 'something': 151, 'makes': 152, \"didn't\": 153, 'There': 154, 'scene': 155, 'film,': 156, 'find': 157, \"I'm\": 158, 'back': 159, 'movie,': 160, 'watching': 161, 'few': 162, 'real': 163, 'scenes': 164, 'actually': 165, 'going': 166, 'same': 167, 'life': 168, '/>I': 169, 'lot': 170, 'quite': 171, 'look': 172, 'while': 173, 'want': 174, 'end': 175, '&': 176, 'thing': 177, 'seems': 178, 'every': 179, 'got': 180, 'old': 181, 'why': 182, 'pretty': 183, \"can't\": 184, 'nothing': 185, 'man': 186, 'another': 187, 'actors': 188, 'years': 189, 'between': 190, 'take': 191, 'before': 192, 'give': 193, 'may': 194, 'gets': 195, 'part': 196, 'young': 197, 'thought': 198, \"I've\": 199, 'around': 200, 'it,': 201, 'things': 202, \"isn't\": 203, 'saw': 204, 'without': 205, 'always': 206, 'own': 207, 'work': 208, 'almost': 209, 'must': 210, 'whole': 211, 'cast': 212, 'down': 213, 'might': 214, 'both': 215, 'new': 216, 'though': 217, 'us': 218, 'come': 219, 'least': 220, 'They': 221, 'bit': 222, 'here': 223, 'enough': 224, 'director': 225, 'horror': 226, 'feel': 227, 'big': 228, 'original': 229, 'am': 230, 'probably': 231, 'As': 232, 'fact': 233, 'rather': 234, 'kind': 235, 'far': 236, 'long': 237, 'found': 238, 'last': 239, 'funny': 240, \"that's\": 241, 'anything': 242, 'comes': 243, 'since': 244, 'making': 245, 'trying': 246, '\"The': 247, 'now': 248, 'each': 249, 'action': 250, 'interesting': 251, 'What': 252, 'done': 253, 'You': 254, 'worst': 255, 'She': 256, 'right': 257, 'our': 258, 'looks': 259, 'believe': 260, 'point': 261, 'put': 262, 'goes': 263, 'played': 264, \"he's\": 265, '/>This': 266, 'main': 267, \"wasn't\": 268, 'family': 269, 'role': 270, 'having': 271, 'guy': 272, 'series': 273, 'seem': 274, 'plays': 275, 'script': 276, 'performance': 277, 'hard': 278, 'takes': 279, 'world': 280, 'watched': 281, 'When': 282, 'worth': 283, 'minutes': 284, 'looking': 285, 'music': 286, 'different': 287, 'TV': 288, 'anyone': 289, 'especially': 290, 'shows': 291, 'sure': 292, 'away': 293, 'set': 294, 'woman': 295, 'times': 296, 'someone': 297, 'during': 298, 'time.': 299, 'yet': 300, 'left': 301, 'comedy': 302, 'three': 303, 'John': 304, 'girl': 305, \"there's\": 306, 'simply': 307, 'American': 308, 'fun': 309, 'seeing': 310, 'completely': 311, 'reason': 312, \"you're\": 313, 'play': 314, 'used': 315, 'special': 316, 'Not': 317, 'read': 318, 'need': 319, 'again': 320, 'One': 321, 'true': 322, 'For': 323, 'use': 324, 'sense': 325, 'version': 326, '--': 327, 'everything': 328, 'idea': 329, 'until': 330, 'place': 331, 'given': 332, 'help': 333, 'So': 334, 'nice': 335, 'rest': 336, 'beautiful': 337, 'truly': 338, 'less': 339, 'job': 340, 'money': 341, 'recommend': 342, 'came': 343, 'DVD': 344, 'try': 345, 'We': 346, 'ending': 347, 'once': 348, 'gives': 349, 'tell': 350, 'That': 351, 'said': 352, 'getting': 353, 'shot': 354, 'everyone': 355, 'high': 356, 'second': 357, 'keep': 358, 'My': 359, 'himself': 360, 'excellent': 361, 'couple': 362, 'All': 363, 'actor': 364, '(and': 365, 'enjoy': 366, 'supposed': 367, 'become': 368, 'half': 369, 'playing': 370, 'audience': 371, 'understand': 372, 'all,': 373, 'felt': 374, 'effects': 375, 'poor': 376, 'However,': 377, 'entire': 378, \"couldn't\": 379, 'liked': 380, 'small': 381, 'fan': 382, 'wife': 383, 'early': 384, 'start': 385, 'went': 386, 'doing': 387, 'together': 388, 'against': 389, 'full': 390, 'book': 391, 'remember': 392, 'Even': 393, 'THE': 394, '2': 395, 'let': 396, 'screen': 397, 'Hollywood': 398, 'definitely': 399, 'absolutely': 400, 'time,': 401, 'becomes': 402, 'often': 403, 'is,': 404, 'sort': 405, '10': 406, 'waste': 407, 'later': 408, 'day': 409, 'seemed': 410, 'along': 411, 'At': 412, 'year': 413, '\\x96': 414, 'After': 415, 'His': 416, 'certainly': 417, 'piece': 418, 'next': 419, 'short': 420, '.': 421, 'maybe': 422, 'several': 423, 'human': 424, 'black': 425, 'wanted': 426, 'instead': 427, 'although': 428, 'else': 429, 'kids': 430, 'wonderful': 431, 'camera': 432, 'production': 433, 'course': 434, 'that,': 435, 'performances': 436, 'classic': 437, 'loved': 438, 'them.': 439, 'tries': 440, 'To': 441, 'totally': 442, 'father': 443, 'women': 444, 'men': 445, 'wants': 446, 'able': 447, \"I'd\": 448, 'live': 449, 'called': 450, 'mind': 451, 'home': 452, 'gave': 453, 'hope': 454, 'top': 455, 'line': 456, 'already': 457, 'enjoyed': 458, 'based': 459, 'person': 460, 'video': 461, 'friends': 462, 'turn': 463, 'perfect': 464, 'story,': 465, '(the': 466, 'one.': 467, 'starts': 468, 'New': 469, 'under': 470, \"won't\": 471, 'turns': 472, 'final': 473, 'this.': 474, 'sex': 475, 'While': 476, 'name': 477, 'care': 478, 'problem': 479, 'mean': 480, 'episode': 481, 'me.': 482, 'written': 483, 'him.': 484, 'low': 485, 'moments': 486, 'stupid': 487, 'finally': 488, 'lost': 489, 'lead': 490, 'either': 491, 'lines': 492, 'Michael': 493, 'favorite': 494, 'took': 495, 'behind': 496, 'cannot': 497, 'this,': 498, \"she's\": 499, 'face': 500, 'Of': 501, \"you'll\": 502, 'No': 503, 'head': 504, 'stars': 505, 'story.': 506, 'budget': 507, 'title': 508, 'sound': 509, 'well.': 510, 'good.': 511, 'death': 512, 'fine': 513, 'guess': 514, 'Some': 515, 'school': 516, \"they're\": 517, 'me,': 518, 'all.': 519, \"film's\": 520, 'dialogue': 521, 'night': 522, 'How': 523, 'extremely': 524, 'lack': 525, 'heard': 526, 'style': 527, 'beginning': 528, 'terrible': 529, 'life.': 530, 'itself': 531, 'star': 532, 'feeling': 533, 'perhaps': 534, 'others': 535, 'house': 536, 'works': 537, 'expect': 538, 'boring': 539, \"wouldn't\": 540, 'kill': 541, 'attempt': 542, 'looked': 543, 'decent': 544, 'Mr.': 545, 'course,': 546, 'lives': 547, 'good,': 548, \"There's\": 549, 'quality': 550, 'Then': 551, '/>In': 552, 'late': 553, 'friend': 554, 'With': 555, 'Just': 556, 'well,': 557, 'entertaining': 558, 'particularly': 559, '/>It': 560, 'fight': 561, 'fans': 562, 'wrong': 563, 'thinking': 564, 'complete': 565, 'viewer': 566, 'case': 567, 'highly': 568, 'finds': 569, 'taken': 570, 'directed': 571, 'whose': 572, 'throughout': 573, 'leave': 574, 'writing': 575, 'obviously': 576, 'exactly': 577, '/>If': 578, 'somewhat': 579, 'amazing': 580, 'movies.': 581, 'guys': 582, 'awful': 583, 'evil': 584, 'laugh': 585, 'says': 586, 'shown': 587, 'obvious': 588, 'Why': 589, 'run': 590, '3': 591, 'movies,': 592, 'close': 593, 'told': 594, 'type': 595, 'James': 596, 'boy': 597, 'past': 598, 'parts': 599, 'number': 600, 'wonder': 601, 'films,': 602, 'group': 603, 'however,': 604, 'across': 605, 'opening': 606, 'picture': 607, 'car': 608, 'acting,': 609, 'mother': 610, 'turned': 611, 'save': 612, 'war': 613, 'soon': 614, 'sometimes': 615, 'white': 616, 'coming': 617, 'dead': 618, 'worse': 619, 'known': 620, 'tells': 621, 'despite': 622, 'started': 623, 'direction': 624, 'strong': 625, 'that.': 626, 'local': 627, 'hour': 628, 'killer': 629, 'side': 630, 'stop': 631, 'wish': 632, 'taking': 633, 'single': 634, 'myself': 635, 'knew': 636, 'children': 637, 'huge': 638, 'act': 639, 'except': 640, ',': 641, 'here.': 642, 'it.<br': 643, 'running': 644, 'killed': 645, 'happens': 646, 'female': 647, 'British': 648, 'way,': 649, 'usually': 650, 'including': 651, 'bad.': 652, \"aren't\": 653, 'stories': 654, 'bring': 655, 'fact,': 656, 'girls': 657, 'supporting': 658, 'major': 659, 'due': 660, 'characters,': 661, 'David': 662, '/>There': 663, 'involved': 664, 'dark': 665, 'humor': 666, 'talking': 667, 'Robert': 668, 'town': 669, 'voice': 670, 'musical': 671, 'out.': 672, \"Don't\": 673, 'hit': 674, 'call': 675, 'game': 676, 'end,': 677, 'brilliant': 678, '!': 679, 'falls': 680, 'giving': 681, 'and,': 682, 'actress': 683, 'relationship': 684, 'ends': 685, 'matter': 686, 'son': 687, \"I'll\": 688, 'mostly': 689, 'saying': 690, 'clearly': 691, 'living': 692, 'again.': 693, 'Well,': 694, 'one,': 695, 'easily': 696, 'serious': 697, 'feels': 698, 'police': 699, 'order': 700, 'bad,': 701, 'drama': 702, 'themselves': 703, 'appears': 704, 'example': 705, 'actual': 706, 'needs': 707, 'moment': 708, 'change': 709, 'similar': 710, 'important': 711, 'chance': 712, 'end.': 713, 'nearly': 714, \"haven't\": 715, 'modern': 716, 'is.': 717, 'child': 718, '/>But': 719, 'history': 720, 'cinema': 721, 'comic': 722, 'Although': 723, 'horrible': 724, 'On': 725, 'seen.': 726, 'way.': 727, '/>': 728, 'mention': 729, 'kept': 730, 'art': 731, 'named': 732, 'heart': 733, 'movie.<br': 734, 'released': 735, 'her.': 736, \"That's\": 737, 'interest': 738, 'English': 739, 'happened': 740, 'knows': 741, 'beyond': 742, 'cut': 743, 'plot,': 744, 'here,': 745, 'within': 746, 'usual': 747, 'films.': 748, 'slow': 749, 'kid': 750, 'using': 751, 'again,': 752, 'brought': 753, 'stuff': 754, 'hours': 755, 'simple': 756, 'typical': 757, 'bunch': 758, 'eyes': 759, 'romantic': 760, 'certain': 761, 'tried': 762, 'whether': 763, 'him,': 764, 'showing': 765, 'near': 766, 'film.<br': 767, '/>A': 768, 'fall': 769, 'George': 770, 'upon': 771, 'life,': 772, 'events': 773, 'strange': 774, 'decided': 775, 'score': 776, 'shots': 777, '/>As': 778, 'working': 779, 'among': 780, 'Her': 781, 'four': 782, 'body': 783, 'surprised': 784, 'hear': 785, 'begins': 786, 'Jack': 787, 'yourself': 788, '(I': 789, 'on.': 790, 'sad': 791, 'five': 792, '(as': 793, 'cheap': 794, 'greatest': 795, '/>And': 796, 'Richard': 797, 'became': 798, 'middle': 799, 'Paul': 800, 'Maybe': 801, 'learn': 802, 'happen': 803, 'none': 804, 'song': 805, 'famous': 806, 'annoying': 807, 'stay': 808, 'daughter': 809, 'basically': 810, 'days': 811, 'too.': 812, 'happy': 813, 'sit': 814, 'jokes': 815, 'experience': 816, 'sexual': 817, 'Peter': 818, 'sets': 819, 'murder': 820, 'songs': 821, \"/>It's\": 822, 'sequence': 823, 'attention': 824, '(which': 825, '1': 826, 'funny.': 827, 'lots': 828, 'realize': 829, 'talk': 830, 'buy': 831, 'hate': 832, 'difficult': 833, 'above': 834, 'view': 835, 'violence': 836, 'documentary': 837, 'stand': 838, 'easy': 839, 'From': 840, 'clear': 841, 'funny,': 842, 'light': 843, 'leaves': 844, 'means': 845, 'husband': 846, 'elements': 847, 'alone': 848, 'characters.': 849, 'Now': 850, 'keeps': 851, 'roles': 852, 'brother': 853, 'episodes': 854, 'meets': 855, 'cinematography': 856, 'genre': 857, '5': 858, 'character,': 859, 'word': 860, 'Its': 861, 'flick': 862, 'figure': 863, 'Japanese': 864, 'ones': 865, 'These': 866, 'them,': 867, 'out,': 868, 'possibly': 869, 'poorly': 870, 'hand': 871, 'First': 872, 'doubt': 873, 'silly': 874, 'brings': 875, 'previous': 876, 'nor': 877, 'say,': 878, 'emotional': 879, 'age': 880, 'NOT': 881, 'French': 882, 'move': 883, \"you've\": 884, 'novel': 885, 'killing': 886, 'apparently': 887, 'ten': 888, 'Like': 889, 'Is': 890, 'possible': 891, 'theme': 892, 'on,': 893, \"who's\": 894, 'problems': 895, 'leads': 896, 'reality': 897, 'moving': 898, 'level': 899, 'message': 900, 'reading': 901, 'straight': 902, 'comments': 903, 'talent': 904, 'television': 905, 'towards': 906, 'Most': 907, 'add': 908, 'career': 909, 'better.': 910, '(or': 911, 'deal': 912, 'meant': 913, 'enjoyable': 914, 'overall': 915, 'Tom': 916, 'review': 917, 'write': 918, 'ridiculous': 919, 'Oscar': 920, 'filmed': 921, 'An': 922, 'begin': 923, 'various': 924, 'herself': 925, 'needed': 926, 'gore': 927, 'somehow': 928, 'cool': 929, 'blood': 930, 'work.': 931, 'create': 932, 'though,': 933, 'incredibly': 934, 'leading': 935, 'feature': 936, 'forced': 937, 'watch.': 938, 'manages': 939, 'room': 940, 'personal': 941, 'particular': 942, '4': 943, 'whom': 944, 'eventually': 945, 'appear': 946, 'hell': 947, 'animation': 948, 'future': 949, 'hero': 950, 'Also': 951, 'show.': 952, 'form': 953, \"He's\": 954, 'up.': 955, 'fairly': 956, 'interested': 957, 'male': 958, 'writer': 959, 'effort': 960, 'points': 961, 'third': 962, 'there.': 963, 'scary': 964, 'hilarious': 965, 'meet': 966, 'rent': 967, 'plenty': 968, 'you.': 969, 'older': 970, 'expecting': 971, 'subject': 972, 'her,': 973, 'weak': 974, 'dramatic': 975, 'hardly': 976, 'reviews': 977, 'power': 978, 'viewers': 979, 'team': 980, 'premise': 981, 'scenes,': 982, 'attempts': 983, 'follow': 984, 'political': 985, 'tale': 986, 'features': 987, 'Joe': 988, 'average': 989, 'imagine': 990, 'scene,': 991, 'Also,': 992, 'York': 993, 'decides': 994, 'total': 995, 'worked': 996, 'Disney': 997, 'Lee': 998, 'up,': 999, 'pay': 1000, 'country': 1001, 'crap': 1002, 'gone': 1003, 'fantastic': 1004, 'forget': 1005, 'words': 1006, 'plain': 1007, 'King': 1008, '20': 1009, 'dialog': 1010, 'expected': 1011, '(a': 1012, 'minute': 1013, 'however': 1014, 'class': 1015, 'Who': 1016, 'uses': 1017, 'storyline': 1018, 'front': 1019, 'By': 1020, 'fails': 1021, 'viewing': 1022, 'spent': 1023, 'forward': 1024, 'badly': 1025, 'theater': 1026, '(who': 1027, 'sees': 1028, 'comment': 1029, 'slightly': 1030, 'admit': 1031, 'waiting': 1032, 'deserves': 1033, 'unique': 1034, 'ask': 1035, 'writers': 1036, 'sequences': 1037, 'sounds': 1038, 'open': 1039, 'stage': 1040, 'hold': 1041, 'dance': 1042, 'nature': 1043, 'fast': 1044, 'portrayed': 1045, 'social': 1046, 'soundtrack': 1047, 'crime': 1048, 'footage': 1049, 'times,': 1050, 'atmosphere': 1051, 'realistic': 1052, 'William': 1053, \"what's\": 1054, '...': 1055, 'dull': 1056, 'check': 1057, 'directors': 1058, 'caught': 1059, 'agree': 1060, 'years.': 1061, 'biggest': 1062, 'recent': 1063, 'there,': 1064, 'telling': 1065, 'talented': 1066, 'etc.': 1067, 'quickly': 1068, 'perfectly': 1069, 'wait': 1070, 'visual': 1071, 'spend': 1072, 'amount': 1073, 'wrote': 1074, 'predictable': 1075, 'Dr.': 1076, 'memorable': 1077, 'parents': 1078, 'battle': 1079, 'entirely': 1080, 'sequel': 1081, 'large': 1082, 'Or': 1083, 'outside': 1084, 'ways': 1085, 'powerful': 1086, 'result': 1087, 'plot.': 1088, 'editing': 1089, 'made.': 1090, 'deep': 1091, 'people.': 1092, 'rating': 1093, 'weird': 1094, 'fighting': 1095, 'disappointed': 1096, 'period': 1097, 'question': 1098, 'said,': 1099, 'pure': 1100, 'people,': 1101, 'actors,': 1102, '/>What': 1103, 'work,': 1104, 'earlier': 1105, 'appreciate': 1106, 'inside': 1107, 'showed': 1108, 'kills': 1109, 'show,': 1110, 'lame': 1111, 'material': 1112, 'compared': 1113, \"weren't\": 1114, 'it!': 1115, 'former': 1116, 'Yes,': 1117, 'filmmakers': 1118, 'release': 1119, 'filled': 1120, 'copy': 1121, 'casting': 1122, 'Ben': 1123, 'moves': 1124, 'city': 1125, 'popular': 1126, 'creepy': 1127, 'Man': 1128, 'unless': 1129, 'decide': 1130, 'positive': 1131, 'sister': 1132, 'comedy,': 1133, 'cheesy': 1134, 'scenes.': 1135, 'superb': 1136, 'runs': 1137, 'portrayal': 1138, 'missing': 1139, 'present': 1140, 'screenplay': 1141, 'character.': 1142, 'consider': 1143, 'considered': 1144, 'free': 1145, 'seriously': 1146, 'rich': 1147, 'more.': 1148, 'be.': 1149, 'too,': 1150, 'ideas': 1151, 'credits': 1152, 'recently': 1153, 'entertainment': 1154, 'thriller': 1155, 'follows': 1156, 'fit': 1157, 'general': 1158, 'married': 1159, 'wasted': 1160, 'Great': 1161, 'ago': 1162, 'common': 1163, 'barely': 1164, 'gay': 1165, 'miss': 1166, 'mystery': 1167, 'ended': 1168, 'believable': 1169, 'whatever': 1170, 'Italian': 1171, 'created': 1172, 'please': 1173, 'German': 1174, 'But,': 1175, 'in.': 1176, 'opinion': 1177, 'familiar': 1178, 'likes': 1179, 'situation': 1180, 'eye': 1181, 'Christmas': 1182, 'setting': 1183, 'further': 1184, 'War': 1185, 'bought': 1186, 'great,': 1187, 'involving': 1188, 'focus': 1189, 'break': 1190, 'suddenly': 1191, 'leaving': 1192, 'Jane': 1193, 'Bill': 1194, 'remake': 1195, 'missed': 1196, 'screen.': 1197, \"'The\": 1198, 'sorry': 1199, 'background': 1200, 'following': 1201, 'role.': 1202, 'box': 1203, 'members': 1204, 'Unfortunately,': 1205, 'utterly': 1206, 'hands': 1207, 'successful': 1208, 'acting.': 1209, 'managed': 1210, 'beauty': 1211, 'won': 1212, 'younger': 1213, 'rate': 1214, 'odd': 1215, 'surprise': 1216, 'Best': 1217, 'Well': 1218, '\"I': 1219, 'fantasy': 1220, '(in': 1221, 'basic': 1222, 'glad': 1223, 'starring': 1224, 'crew': 1225, 'Another': 1226, 'you,': 1227, 'avoid': 1228, 'animated': 1229, 'great.': 1230, 'man,': 1231, 'God': 1232, 'considering': 1233, 'Scott': 1234, 'OF': 1235, 'solid': 1236, \"movie's\": 1237, 'hot': 1238, 'crazy': 1239, 'dumb': 1240, 'zombie': 1241, 'potential': 1242, 'thinks': 1243, 'Good': 1244, 'was,': 1245, 'series,': 1246, 'ultimately': 1247, 'Perhaps': 1248, 'laughing': 1249, 'times.': 1250, 'sitting': 1251, 'effect': 1252, 'world.': 1253, '30': 1254, 'bored': 1255, 'chemistry': 1256, '/>My': 1257, 'acted': 1258, 'mentioned': 1259, 'IMDb': 1260, 'slasher': 1261, 'shame': 1262, 'season': 1263, 'US': 1264, 'public': 1265, 'bizarre': 1266, 'directing': 1267, 'suspense': 1268, 'book,': 1269, 'cover': 1270, 'truth': 1271, 'generally': 1272, 'singing': 1273, 'not.': 1274, 'speak': 1275, 'cast,': 1276, 'likely': 1277, 'convincing': 1278, 'explain': 1279, '/>All': 1280, 'space': 1281, 'changed': 1282, 'win': 1283, 'added': 1284, 'cop': 1285, 'intelligent': 1286, 'twist': 1287, 'walk': 1288, 'cinematic': 1289, 'fully': 1290, 'romance': 1291, 'adult': 1292, 'much.': 1293, 'thrown': 1294, 'Oh': 1295, 'monster': 1296, 'off,': 1297, 'business': 1298, 'failed': 1299, 'rare': 1300, 'catch': 1301, 'neither': 1302, 'impossible': 1303, 'return': 1304, '15': 1305, 'Steve': 1306, 'remains': 1307, 'effective': 1308, '/>One': 1309, 'match': 1310, 'sent': 1311, 'incredible': 1312, 'clever': 1313, 'hoping': 1314, 'Every': 1315, 'equally': 1316, 'scene.': 1317, 'development': 1318, 'World': 1319, 'Star': 1320, '10.': 1321, 'historical': 1322, \"we're\": 1323, 'produced': 1324, 'violent': 1325, 'constantly': 1326, 'dancing': 1327, 'lady': 1328, 'aspect': 1329, 'today': 1330, 'series.': 1331, 'credit': 1332, 'ability': 1333, 'longer': 1334, 'success': 1335, 'shooting': 1336, 'off.': 1337, 'boys': 1338, 'script,': 1339, 'years,': 1340, 'escape': 1341, 'Mary': 1342, 'Tony': 1343, 'concept': 1344, 'choice': 1345, 'producers': 1346, 'stick': 1347, 'computer': 1348, 'otherwise': 1349, 'indeed': 1350, 'know,': 1351, 'comedy.': 1352, 'Only': 1353, 'excuse': 1354, 'trouble': 1355, 'knowing': 1356, 'puts': 1357, 'pick': 1358, 'music,': 1359, 'cute': 1360, 'not,': 1361, 'immediately': 1362, 'exciting': 1363, 'do.': 1364, 'contains': 1365, 'was.': 1366, 'Big': 1367, 'mean,': 1368, 'tension': 1369, 'secret': 1370, 'reasons': 1371, 'Because': 1372, 'Very': 1373, 'dream': 1374, 'Sam': 1375, 'So,': 1376, 'cause': 1377, '/>So': 1378, 'suggest': 1379, '(': 1380, 'depth': 1381, 'then,': 1382, 'cult': 1383, 'together.': 1384, 'questions': 1385, 'literally': 1386, 'unlike': 1387, 'Jim': 1388, 'trip': 1389, 'military': 1390, 'flat': 1391, 'lacks': 1392, 'stands': 1393, 'cartoon': 1394, 'laughs': 1395, 'makers': 1396, 'air': 1397, 'enough,': 1398, 'list': 1399, 'surprisingly': 1400, 'society': 1401, 'villain': 1402, 'Do': 1403, 'presented': 1404, 'tough': 1405, 'OK': 1406, 'girlfriend': 1407, 'include': 1408, 'pace': 1409, 'terms': 1410, 'studio': 1411, 'died': 1412, \"hasn't\": 1413, 'respect': 1414, 'Though': 1415, 'college': 1416, 'joke': 1417, 'amusing': 1418, 'best.': 1419, 'minor': 1420, 'Frank': 1421, 'impression': 1422, 'adaptation': 1423, 'unfortunately': 1424, 'nudity': 1425, 'beautifully': 1426, 'images': 1427, 'fake': 1428, 'die': 1429, 'plus': 1430, 'either.': 1431, 'impressive': 1432, 'example,': 1433, '\"the': 1434, 'merely': 1435, 'De': 1436, 'note': 1437, 'thing.': 1438, 'Indian': 1439, 'band': 1440, 'walking': 1441, 'control': 1442, 'state': 1443, 'appeal': 1444, 'America': 1445, 'apart': 1446, 'Film': 1447, 'drawn': 1448, 'Watch': 1449, 'about.': 1450, 'appearance': 1451, 'innocent': 1452, 'proves': 1453, 'Director': 1454, \"you'd\": 1455, 'pointless': 1456, '\"': 1457, 'rock': 1458, 'Christopher': 1459, 'pass': 1460, '8': 1461, 'provides': 1462, 'mess': 1463, 'red': 1464, 'in,': 1465, 'falling': 1466, 'natural': 1467, 'shoot': 1468, 'brief': 1469, 'day.': 1470, 'Chris': 1471, 'fell': 1472, 'sick': 1473, 'loves': 1474, 'becoming': 1475, 'standard': 1476, 'sci-fi': 1477, 'dog': 1478, 'minutes.': 1479, 'silent': 1480, 'share': 1481, 'suppose': 1482, 'subtle': 1483, 'role,': 1484, 'touch': 1485, 'helps': 1486, 'aspects': 1487, 'Night': 1488, 'fellow': 1489, 'appeared': 1490, 'with.': 1491, 'mysterious': 1492, 'mainly': 1493, 'support': 1494, 'reminded': 1495, 'meaning': 1496, 'narrative': 1497, 'IS': 1498, 'Here': 1499, 'pull': 1500, 'Harry': 1501, 'acts': 1502, 'normal': 1503, 'offers': 1504, 'throw': 1505, 'actors.': 1506, 'adds': 1507, 'did.': 1508, 'thoroughly': 1509, 'touching': 1510, 'complex': 1511, 'store': 1512, 'followed': 1513, 'audiences': 1514, 'sweet': 1515, 'fun.': 1516, 'disturbing': 1517, 'drug': 1518, 'seemingly': 1519, 'somewhere': 1520, 'revenge': 1521, 'wondering': 1522, 'putting': 1523, 'central': 1524, 'tired': 1525, 'value': 1526, 'ever.': 1527, 'effects,': 1528, 'director,': 1529, 'held': 1530, 'so,': 1531, 'stuck': 1532, 'Charles': 1533, 'Black': 1534, 'mood': 1535, 'rented': 1536, 'terrific': 1537, 'fear': 1538, 'Van': 1539, 'Despite': 1540, 'serial': 1541, 'surely': 1542, 'South': 1543, 'paid': 1544, 'turning': 1545, 'books': 1546, 'world,': 1547, 'picked': 1548, 'company': 1549, 'onto': 1550, 'gun': 1551, 'delivers': 1552, 'to.': 1553, 'charming': 1554, 'day,': 1555, 'tone': 1556, 'themes': 1557, 'redeeming': 1558, 'action,': 1559, 'minutes,': 1560, 'painful': 1561, 'slowly': 1562, 'Nothing': 1563, 'fire': 1564, 'People': 1565, 'AND': 1566, 'self': 1567, 'language': 1568, 'And,': 1569, 'moral': 1570, 'heavy': 1571, 'science': 1572, 'lived': 1573, 'lovely': 1574, 'finding': 1575, 'office': 1576, 'Charlie': 1577, 'point,': 1578, 'Will': 1579, 'presence': 1580, 'Once': 1581, 'Bruce': 1582, 'led': 1583, 'absolute': 1584, 'yes,': 1585, 'hand,': 1586, 'negative': 1587, 'willing': 1588, 'opportunity': 1589, 'performance.': 1590, 'funniest': 1591, 'element': 1592, 'rated': 1593, 'allowed': 1594, 'man.': 1595, 'twists': 1596, 'doctor': 1597, 'water': 1598, 'Billy': 1599, 'interesting.': 1600, 'changes': 1601, 'cast.': 1602, 'Henry': 1603, 'script.': 1604, 'Both': 1605, 'now,': 1606, 'key': 1607, 'prison': 1608, 'baby': 1609, 'random': 1610, 'force': 1611, 'awful.': 1612, 'party': 1613, 'giant': 1614, 'love,': 1615, 'de': 1616, 'spirit': 1617, 'available': 1618, \"let's\": 1619, \"shouldn't\": 1620, 'values': 1621, 'though.': 1622, 'Two': 1623, 'other.': 1624, 'Christian': 1625, 'later,': 1626, 'away.': 1627, 'filming': 1628, 'carry': 1629, 'Many': 1630, 'Little': 1631, 'track': 1632, 'movie!': 1633, 'laughed': 1634, 'issues': 1635, 'helped': 1636, 'thing,': 1637, 'supposedly': 1638, 'nobody': 1639, 'actor,': 1640, '(not': 1641, 'seen,': 1642, 'includes': 1643, 'describe': 1644, 'time.<br': 1645, \"She's\": 1646, 'Where': 1647, 'bother': 1648, 'magic': 1649, 'Other': 1650, 'thanks': 1651, 'but,': 1652, '7': 1653, 'sexy': 1654, '/>For': 1655, 'soldiers': 1656, '(with': 1657, 'image': 1658, 'mix': 1659, 'asks': 1660, 'before.': 1661, 'fair': 1662, 'Their': 1663, 'drive': 1664, 'wrong.': 1665, 'ready': 1666, 'approach': 1667, 'pieces': 1668, 'names': 1669, 'critics': 1670, 'book.': 1671, \"character's\": 1672, 'boring.': 1673, 'impressed': 1674, 'feelings': 1675, 'Too': 1676, 'interesting,': 1677, 'Movie': 1678, 'humour': 1679, 'martial': 1680, 'audience.': 1681, 'intended': 1682, 'family.': 1683, 'damn': 1684, 'favourite': 1685, 'technical': 1686, 'provide': 1687, 'done.': 1688, 'else.': 1689, 'creative': 1690, 'Oh,': 1691, 'masterpiece': 1692, \"man's\": 1693, 'moved': 1694, 'pathetic': 1695, 'stunning': 1696, 'adventure': 1697, 'chase': 1698, 'gang': 1699, 'tragic': 1700, 'outstanding': 1701, 'Brian': 1702, 'lose': 1703, 'artistic': 1704, 'compelling': 1705, 'Jerry': 1706, 'era': 1707, 'gotten': 1708, 'physical': 1709, 'ending.': 1710, 'Alan': 1711, 'victim': 1712, 'wearing': 1713, 'teenage': 1714, 'Instead': 1715, 'comedic': 1716, 'back.': 1717, 'mental': 1718, 'performance,': 1719, 'allow': 1720, 'inspired': 1721, 'Red': 1722, 'opinion,': 1723, 'hair': 1724, 'attack': 1725, 'House': 1726, 'deeply': 1727, 'street': 1728, 'case,': 1729, 'awesome': 1730, 'wife,': 1731, 'purpose': 1732, 'boring,': 1733, 'developed': 1734, 'reminds': 1735, 'million': 1736, 'sat': 1737, 'fascinating': 1738, 'continue': 1739, 'struggle': 1740, 'journey': 1741, 'project': 1742, 'money.': 1743, 'accept': 1744, 'now.': 1745, 'before,': 1746, 'honestly': 1747, 'situations': 1748, 'train': 1749, 'step': 1750, 'build': 1751, 'naked': 1752, \"80's\": 1753, 'place.': 1754, 'holds': 1755, 'difference': 1756, 'comedies': 1757, 'better,': 1758, 'wanting': 1759, 'do,': 1760, 'Chinese': 1761, 'afraid': 1762, 'Kelly': 1763, 'wonderfully': 1764, 'first,': 1765, 'government': 1766, 'London': 1767, 'member': 1768, 'motion': 1769, 'deliver': 1770, '\"A': 1771, 'made,': 1772, 'search': 1773, 'Ms.': 1774, 'family,': 1775, 'realized': 1776, 'lets': 1777, 'Stewart': 1778, 'himself.': 1779, 'for.': 1780, 'be,': 1781, 'worthy': 1782, 'disappointed.': 1783, 'sense.': 1784, 'rarely': 1785, 'plan': 1786, 'details': 1787, 'compare': 1788, 'honest': 1789, 'offer': 1790, 'answer': 1791, 'culture': 1792, 'direct': 1793, 'notice': 1794, 'cold': 1795, 'western': 1796, 'detective': 1797, 'thus': 1798, 'Mr': 1799, 'places': 1800, 'trash': 1801, 'toward': 1802, 'nowhere': 1803, 'emotions': 1804, 'gorgeous': 1805, 'began': 1806, 'Stephen': 1807, 'thats': 1808, 'OK,': 1809, 'dialogue,': 1810, 'part,': 1811, 'taste': 1812, 'confused': 1813, 'job.': 1814, 'original,': 1815, '/>When': 1816, 'earth': 1817, 'mediocre': 1818, 'Smith': 1819, '9': 1820, 'attractive': 1821, 'edge': 1822, 'latter': 1823, 'marriage': 1824, 'apparent': 1825, 'brain': 1826, 'watch,': 1827, 'Having': 1828, 'flying': 1829, 'intense': 1830, 'zombies': 1831, 'student': 1832, 'introduced': 1833, 'fresh': 1834, 'pictures': 1835, 'Since': 1836, 'However': 1837, 'genius': 1838, 'six': 1839, 'constant': 1840, 'religious': 1841, 'creating': 1842, 'listen': 1843, 'climax': 1844, 'Everything': 1845, \"hadn't\": 1846, 'beat': 1847, 'asked': 1848, 'best,': 1849, 'Martin': 1850, 'spot': 1851, 'pulled': 1852, 'capture': 1853, 'manage': 1854, 'Unfortunately': 1855, 'mad': 1856, 'building': 1857, 'Miss': 1858, 'professional': 1859, 'costumes': 1860, 'bottom': 1861, \"today's\": 1862, 'nasty': 1863, 'B': 1864, 'independent': 1865, 'accent': 1866, 'exception': 1867, 'information': 1868, 'shock': 1869, 'it?': 1870, 'deserve': 1871, 'ghost': 1872, 'extra': 1873, 'Mark': 1874, 'likable': 1875, 'other,': 1876, 'hated': 1877, 'genre.': 1878, 'much,': 1879, 'photography': 1880, 'down.': 1881, 'treated': 1882, '(like': 1883, 'blame': 1884, 'ahead': 1885, 'Russian': 1886, 'woman,': 1887, 'disappointing': 1888, '(if': 1889, 'limited': 1890, 'quick': 1891, 'desperate': 1892, 'animals': 1893, 'students': 1894, 'Rock': 1895, 'master': 1896, 'ago,': 1897, 'numerous': 1898, 'charm': 1899, 'Now,': 1900, 'see.': 1901, 'hidden': 1902, 'personally': 1903, 'scenery': 1904, 'loud': 1905, '90': 1906, 'Those': 1907, 'porn': 1908, 'so.': 1909, 'Did': 1910, 'winning': 1911, 'Tim': 1912, 'superior': 1913, '/>To': 1914, 'Fred': 1915, 'murders': 1916, 'actresses': 1917, 'cinema.': 1918, 'right.': 1919, 'unusual': 1920, 'Yet': 1921, 'terribly': 1922, 'addition': 1923, 'trailer': 1924, 'growing': 1925, 'discover': 1926, 'City': 1927, 'DVD.': 1928, \"/>I'm\": 1929, 'extreme': 1930, 'Love': 1931, 'flick.': 1932, '6': 1933, 'dealing': 1934, 'hopes': 1935, 'sadly': 1936, 'Was': 1937, 'prove': 1938, 'love.': 1939, 'long,': 1940, 'with,': 1941, 'responsible': 1942, 'Ed': 1943, 'phone': 1944, 'smart': 1945, 'finish': 1946, 'loses': 1947, 'Jason': 1948, 'keeping': 1949, 'week': 1950, 'terrible.': 1951, 'Joan': 1952, 'emotion': 1953, 'portray': 1954, 'unbelievable': 1955, '?': 1956, 'mixed': 1957, 'door': 1958, 'returns': 1959, 'Don': 1960, 'rape': 1961, '/>While': 1962, 'Everyone': 1963, 'mistake': 1964, 'deals': 1965, 'pain': 1966, 'girl,': 1967, '100': 1968, 'Americans': 1969, 'impact': 1970, 'genuinely': 1971, 'ride': 1972, 'wild': 1973, 'affair': 1974, 'kinda': 1975, 'road': 1976, '40': 1977, 'relationships': 1978, 'Danny': 1979, 'saved': 1980, 'Batman': 1981, 'noir': 1982, 'fun,': 1983, 'point.': 1984, 'are,': 1985, 'understanding': 1986, 'low-budget': 1987, 'producer': 1988, 'featuring': 1989, 'starting': 1990, 'numbers': 1991, 'camp': 1992, 'original.': 1993, 'dad': 1994, 'together,': 1995, 'psychological': 1996, 'remarkable': 1997, 'Jeff': 1998, 'brothers': 1999, 'them.<br': 2000, 'color': 2001, 'proved': 2002, 'cat': 2003, 'expectations': 2004, 'allows': 2005, 'grew': 2006, 'childhood': 2007, 'oh': 2008, 'current': 2009, 'Arthur': 2010, 'lies': 2011, 'scared': 2012, 'breaks': 2013, 'mind.': 2014, 'traditional': 2015, 'European': 2016, 'finished': 2017, 'Eddie': 2018, 'father,': 2019, 'aware': 2020, 'wrong,': 2021, 'dreams': 2022, 'dying': 2023, 'today.': 2024, \"one's\": 2025, 'sheer': 2026, 'unable': 2027, 'really,': 2028, 'house,': 2029, 'brutal': 2030, 'continues': 2031, 'Anyway,': 2032, 'Still,': 2033, 'hotel': 2034, 'moments,': 2035, 'suspect': 2036, 'discovers': 2037, 'bed': 2038, 'content': 2039, 'gonna': 2040, 'location': 2041, 'scientist': 2042, 'Before': 2043, 'Dead': 2044, 'bar': 2045, 'calls': 2046, 'Bad': 2047, 'shocking': 2048, 'Allen': 2049, 'forgotten': 2050, 'adults': 2051, 'noticed': 2052, 'learned': 2053, 'surprising': 2054, 'wooden': 2055, 'hits': 2056, 'everybody': 2057, 'finest': 2058, 'desire': 2059, 'through.': 2060, 'fan,': 2061, 'Al': 2062, 'lighting': 2063, 'Kevin': 2064, 'opens': 2065, \"70's\": 2066, 'epic': 2067, 'Andy': 2068, 'vampire': 2069, 'soul': 2070, 'regular': 2071, 'happening': 2072, 'originally': 2073, 'short,': 2074, 'Which': 2075, 'boyfriend': 2076, 'Young': 2077, 'loving': 2078, 'performances,': 2079, 'Johnny': 2080, 'speaking': 2081, 'bloody': 2082, 'confusing': 2083, 'victims': 2084, 'CGI': 2085, 'majority': 2086, 'arts': 2087, 'cliché': 2088, 'fail': 2089, 'Gene': 2090, 'ultimate': 2091, 'angry': 2092, 'screen,': 2093, 'music.': 2094, 'pop': 2095, 'THIS': 2096, 'double': 2097, 'anybody': 2098, 'plane': 2099, 'teen': 2100, 'bigger': 2101, 'Adam': 2102, 'driving': 2103, 'bits': 2104, 'necessary': 2105, \"Let's\": 2106, 'visit': 2107, 'Ray': 2108, 'manner': 2109, 'Instead,': 2110, 'to,': 2111, 'caused': 2112, 'island': 2113, 'steal': 2114, 'criminal': 2115, 'event': 2116, 'date': 2117, 'news': 2118, 'creates': 2119, 'seconds': 2120, 'reach': 2121, 'guy,': 2122, 'apartment': 2123, 'ending,': 2124, 'conclusion': 2125, 'super': 2126, 'right,': 2127, '(for': 2128, 'Kate': 2129, 'overly': 2130, 'painfully': 2131, 'watching.': 2132, 'dangerous': 2133, 'garbage': 2134, 'Paris': 2135, 'pleasure': 2136, 'friends,': 2137, 'performances.': 2138, 'met': 2139, 'Williams': 2140, 'West': 2141, 'born': 2142, 'enough.': 2143, 'received': 2144, 'dies': 2145, '(played': 2146, \"they've\": 2147, 'entertaining.': 2148, 'Nick': 2149, 'Jean': 2150, 'genuine': 2151, 'somebody': 2152, 'twice': 2153, 'century': 2154, 'alive': 2155, 'industry': 2156, 'cops': 2157, 'cry': 2158, 'fiction': 2159, \"director's\": 2160, 'faces': 2161, 'captured': 2162, 'involves': 2163, 'relate': 2164, 'collection': 2165, 'discovered': 2166, 'practically': 2167, 'race': 2168, 'personality': 2169, 'jump': 2170, '(The': 2171, 'awful,': 2172, 'believes': 2173, 'Anthony': 2174, 'sign': 2175, '/': 2176, 'Sean': 2177, 'detail': 2178, 'opera': 2179, 'filmmaker': 2180, 'teacher': 2181, 'more,': 2182, 'boss': 2183, 'utter': 2184, 'design': 2185, 'drama,': 2186, 'learns': 2187, 'heads': 2188, 'itself,': 2189, 'done,': 2190, 'Douglas': 2191, 'intriguing': 2192, 'movie?': 2193, 'hospital': 2194, 'animal': 2195, 'White': 2196, 'Spanish': 2197, 'see,': 2198, 'area': 2199, 'Jimmy': 2200, 'issue': 2201, 'energy': 2202, 'delightful': 2203, 'night,': 2204, 'largely': 2205, 'himself,': 2206, 'who,': 2207, 'reviewers': 2208, 'essentially': 2209, 'develop': 2210, 'ago.': 2211, 'capable': 2212, 'described': 2213, 'whilst': 2214, 'actions': 2215, 'asking': 2216, 'effects.': 2217, 'loose': 2218, 'eat': 2219, 'action.': 2220, 'crappy': 2221, 'direction,': 2222, 'fill': 2223, 'forces': 2224, 'commentary': 2225, 'therefore': 2226, 'roles.': 2227, 'laughable': 2228, 'ran': 2229, 'Earth': 2230, 'whenever': 2231, '/>Overall,': 2232, 'tend': 2233, 'job,': 2234, 'system': 2235, 'months': 2236, 'classic.': 2237, 'holes': 2238, ':': 2239, 'unnecessary': 2240, 'Lady': 2241, 'captures': 2242, 'part.': 2243, 'unknown': 2244, 'media': 2245, 'recommended': 2246, 'ship': 2247, 'deserved': 2248, 'history.': 2249, 'clothes': 2250, 'around.': 2251, 'okay': 2252, 'Mike': 2253, 'nicely': 2254, 'Davis': 2255, 'soap': 2256, 'others.': 2257, 'sight': 2258, 'pacing': 2259, 'lacking': 2260, 'picture.': 2261, 'Dick': 2262, 'portrays': 2263, 'fights': 2264, 'pair': 2265, '(but': 2266, 'initial': 2267, 'bright': 2268, 'god': 2269, 'remembered': 2270, 'proper': 2271, 'connection': 2272, 'combination': 2273, '(even': 2274, 'ugly': 2275, 'knowledge': 2276, 'portraying': 2277, 'treat': 2278, 'like,': 2279, 'Gary': 2280, 'money,': 2281, 'food': 2282, 'alien': 2283, 'aside': 2284, 'produce': 2285, 'seven': 2286, 'place,': 2287, 'tears': 2288, 'Sure,': 2289, 'army': 2290, 'tragedy': 2291, 'assume': 2292, 'engaging': 2293, 'Go': 2294, 'death.': 2295, 'walks': 2296, 'over.': 2297, 'justice': 2298, 'normally': 2299, 'flicks': 2300, 'passed': 2301, 'higher': 2302, 'Ann': 2303, 'occasionally': 2304, 'choose': 2305, 'Bob': 2306, 'steals': 2307, 'itself.': 2308, 'grow': 2309, 'creature': 2310, 'Part': 2311, 'home,': 2312, 'thin': 2313, 'anything.': 2314, 'included': 2315, 'ancient': 2316, 'lover': 2317, 'VHS': 2318, 'awkward': 2319, 'Jackson': 2320, 'away,': 2321, 'No,': 2322, 'reason,': 2323, 'bringing': 2324, 'built': 2325, 'crap.': 2326, 'Can': 2327, 'strongly': 2328, 'sell': 2329, 'brilliantly': 2330, 'quiet': 2331, 'trust': 2332, 'Steven': 2333, 'radio': 2334, 'count': 2335, 'stopped': 2336, '(although': 2337, 'study': 2338, 'foreign': 2339, 'folks': 2340, 'long.': 2341, 'losing': 2342, 'record': 2343, 'flaws': 2344, 'stock': 2345, 'relatively': 2346, 'unexpected': 2347, 'intelligence': 2348, 'absurd': 2349, 'Academy': 2350, 'night.': 2351, 'him.<br': 2352, 'send': 2353, 'virtually': 2354, 'below': 2355, 'explanation': 2356, 'kick': 2357, 'twenty': 2358, 'memories': 2359, 'graphic': 2360, 'Eric': 2361, 'cameo': 2362, 'agent': 2363, 'remain': 2364, 'fits': 2365, 'memory': 2366, 'ruined': 2367, 'Anyone': 2368, 'humor,': 2369, 'grown': 2370, 'gratuitous': 2371, 'smile': 2372, 'emotionally': 2373, 'delivered': 2374, 'dressed': 2375, 'gangster': 2376, 'land': 2377, 'gem': 2378, 'meeting': 2379, 'ground': 2380, 'players': 2381, 'eating': 2382, 'empty': 2383, 'vision': 2384, 'hurt': 2385, 'actor.': 2386, 'ordinary': 2387, '****': 2388, 'endless': 2389, 'director.': 2390, 'convinced': 2391, 'standing': 2392, 'soldier': 2393, 'See': 2394, 'serves': 2395, 'lines,': 2396, 'holding': 2397, 'worse.': 2398, 'claim': 2399, 'destroy': 2400, '/>You': 2401, 'violence,': 2402, 'Morgan': 2403, '/>At': 2404, '/>Not': 2405, 'saving': 2406, \"i'm\": 2407, 'joy': 2408, 'mouth': 2409, 'Jennifer': 2410, 'suffers': 2411, 'least,': 2412, 'artist': 2413, 'blind': 2414, 'spoil': 2415, 'curious': 2416, 'pretentious': 2417, 'bland': 2418, 'murdered': 2419, 'cuts': 2420, 'Watching': 2421, 'obsessed': 2422, '50': 2423, 'does.': 2424, 'station': 2425, 'about,': 2426, 'lives.': 2427, \"would've\": 2428, 'Taylor': 2429, 'humans': 2430, 'loss': 2431, 'pilot': 2432, 'themselves.': 2433, 'around,': 2434, 'Barbara': 2435, 'beautiful,': 2436, 'ever,': 2437, 'references': 2438, \"we've\": 2439, 'style,': 2440, 'law': 2441, 'down,': 2442, 'year.': 2443, 'mom': 2444, 'latest': 2445, 'hearing': 2446, 'Jones': 2447, '(though': 2448, '/>Even': 2449, 'Joseph': 2450, 'days.': 2451, 'heroes': 2452, 'broken': 2453, 'travel': 2454, 'conflict': 2455, 'Last': 2456, 'death,': 2457, 'Asian': 2458, 'talks': 2459, 'insult': 2460, 'length': 2461, 'accurate': 2462, 'chose': 2463, 'versions': 2464, 'son,': 2465, 'pleasant': 2466, 'bet': 2467, 'reaction': 2468, 'wide': 2469, 'remotely': 2470, 'strength': 2471, 'stupid,': 2472, 'Canadian': 2473, 'award': 2474, 'well.<br': 2475, '(especially': 2476, 'underrated': 2477, 'budget,': 2478, 'contemporary': 2479, 'Horror': 2480, 'horribly': 2481, 'drunk': 2482, 'Santa': 2483, 'U.S.': 2484, 'today,': 2485, 'marry': 2486, 'viewed': 2487, 'reviewer': 2488, 'mark': 2489, '/>On': 2490, 'chosen': 2491, 'Are': 2492, 'sleep': 2493, 'talents': 2494, 'theatrical': 2495, 'back,': 2496, 'realizes': 2497, 'Without': 2498, 'hide': 2499, 'ruin': 2500, 'anyway.': 2501, 'provided': 2502, 'passion': 2503, 'moments.': 2504, 'screaming': 2505, 'flick,': 2506, 'anywhere': 2507, 'sing': 2508, 'range': 2509, '/>After': 2510, 'occasional': 2511, 'house.': 2512, 'walked': 2513, 'presents': 2514, 'church': 2515, 'imagination': 2516, 'sense,': 2517, 'blonde': 2518, '12': 2519, 'thank': 2520, 'unfortunate': 2521, 'Overall,': 2522, 'Please': 2523, 'spends': 2524, 'scare': 2525, 'reason.': 2526, 'stars.': 2527, 'Being': 2528, 'suffering': 2529, 'partner': 2530, 'types': 2531, '/>However,': 2532, 'cant': 2533, 'excited': 2534, 'of.': 2535, 'cross': 2536, ')': 2537, 'depicted': 2538, 'gags': 2539, 'kinds': 2540, 'VERY': 2541, 'drugs': 2542, 'wall': 2543, 'United': 2544, 'yet,': 2545, 'hanging': 2546, 'edited': 2547, 'downright': 2548, 'sudden': 2549, 'disappointment': 2550, 'till': 2551, 'version,': 2552, 'Australian': 2553, 'which,': 2554, 'terrible,': 2555, 'humorous': 2556, 'process': 2557, 'draw': 2558, 'old,': 2559, 'experience.': 2560, 'summer': 2561, 'production.': 2562, 'Jackie': 2563, 'convince': 2564, 'Superman': 2565, 'planet': 2566, 'episode.': 2567, 'More': 2568, 'suicide': 2569, 'Then,': 2570, 'nominated': 2571, 'heroine': 2572, 'inner': 2573, 'cinema,': 2574, 'Keaton': 2575, 'Lord': 2576, 'revealed': 2577, 'DVD,': 2578, 'pile': 2579, 'are.': 2580, 'own.': 2581, 'Much': 2582, 'Woody': 2583, 'Story': 2584, 'results': 2585, 'vehicle': 2586, 'things.': 2587, 'player': 2588, 'skip': 2589, 'genre,': 2590, 'channel': 2591, 'pulls': 2592, 'struggling': 2593, 'suit': 2594, 'story.<br': 2595, 'rescue': 2596, 'Simon': 2597, 'accidentally': 2598, 'featured': 2599, 'claims': 2600, 'all.<br': 2601, 'individual': 2602, 'explained': 2603, 'visually': 2604, 'offered': 2605, 'Hong': 2606, 'Thomas': 2607, 'torture': 2608, 'unlikely': 2609, 'unfunny': 2610, 'closer': 2611, 'decision': 2612, 'clichés': 2613, '/>Now': 2614, '/>Some': 2615, 'satire': 2616, 'bank': 2617, 'massive': 2618, 'drama.': 2619, 'author': 2620, 'opposite': 2621, \"What's\": 2622, '/>Of': 2623, 'debut': 2624, 'ALL': 2625, 'private': 2626, 'magnificent': 2627, 'Tarzan': 2628, 'Jesus': 2629, 'woods': 2630, 'Albert': 2631, 'repeated': 2632, 'parody': 2633, 'Ford': 2634, 'one.<br': 2635, 'covered': 2636, 'plots': 2637, 'villains': 2638, 'commercial': 2639, 'Any': 2640, 'context': 2641, 'speaks': 2642, 'skills': 2643, 'disaster': 2644, 'know.': 2645, 'lovers': 2646, 'according': 2647, 'episode,': 2648, 'recognize': 2649, 'Roger': 2650, '/>Then': 2651, 'writing,': 2652, 'fat': 2653, 'focuses': 2654, '/>Another': 2655, 'something.': 2656, 'lower': 2657, 'Victor': 2658, 'Walter': 2659, 'source': 2660, 'cinematography,': 2661, 'evidence': 2662, 'naive': 2663, 'stupid.': 2664, 'Matt': 2665, 'multiple': 2666, 'line,': 2667, 'reveals': 2668, 'humor.': 2669, 'worse,': 2670, 'test': 2671, 'None': 2672, 'During': 2673, 'beginning,': 2674, 'lesson': 2675, 'segment': 2676, 'cable': 2677, 'sex,': 2678, 'program': 2679, 'stayed': 2680, 'Dark': 2681, 'owner': 2682, 'shot,': 2683, 'stuff.': 2684, 'paying': 2685, 'audience,': 2686, 'leader': 2687, 'directly': 2688, 'Western': 2689, 'lousy': 2690, 'me.<br': 2691, 'soft': 2692, 'stays': 2693, 'Luke': 2694, 'boy,': 2695, 'About': 2696, 'advice': 2697, 'Captain': 2698, 'community': 2699, 'Kong': 2700, 'Howard': 2701, 'and/or': 2702, 'instead.': 2703, 'think,': 2704, 'spite': 2705, 'dated': 2706, '/>-': 2707, 'friendship': 2708, 'Daniel': 2709, 'gory': 2710, 'of,': 2711, 'clue': 2712, 'Saturday': 2713, 'plans': 2714, 'friends.': 2715, 'fashion': 2716, 'nothing.': 2717, 'survive': 2718, 'explains': 2719, 'standards': 2720, 'club': 2721, 'rise': 2722, 'judge': 2723, 'first.': 2724, 'recall': 2725, '/>That': 2726, 'Prince': 2727, 'favor': 2728, 'High': 2729, 'ridiculous.': 2730, 'figured': 2731, 'theatre': 2732, 'days,': 2733, 'handled': 2734, 'Robin': 2735, 'Freddy': 2736, 'blue': 2737, 'Dan': 2738, 'exist': 2739, 'Irish': 2740, 'children,': 2741, 'influence': 2742, 'formula': 2743, 'cars': 2744, 'football': 2745, '/>We': 2746, 'Louis': 2747, 'vote': 2748, 'shallow': 2749, 'focused': 2750, 'did,': 2751, 'appropriate': 2752, 'touches': 2753, 'behavior': 2754, 'necessarily': 2755, 'handsome': 2756, 'realism': 2757, 'wise': 2758, 'facial': 2759, 'reveal': 2760, 'heavily': 2761, 'bothered': 2762, \"You'll\": 2763, 'Broadway': 2764, 'sympathetic': 2765, 'mind,': 2766, 'possible.': 2767, 'green': 2768, 'cost': 2769, 'hired': 2770, 'this.<br': 2771, 'THAT': 2772, 'else,': 2773, 'treatment': 2774, 'Donald': 2775, 'novel,': 2776, 'TV.': 2777, 'legendary': 2778, 'sounded': 2779, 'very,': 2780, 'home.': 2781, 'machine': 2782, 'amateur': 2783, \"/>Don't\": 2784, 'hilarious.': 2785, 'excellent.': 2786, 'starred': 2787, 'mainstream': 2788, 'contrast': 2789, 'designed': 2790, 'baseball': 2791, 'mess.': 2792, 'shocked': 2793, 'dozen': 2794, 'exact': 2795, 'officer': 2796, 'research': 2797, 'regret': 2798, 'Especially': 2799, 'Each': 2800, 'words,': 2801, 'spectacular': 2802, 'friend,': 2803, 'Patrick': 2804, 'horse': 2805, 'exploitation': 2806, 'sports': 2807, 'throws': 2808, 'significant': 2809, 'buying': 2810, 'dubbed': 2811, \"he'd\": 2812, 'fate': 2813, 'mere': 2814, 'greater': 2815, \"could've\": 2816, 'drop': 2817, 'board': 2818, \"father's\": 2819, \"They're\": 2820, 'African': 2821, 'Mrs.': 2822, 'ladies': 2823, 'attitude': 2824, '(at': 2825, 'depiction': 2826, 'experienced': 2827, 'Let': 2828, 'post': 2829, 'wedding': 2830, 'placed': 2831, 'things,': 2832, 'career.': 2833, 'guns': 2834, 'model': 2835, 'trapped': 2836, 'supernatural': 2837, 'Day': 2838, 'cutting': 2839, 'Have': 2840, 'San': 2841, 'amazed': 2842, 'appealing': 2843, 'Roy': 2844, 'Never': 2845, 'faith': 2846, 'depressing': 2847, 'excellent,': 2848, 'Hitler': 2849, 'morning': 2850, 'picks': 2851, 'reminiscent': 2852, 'warm': 2853, 'lucky': 2854, 'over,': 2855, 'II': 2856, 'Edward': 2857, 'Anne': 2858, 'zero': 2859, 'desperately': 2860, 'embarrassing': 2861, 'entertainment.': 2862, 'accident': 2863, 'wit': 2864, 'voices': 2865, 'Alex': 2866, 'seriously.': 2867, 'irritating': 2868, 'roles,': 2869, 'passing': 2870, 'Nancy': 2871, 'daughter,': 2872, 'so-called': 2873, 'over-the-top': 2874, 'veteran': 2875, '/>With': 2876, 'Cage': 2877, 'enjoying': 2878, 'dry': 2879, 'helping': 2880, 'ring': 2881, 'combined': 2882, 'Life': 2883, 'wear': 2884, 'forever': 2885, '(he': 2886, 'roll': 2887, 'abandoned': 2888, 'this?': 2889, 'Washington': 2890, 'games': 2891, 'previously': 2892, 'committed': 2893, 'protagonist': 2894, 'fictional': 2895, \"people's\": 2896, 'laugh.': 2897, 'display': 2898, 'Thank': 2899, 'fly': 2900, 'TO': 2901, 'dreadful': 2902, 'carries': 2903, 'Sometimes': 2904, 'girl.': 2905, 'others,': 2906, 'mother,': 2907, 'witty': 2908, 'carried': 2909, 'drag': 2910, 'guy.': 2911, 'required': 2912, 'la': 2913, 'real.': 2914, 'fault': 2915, '(including': 2916, 'suffer': 2917, 'efforts': 2918, 'international': 2919, 'identity': 2920, '(it': 2921, 'out.<br': 2922, 'laugh,': 2923, 'gore,': 2924, 'join': 2925, 'youth': 2926, 'war,': 2927, 'convey': 2928, 'serve': 2929, 'prepared': 2930, 'mission': 2931, 'native': 2932, 'version.': 2933, 'locations': 2934, 'blown': 2935, 'IN': 2936, 'decade': 2937, 'Kim': 2938, 'refreshing': 2939, 'instance,': 2940, 'guilty': 2941, 'powers': 2942, 'purely': 2943, 'sake': 2944, 'prefer': 2945, 'children.': 2946, 'round': 2947, 'product': 2948, 'stolen': 2949, 'changing': 2950, 'satisfying': 2951, 'driven': 2952, 'foot': 2953, 'listening': 2954, 'grand': 2955, 'Wayne': 2956, 'women,': 2957, 'paint': 2958, 'face.': 2959, 'renting': 2960, 'that.<br': 2961, 'village': 2962, 'via': 2963, 'perspective': 2964, 'kids,': 2965, 'us.': 2966, 'deeper': 2967, 'nude': 2968, 'technology': 2969, 'prior': 2970, 'seek': 2971, 'clean': 2972, 'beloved': 2973, 'faithful': 2974, 'Old': 2975, 'Marie': 2976, 'film-making': 2977, 'corny': 2978, 'mine': 2979, 'throwing': 2980, 'training': 2981, 'actress,': 2982, 'feet': 2983, 'logic': 2984, 'streets': 2985, 'production,': 2986, 'another.': 2987, '/>So,': 2988, 'reporter': 2989, 'frame': 2990, 'experiences': 2991, 'touched': 2992, 'yes': 2993, 'say.': 2994, 'way.<br': 2995, 'watchable': 2996, 'remind': 2997, 'sympathy': 2998, 'tape': 2999, 'print': 3000, 'Oliver': 3001, 'for,': 3002, 'protect': 3003, 'closing': 3004, 'another,': 3005, 'tiny': 3006, 'Time': 3007, 'carrying': 3008, 'finale': 3009, 'movie:': 3010, 'insane': 3011, 'attempting': 3012, 'center': 3013, 'besides': 3014, 'haunted': 3015, 'slow,': 3016, 'insight': 3017, 'flashbacks': 3018, '80s': 3019, 'Anna': 3020, 'TV,': 3021, 'Sinatra': 3022, 'us,': 3023, 'regarding': 3024, 'halfway': 3025, 'past,': 3026, 'predictable,': 3027, 'stereotypes': 3028, 'visuals': 3029, 'matters': 3030, 'here.<br': 3031, 'raised': 3032, 'monsters': 3033, 'Come': 3034, 'comparison': 3035, 'false': 3036, 'wife.': 3037, 'causes': 3038, '2.': 3039, 'film?': 3040, 'why.': 3041, 'floor': 3042, 'no,': 3043, 'happened.': 3044, 'dark,': 3045, 'surreal': 3046, 'Bond': 3047, 'Ron': 3048, 'relief': 3049, 'Rob': 3050, 'Texas': 3051, 'throughout.': 3052, 'school.': 3053, 'chick': 3054, 'highlight': 3055, 'jumps': 3056, 'critical': 3057, 'kids.': 3058, 'witness': 3059, 'promising': 3060, 'concerned': 3061, 'magical': 3062, 'Columbo': 3063, 'raise': 3064, 'brother,': 3065, 'while,': 3066, 'Okay,': 3067, 'Mexican': 3068, 'facts': 3069, 'revolves': 3070, 'embarrassed': 3071, 'dull,': 3072, 'Get': 3073, 'true.': 3074, 'replaced': 3075, \"/>There's\": 3076, 'haunting': 3077, 'Lynch': 3078, 'variety': 3079, 'Parker': 3080, 'existence': 3081, 'security': 3082, '/>Although': 3083, 'anything,': 3084, 'humanity': 3085, 'performed': 3086, 'disgusting': 3087, 'Avoid': 3088, 'horrible.': 3089, 'understood': 3090, 'Me': 3091, 'wears': 3092, 'entertaining,': 3093, 'costume': 3094, 'harsh': 3095, 'families': 3096, 'crying': 3097, \"children's\": 3098, 'account': 3099, 'Your': 3100, 'contrived': 3101, 'figures': 3102, 'side,': 3103, 'cash': 3104, 'film!': 3105, 'life.<br': 3106, 'sharp': 3107, 'warn': 3108, 'rental': 3109, 'makeup': 3110, 'singer': 3111, 'grade': 3112, 'wasting': 3113, 'cultural': 3114, 'Bourne': 3115, 'Welles': 3116, 'effectively': 3117, '11': 3118, 'Harris': 3119, 'mildly': 3120, 'safe': 3121, 'ignore': 3122, 'amongst': 3123, 'refuses': 3124, 'proud': 3125, 'sorts': 3126, 'glimpse': 3127, 'bear': 3128, 'blow': 3129, 'believed': 3130, 'Japan': 3131, 'later.': 3132, 'Russell': 3133, 'luck': 3134, 'head.': 3135, 'father.': 3136, 'Brad': 3137, 'breaking': 3138, 'woman.': 3139, 'lesbian': 3140, 'thoughts': 3141, 'handle': 3142, 'Does': 3143, 'achieve': 3144, 'Julie': 3145, 'determined': 3146, 'England': 3147, 'Fox': 3148, 'anime': 3149, 'UK': 3150, 'site': 3151, 'asleep': 3152, 'forgot': 3153, 'window': 3154, 'teenagers': 3155, 'calling': 3156, 'adding': 3157, 'sold': 3158, 'amateurish': 3159, 'fans.': 3160, 'make-up': 3161, 'heart.': 3162, 'school,': 3163, 'executed': 3164, 'twisted': 3165, 'warning': 3166, 'pity': 3167, 'mentally': 3168, 'suffered': 3169, 'boat': 3170, 'lawyer': 3171, 'Jon': 3172, 'score,': 3173, 'medical': 3174, 'weeks': 3175, 'game.': 3176, 'Stone': 3177, 'indie': 3178, 'Victoria': 3179, '/>First': 3180, 'speech': 3181, 'NO': 3182, 'Powell': 3183, 'erotic': 3184, 'succeeds': 3185, 'witch': 3186, 'stereotypical': 3187, 'direction.': 3188, 'Dean': 3189, 'amazingly': 3190, 'correct': 3191, 'consists': 3192, 'May': 3193, 'lot.': 3194, 'quest': 3195, 'Shakespeare': 3196, 'Death': 3197, 'Full': 3198, 'encounter': 3199, 'drives': 3200, 'field': 3201, 'priest': 3202, 'laughter': 3203, 'Dennis': 3204, 'complicated': 3205, 'gold': 3206, '25': 3207, 'again.<br': 3208, 'end.<br': 3209, 'horror,': 3210, 'ripped': 3211, 'Had': 3212, 'sensitive': 3213, 'mature': 3214, 'creatures': 3215, 'crowd': 3216, 'Max': 3217, 'history,': 3218, 'Rachel': 3219, 'seasons': 3220, 'Take': 3221, 'camera.': 3222, 'shop': 3223, 'annoying.': 3224, 'Unlike': 3225, 'inept': 3226, 'learning': 3227, 'fallen': 3228, 'Warner': 3229, 'Jewish': 3230, 'Trek': 3231, 'attacked': 3232, 'Still': 3233, \"ain't\": 3234, 'go.': 3235, 'vs.': 3236, 'Three': 3237, 'offensive': 3238, 'Over': 3239, 'brave': 3240, 'DO': 3241, 'Our': 3242, 'served': 3243, 'contain': 3244, 'deadly': 3245, 'dirty': 3246, 'target': 3247, 'men,': 3248, 'locked': 3249, 'stars,': 3250, 'Drew': 3251, 'town.': 3252, 'essential': 3253, 'year,': 3254, 'that?': 3255, 'BBC': 3256, 'Leslie': 3257, 'sum': 3258, 'price': 3259, 'Moore': 3260, 'destroyed': 3261, 'sinister': 3262, 'established': 3263, 'express': 3264, 'level.': 3265, 'separate': 3266, '/>Anyway,': 3267, 'continuity': 3268, 'Sure': 3269, 'grace': 3270, 'related': 3271, 'intellectual': 3272, 'excitement': 3273, 'bodies': 3274, 'war.': 3275, 'abuse': 3276, 'Lost': 3277, 'different.': 3278, 'section': 3279, 'violence.': 3280, 'dislike': 3281, 'MGM': 3282, 'remote': 3283, 'Julia': 3284, 'whatsoever.': 3285, 'mistakes': 3286, 'ones.': 3287, 'written,': 3288, 'views': 3289, 'tied': 3290, 'North': 3291, 'desert': 3292, 'letting': 3293, 'predictable.': 3294, 'reference': 3295, 'struggles': 3296, 'past.': 3297, 'shot.': 3298, '70s': 3299, 'works.': 3300, 'qualities': 3301, 'Sunday': 3302, 'rough': 3303, 'Halloween': 3304, 'myself,': 3305, 'Definitely': 3306, 'seat': 3307, 'clips': 3308, 'something,': 3309, 'introduction': 3310, 'routine': 3311, 'Why?': 3312, 'appreciated': 3313, 'uncle': 3314, 'propaganda': 3315, 'forth': 3316, 'sends': 3317, 'dialogue.': 3318, 'examples': 3319, 'interview': 3320, 'overcome': 3321, 'reputation': 3322, 'title,': 3323, 'failure': 3324, 'ways,': 3325, 'concerns': 3326, 'alone.': 3327, 'Cinderella': 3328, 'classics': 3329, 'notable': 3330, 'path': 3331, 'tedious': 3332, 'teenager': 3333, 'connected': 3334, '14': 3335, 'conversation': 3336, 'face,': 3337, 'hundreds': 3338, 'Someone': 3339, 'regard': 3340, 'on.<br': 3341, 'infamous': 3342, 'frequently': 3343, 'everyday': 3344, 'case.': 3345, 'fans,': 3346, 'skin': 3347, 'Sir': 3348, 'Sidney': 3349, 'happen.': 3350, 'Sarah': 3351, 'crude': 3352, 'perfect.': 3353, 'future.': 3354, 'garbage.': 3355, 'delivery': 3356, 'eight': 3357, 'Larry': 3358, 'dealt': 3359, 'heck': 3360, 'position': 3361, 'believable.': 3362, 'eyes,': 3363, 'hint': 3364, 'ball': 3365, 'degree': 3366, 'saves': 3367, 'picture,': 3368, 'searching': 3369, 'ashamed': 3370, 'interest.': 3371, 'fan.': 3372, 'extraordinary': 3373, 'interpretation': 3374, 'broke': 3375, 'gruesome': 3376, 'generation': 3377, 'Burt': 3378, 'greatly': 3379, 'seeking': 3380, 'checking': 3381, 'quality.': 3382, 'challenge': 3383, 'Ryan': 3384, 'core': 3385, 'Street': 3386, 'perform': 3387, 'bomb': 3388, 'Friday': 3389, 'anymore.': 3390, 'description': 3391, 'wealthy': 3392, 'stops': 3393, 'act.': 3394, 'Something': 3395, 'Overall': 3396, 'sucked': 3397, 'REALLY': 3398, 'statement': 3399, 'Such': 3400, 'rubbish': 3401, 'unrealistic': 3402, 'career,': 3403, 'sequels': 3404, 'flesh': 3405, '(one': 3406, 'play,': 3407, 'Jessica': 3408, 'lesser': 3409, 'dragged': 3410, 'device': 3411, 'pointed': 3412, 'gas': 3413, 'prime': 3414, 'friendly': 3415, 'Gordon': 3416, 'person,': 3417, 'through,': 3418, 'women.': 3419, 'hundred': 3420, 'nonsense': 3421, 'style.': 3422, '/>From': 3423, 'play.': 3424, 'opened': 3425, 'Ned': 3426, 'stories,': 3427, 'arrives': 3428, 'editing,': 3429, 'Wars': 3430, 'angles': 3431, 'wind': 3432, 'successfully': 3433, 'freedom': 3434, 'lonely': 3435, 'thousands': 3436, 'husband,': 3437, 'campy': 3438, 'expert': 3439, 'disappointment.': 3440, \"woman's\": 3441, 'is.<br': 3442, 'gritty': 3443, 'sleeping': 3444, 'Brothers': 3445, 'Anderson': 3446, 'then.': 3447, 'Be': 3448, 'Bette': 3449, 'Murphy': 3450, 'sad,': 3451, 'escapes': 3452, 'poor,': 3453, 'cares': 3454, \"i've\": 3455, 'teach': 3456, 'wishes': 3457, 'horrific': 3458, 'Stanley': 3459, 'develops': 3460, 'sleazy': 3461, 'Ralph': 3462, 'solve': 3463, 'Nazi': 3464, 'surrounding': 3465, 'entertain': 3466, 'Lisa': 3467, 'rip': 3468, 'person.': 3469, '10/10': 3470, 'Zombie': 3471, 'spoof': 3472, 'Michelle': 3473, 'lines.': 3474, 'ill': 3475, 'moment,': 3476, 'gain': 3477, 'material.': 3478, 'honest,': 3479, 'mindless': 3480, 'Susan': 3481, 'Along': 3482, 'amazing.': 3483, 'Claire': 3484, 'birth': 3485, 'clichéd': 3486, 'deaths': 3487, 'suspenseful': 3488, 'breath': 3489, 'cynical': 3490, 'execution': 3491, 'real,': 3492, 'dialog,': 3493, 'ass': 3494, 'Green': 3495, 'Police': 3496, 'horrible,': 3497, 'physically': 3498, 'countless': 3499, 'Things': 3500, 'J.': 3501, 'levels': 3502, 'minds': 3503, 'Probably': 3504, ':)': 3505, 'criticism': 3506, 'old.': 3507, 'increasingly': 3508, 'usual,': 3509, '/>the': 3510, 'child.': 3511, 'Blood': 3512, 'Ted': 3513, 'shines': 3514, 'structure': 3515, 'themselves,': 3516, 'shoots': 3517, 'upset': 3518, '(that': 3519, 'lazy': 3520, 'FBI': 3521, 'plastic': 3522, 'speed': 3523, 'Lewis': 3524, 'par': 3525, 'idea.': 3526, 'sets,': 3527, 'Wood': 3528, 'skill': 3529, 'narration': 3530, 'slapstick': 3531, 'Grant': 3532, 'Maria': 3533, 'Bo': 3534, 'portrait': 3535, 'ensemble': 3536, 'answers': 3537, 'thriller,': 3538, 'uncomfortable': 3539, 'mass': 3540, 'hoped': 3541, 'sure,': 3542, 'suspense,': 3543, 'initially': 3544, 'lights': 3545, 'promise': 3546, 'spending': 3547, 'flashback': 3548, 'associated': 3549, 'drinking': 3550, 'troubled': 3551, 'unfortunately,': 3552, 'belongs': 3553, 'wonderful.': 3554, 'Kurt': 3555, 'Killer': 3556, 'spoken': 3557, 'once,': 3558, 'awards': 3559, 'naturally': 3560, 'child,': 3561, 'market': 3562, 'accents': 3563, 'opinion.': 3564, 'fairy': 3565, 'advantage': 3566, 'Army': 3567, 'Vietnam': 3568, 'own,': 3569, 'fabulous': 3570, 'base': 3571, 'anger': 3572, 'Jr.': 3573, 'Sadly,': 3574, 'adapted': 3575, 'interviews': 3576, 'surrounded': 3577, 'Freeman': 3578, 'stood': 3579, 'either,': 3580, 'factor': 3581, 'YOU': 3582, 'miles': 3583, 'grows': 3584, 'politics': 3585, 'viewer.': 3586, 'inspiration': 3587, 'hang': 3588, 'like.': 3589, 'uninteresting': 3590, 'covers': 3591, 'Elizabeth': 3592, 'praise': 3593, 'storyline,': 3594, 'Give': 3595, 'reality,': 3596, 'attempted': 3597, 'La': 3598, 'expression': 3599, 'yeah,': 3600, 'episodes,': 3601, 'Yes': 3602, 'fare': 3603, 'teens': 3604, 'shows.': 3605, 'caring': 3606, 'entertained': 3607, 'status': 3608, 'allowing': 3609, 'says,': 3610, 'movement': 3611, 'aged': 3612, '(of': 3613, 'enters': 3614, 'religion': 3615, 'pregnant': 3616, \"they'd\": 3617, 'happily': 3618, 'what?': 3619, 'nudity,': 3620, 'bus': 3621, 'pleasantly': 3622, 'term': 3623, 'unconvincing': 3624, 'suggests': 3625, 'right?': 3626, 'talent.': 3627, 'annoyed': 3628, 'cartoons': 3629, 'obsession': 3630, 'classic,': 3631, 'etc': 3632, 'go,': 3633, 'Philip': 3634, 'Germany': 3635, 'Bettie': 3636, 'opposed': 3637, 'believing': 3638, 'chance.': 3639, 'sings': 3640, 'balance': 3641, 'tight': 3642, 'identify': 3643, 'ridiculously': 3644, 'goofy': 3645, 'festival': 3646, 'town,': 3647, 'represents': 3648, 'marvelous': 3649, 'tradition': 3650, 'lying': 3651, 'tribute': 3652, 'Golden': 3653, 'budget.': 3654, 'hiding': 3655, 'there.<br': 3656, 'equal': 3657, 'SO': 3658, 'sequel.': 3659, 'Show': 3660, 'legend': 3661, 'realise': 3662, '1st': 3663, 'Bollywood': 3664, '\"What': 3665, 'stylish': 3666, 'game,': 3667, 'werewolf': 3668, 'extras': 3669, 'frightening': 3670, 'quirky': 3671, 'evening': 3672, 'riding': 3673, 'Here,': 3674, 'bitter': 3675, 'flow': 3676, 'wins': 3677, 'good.<br': 3678, 'masterpiece.': 3679, 'have.': 3680, 'daily': 3681, 'nice,': 3682, 'accepted': 3683, 'mountain': 3684, 'rolling': 3685, 'mob': 3686, 'essence': 3687, 'Alice': 3688, 'prevent': 3689, 'imagery': 3690, 'young,': 3691, 'surprises': 3692, 'thousand': 3693, 'everyone.': 3694, 'substance': 3695, 'steps': 3696, 'rules': 3697, 'Andrew': 3698, 'millions': 3699, 'contact': 3700, 'Queen': 3701, 'bore': 3702, 'torn': 3703, '1.': 3704, 'handful': 3705, 'enter': 3706, 'artists': 3707, 'C.': 3708, 'rival': 3709, 'nuclear': 3710, 'far,': 3711, 'attention.': 3712, 'dollars': 3713, 'tons': 3714, 'welcome': 3715, 'aforementioned': 3716, 'attracted': 3717, 'urban': 3718, 'matter.': 3719, 'Chan': 3720, 'specific': 3721, 'chasing': 3722, 'does,': 3723, '\"I\\'m': 3724, 'horror.': 3725, '/>By': 3726, 'reduced': 3727, 'Rose': 3728, 'striking': 3729, 'ironic': 3730, 'requires': 3731, 'Emma': 3732, 'sexually': 3733, 'hopefully': 3734, 'seriously,': 3735, 'guessing': 3736, 'directorial': 3737, 'danger': 3738, 'oil': 3739, 'cell': 3740, 'beautiful.': 3741, 'raw': 3742, 'split': 3743, 'bound': 3744, 'comical': 3745, '10,': 3746, 'blood,': 3747, 'thriller.': 3748, 'thought,': 3749, 'Europe': 3750, 'Apparently': 3751, 'dollar': 3752, 'strictly': 3753, 'Living': 3754, 'wondered': 3755, 'struck': 3756, 'paced': 3757, 'fourth': 3758, 'Back': 3759, 'release.': 3760, 'really.': 3761, 'neat': 3762, 'risk': 3763, '/>Well,': 3764, 'teeth': 3765, 'dress': 3766, 'underground': 3767, 'States': 3768, 'pushed': 3769, 'adventures': 3770, 'Pacino': 3771, 'strangely': 3772, 'shots,': 3773, 'theory': 3774, 'Pitt': 3775, 'Look': 3776, 'remaining': 3777, 'repeat': 3778, 'talked': 3779, 'problems,': 3780, 'eyes.': 3781, 'task': 3782, 'Almost': 3783, 'theaters': 3784, 'enjoyable.': 3785, 'profound': 3786, 'appearing': 3787, 'funnier': 3788, 'Wilson': 3789, 'Lugosi': 3790, 'moment.': 3791, 'Sci-Fi': 3792, 'problem.': 3793, 'kicks': 3794, 'intensity': 3795, 'Jay': 3796, '(John': 3797, '(except': 3798, 'Movies': 3799, 'release,': 3800, 'pulling': 3801, 'real-life': 3802, 'anyway,': 3803, 'idea,': 3804, 'First,': 3805, 'paper': 3806, 'Caine': 3807, 'rights': 3808, 'basis': 3809, 'metal': 3810, 'courage': 3811, 'chief': 3812, 'entertainment,': 3813, 'video.': 3814, 'rid': 3815, 'Catherine': 3816, 'page': 3817, 'subsequent': 3818, 'commit': 3819, 'screening': 3820, 'heart,': 3821, 'Page': 3822, 'object': 3823, 'circumstances': 3824, 'castle': 3825, 'draws': 3826, 'once.': 3827, 'per': 3828, 'buddy': 3829, \"here's\": 3830, 'slight': 3831, 'fifteen': 3832, 'Finally,': 3833, 'drags': 3834, 'Guy': 3835, 'entry': 3836, 'wanna': 3837, 'her.<br': 3838, 'text': 3839, 'Clark': 3840, 'lot,': 3841, 'yourself.': 3842, 'too.<br': 3843, 'Music': 3844, 'Stan': 3845, 'expensive': 3846, 'performing': 3847, '20th': 3848, 'Really': 3849, 'Sky': 3850, 'sisters': 3851, 'tremendous': 3852, 'anyone.': 3853, 'scary,': 3854, 'recommended.': 3855, 'reasonably': 3856, 'topic': 3857, 'controversial': 3858, 'colors': 3859, 'authentic': 3860, 'killer,': 3861, 'hitting': 3862, 'Hollywood.': 3863, 'IT': 3864, 'sitcom': 3865, 'silly,': 3866, 'Soon': 3867, 'Korean': 3868, 'Award': 3869, 'achieved': 3870, 'unforgettable': 3871, 'Reed': 3872, 'parts,': 3873, 'Uncle': 3874, 'mask': 3875, '(this': 3876, 'Channel': 3877, 'Again,': 3878, 'homeless': 3879, 'mid': 3880, 'murderer': 3881, 'reactions': 3882, 'threw': 3883, \"/>I've\": 3884, 'idiot': 3885, 'watches': 3886, 'film;': 3887, 'television.': 3888, 'dropped': 3889, 'ways.': 3890, 'quote': 3891, 'larger': 3892, 'river': 3893, 'sole': 3894, 'shut': 3895, 'Hitchcock': 3896, 'lacked': 3897, 'sheriff': 3898, 'forgive': 3899, 'criminals': 3900, 'punch': 3901, 'spy': 3902, 'crafted': 3903, 'charismatic': 3904, 'everything.': 3905, 'tense': 3906, 'at.': 3907, 'seeing.': 3908, 'France': 3909, 'matter,': 3910, 'frankly': 3911, 'definite': 3912, 'racist': 3913, 'killer.': 3914, 'line.': 3915, 'obnoxious': 3916, 'hours.': 3917, 'revealing': 3918, 'screenwriter': 3919, 'partly': 3920, 'faced': 3921, 'aimed': 3922, 'movies.<br': 3923, 'graphics': 3924, 'stealing': 3925, 'two.': 3926, 'thrilling': 3927, 'melodrama': 3928, 'sequence,': 3929, 'rule': 3930, 'relevant': 3931, 'sides': 3932, 'Long': 3933, 'jokes,': 3934, 'listed': 3935, 'Hardy': 3936, 'closely': 3937, 'complaint': 3938, 'notorious': 3939, 'men.': 3940, 'retarded': 3941, 'laid': 3942, 'attached': 3943, 'beating': 3944, 'Universal': 3945, 'society.': 3946, '/>Despite': 3947, 'jobs': 3948, 'Science': 3949, 'killers': 3950, 'flawed': 3951, 'wonders': 3952, '13': 3953, 'car,': 3954, 'fitting': 3955, 'kid,': 3956, 'better.<br': 3957, 'providing': 3958, 'choices': 3959, 'murder,': 3960, 'busy': 3961, 'packed': 3962, 'stories.': 3963, 'host': 3964, 'credible': 3965, 'van': 3966, 'eerie': 3967, 'national': 3968, 'picking': 3969, 'ages': 3970, 'Catholic': 3971, 'idiotic': 3972, 'bad.<br': 3973, 'forgettable': 3974, 'guys,': 3975, 'same.': 3976, 'Blair': 3977, 'talent,': 3978, 'whoever': 3979, 'continued': 3980, 'stronger': 3981, 'sword': 3982, 'scary.': 3983, 'age,': 3984, 'NEVER': 3985, 'dead.': 3986, 'sucks': 3987, 'level,': 3988, 'truck': 3989, 'push': 3990, 'u': 3991, 'stunt': 3992, 'Pretty': 3993, 'chilling': 3994, 'Dorothy': 3995, 'gripping': 3996, 'Jeremy': 3997, 'reality.': 3998, 'scenario': 3999, 'terrifying': 4000, 'film:': 4001, 'daughters': 4002, 'enjoys': 4003, 'imaginative': 4004, 'dig': 4005, 'worthwhile': 4006, 'belief': 4007, 'age.': 4008, 'Timothy': 4009, 'scream': 4010, 'film)': 4011, 'fool': 4012, 'head,': 4013, 'displays': 4014, 'receive': 4015, 'Helen': 4016, \"show's\": 4017, 'Yeah,': 4018, 'Los': 4019, 'is:': 4020, 'concerning': 4021, 'accused': 4022, 'perfect,': 4023, 'easier': 4024, 'corrupt': 4025, 'stomach': 4026, 'competent': 4027, 'planning': 4028, 'directing,': 4029, '(to': 4030, 'Out': 4031, 'lovable': 4032, 'elderly': 4033, 'cruel': 4034, 'brilliant,': 4035, 'pleased': 4036, 'decades': 4037, 'parts.': 4038, 'trick': 4039, 'Laurel': 4040, 'Evil': 4041, 'scares': 4042, 'scale': 4043, 'messages': 4044, 'ought': 4045, 'shape': 4046, 'admire': 4047, 'lie': 4048, 'fame': 4049, 'effort.': 4050, 'resembles': 4051, 'cool,': 4052, 'proof': 4053, 'God,': 4054, 'meaningful': 4055, 'romance,': 4056, 'poignant': 4057, 'settings': 4058, 'Besides': 4059, 'Highly': 4060, 'attacks': 4061, 'America.': 4062, 'Tracy': 4063, 'enjoyment': 4064, 'because,': 4065, 'Stanwyck': 4066, 'arm': 4067, 'costs.': 4068, \"world's\": 4069, 'extended': 4070, 'true,': 4071, 'instantly': 4072, 'laughable.': 4073, 'name.': 4074, 'stuff,': 4075, 'hole': 4076, 'explore': 4077, 'returning': 4078, 'Blue': 4079, 'buried': 4080, 'joke.': 4081, 'happen,': 4082, 'help.': 4083, 'weapons': 4084, 'jumping': 4085, 'mother.': 4086, 'blend': 4087, 'start.': 4088, 'pretend': 4089, 'sea': 4090, 'Mel': 4091, 'Lucy': 4092, 'Denzel': 4093, '\"You': 4094, 'reached': 4095, 'short.': 4096, 'shows,': 4097, 'personalities': 4098, 'crash': 4099, 'Ian': 4100, 'sister,': 4101, 'National': 4102, 'wake': 4103, 'Vincent': 4104, 'video,': 4105, 'exists': 4106, 'relies': 4107, 'dogs': 4108, 'Park': 4109, 'string': 4110, 'gotta': 4111, 'segments': 4112, 'Seagal': 4113, 'homage': 4114, 'girls,': 4115, 'Cooper': 4116, 'unintentionally': 4117, 'psycho': 4118, 'cared': 4119, 'junk': 4120, 'hero,': 4121, 'extent': 4122, 'enjoy.': 4123, 'frustrated': 4124, 'patient': 4125, 'carefully': 4126, 'atmosphere,': 4127, '/>He': 4128, '(no': 4129, 'have,': 4130, 'Yet,': 4131, 'encounters': 4132, 'sophisticated': 4133, 'exercise': 4134, 'aired': 4135, 'expressions': 4136, 'think.': 4137, 'camera,': 4138, 'dare': 4139, 'charge': 4140, 'driver': 4141, 'conclusion,': 4142, 'devoted': 4143, 'throughout,': 4144, 'General': 4145, 'precious': 4146, 'service': 4147, 'weekend': 4148, 'spell': 4149, 'distant': 4150, 'cameos': 4151, 'development,': 4152, 'dubbing': 4153, 'sticks': 4154, 'arms': 4155, 'Whether': 4156, 'it)': 4157, 'hire': 4158, '2,': 4159, 'ridiculous,': 4160, 'R': 4161, 'this:': 4162, 'Brooks': 4163, 'thirty': 4164, 'concert': 4165, 'Sandler': 4166, 'Neil': 4167, 'Jamie': 4168, 'Wes': 4169, 'star.': 4170, 'colorful': 4171, '(such': 4172, 'spiritual': 4173, 'while.': 4174, 'albeit': 4175, 'technically': 4176, 'Eva': 4177, 'films.<br': 4178, 'brilliant.': 4179, 'treasure': 4180, 'stretch': 4181, 'Johnson': 4182, 'dude': 4183, 'Africa': 4184, 'musicals': 4185, 'load': 4186, 'hilarious,': 4187, 'ratings': 4188, 'unbelievably': 4189, 'manager': 4190, 'stated': 4191, 'evident': 4192, 'sentimental': 4193, 'appearances': 4194, 'Hollywood,': 4195, 'knock': 4196, 'scientists': 4197, 'kung': 4198, 'rural': 4199, 'Spike': 4200, 'accomplished': 4201, 'robot': 4202, 'laughs,': 4203, 'importance': 4204, 'vampires': 4205, 'ice': 4206, 'atmospheric': 4207, 'gradually': 4208, '1950s': 4209, 'countries': 4210, 'forest': 4211, 'viewing.': 4212, 'mansion': 4213, 'novel.': 4214, 'Meanwhile,': 4215, 'bit,': 4216, 'typically': 4217, 'darker': 4218, 'bit.': 4219, 'MST3K': 4220, 'intention': 4221, 'enemy': 4222, 'drink': 4223, '/>They': 4224, 'involved.': 4225, \"they'll\": 4226, 'shower': 4227, 'cardboard': 4228, 'gross': 4229, 'fired': 4230, 'daughter.': 4231, 'states': 4232, 'theater.': 4233, 'tune': 4234, '\"My': 4235, 'reasons.': 4236, '/>Also': 4237, 'son.': 4238, 'Matthau': 4239, 'exceptional': 4240, 'always,': 4241, 'minimal': 4242, 'laughs.': 4243, 'surface': 4244, 'York,': 4245, 'name,': 4246, 'irony': 4247, 'suited': 4248, 'shall': 4249, 'futuristic': 4250, 'neighborhood': 4251, 'Whatever': 4252, 'it...': 4253, 'poor.': 4254, 'overall,': 4255, 'category': 4256, 'kidnapped': 4257, '*': 4258, 'President': 4259, 'protagonists': 4260, 'Mystery': 4261, 'art.': 4262, 'disbelief': 4263, 'catches': 4264, 'returned': 4265, 'comedian': 4266, 'ludicrous': 4267, 'Plus,': 4268, 'Ghost': 4269, 'care.': 4270, 'Branagh': 4271, 'builds': 4272, 'superbly': 4273, 'breathtaking': 4274, 'Think': 4275, \"he'll\": 4276, 'dead,': 4277, 'deliberately': 4278, 'cases': 4279, 'timing': 4280, 'trilogy': 4281, 'trash.': 4282, 'Festival': 4283, 'causing': 4284, 'financial': 4285, 'star,': 4286, 'holiday': 4287, 'primary': 4288, 'technique': 4289, 'Nightmare': 4290, 'experience,': 4291, 'root': 4292, 'intrigued': 4293, 'Way': 4294, 'notably': 4295, 'tour': 4296, 'Soviet': 4297, 'credibility': 4298, 'chances': 4299, 'Actually,': 4300, 'killed,': 4301, 'Man,': 4302, 'Indeed,': 4303, 'Bobby': 4304, 'noted': 4305, 'murderous': 4306, 'assistant': 4307, 'complain': 4308, 'devoid': 4309, 'properly': 4310, 'actress.': 4311, 'Che': 4312, 'Streisand': 4313, 'Mexico': 4314, 'Men': 4315, 'connect': 4316, 'Jake': 4317, 'king': 4318, 'Roberts': 4319, 'pool': 4320, 'novels': 4321, 'currently': 4322, 'hey,': 4323, 'Laura': 4324, 'Derek': 4325, 'Season': 4326, 'tricks': 4327, 'Quite': 4328, 'LOVE': 4329, 'innocence': 4330, 'hat': 4331, 'simple,': 4332, 'escaped': 4333, 'jail': 4334, 'titles': 4335, 'vague': 4336, '/>Also,': 4337, 'storytelling': 4338, 'realistic.': 4339, 'fond': 4340, 'country.': 4341, 'anyway': 4342, 'affected': 4343, 'colour': 4344, 'Ken': 4345, 'demands': 4346, 'delight': 4347, 'splendid': 4348, 'ad': 4349, 'con': 4350, 'Holmes': 4351, 'Moon': 4352, 'atrocious': 4353, 'slightest': 4354, 'burned': 4355, '100%': 4356, '2nd': 4357, 'relative': 4358, 'ghosts': 4359, 'confusion': 4360, 'describes': 4361, 'menacing': 4362, \"mother's\": 4363, 'reasonable': 4364, 'happens.': 4365, 'look.': 4366, 'summary': 4367, \"Here's\": 4368, 'Hoffman': 4369, 'beaten': 4370, 'beginning.': 4371, 'characters.<br': 4372, 'Later': 4373, 'movie)': 4374, 'Plus': 4375, 'ranks': 4376, 'contract': 4377, 'room,': 4378, 'sequences,': 4379, '/>Just': 4380, 'digital': 4381, 'special.': 4382, 'resemblance': 4383, 'Power': 4384, 'dozens': 4385, 'pack': 4386, 'reaches': 4387, 'dialogs': 4388, 'scripts': 4389, 'sequel,': 4390, 'songs,': 4391, 'selling': 4392, \"Hollywood's\": 4393, 'ON': 4394, 'Cold': 4395, 'guard': 4396, 'jokes.': 4397, 'Could': 4398, 'scientific': 4399, 'particular,': 4400, 'Gothic': 4401, 'occurs': 4402, 'watching,': 4403, 'pays': 4404, 'non': 4405, 'lives,': 4406, 'Chuck': 4407, 'elaborate': 4408, 'along.': 4409, 'afternoon': 4410, 'guest': 4411, 'peace': 4412, 'Brown': 4413, 'lasted': 4414, 'strikes': 4415, 'disappointing.': 4416, 'period.': 4417, 'stranger': 4418, 'gag': 4419, 'rating.': 4420, 'surprise,': 4421, 'California': 4422, 'dynamic': 4423, 'subplot': 4424, 'staying': 4425, 'consistently': 4426, 'urge': 4427, 'title.': 4428, 'also,': 4429, \"We're\": 4430, 'mixture': 4431, 'Next': 4432, 'annoying,': 4433, 'burn': 4434, 'drunken': 4435, 'Greek': 4436, 'funny.<br': 4437, 'guts': 4438, 'dedicated': 4439, 'Unless': 4440, 'Kenneth': 4441, 'originality': 4442, 'everyone,': 4443, 'view.': 4444, 'broad': 4445, 'Lincoln': 4446, 'involve': 4447, 'succeed': 4448, 'psychotic': 4449, 'evil,': 4450, 'whereas': 4451, 'vicious': 4452, 'work.<br': 4453, 'benefit': 4454, 'discuss': 4455, 'Add': 4456, 'OK.': 4457, 'AT': 4458, 'misses': 4459, 'focusing': 4460, 'B.': 4461, 'animation,': 4462, 'Barry': 4463, 'briefly': 4464, 'world.<br': 4465, 'wave': 4466, 'afford': 4467, 'spoilers': 4468, 'comfortable': 4469, \"girl's\": 4470, 'spoiled': 4471, 'truth.': 4472, 'producing': 4473, 'beats': 4474, 'Otherwise,': 4475, '17': 4476, 'presentation': 4477, 'Inspector': 4478, 'Grand': 4479, 'hunting': 4480, 'alone,': 4481, 'acceptable': 4482, 'Lloyd': 4483, 'environment': 4484, 'forever.': 4485, 'titled': 4486, 'stupidity': 4487, 'two,': 4488, 'highest': 4489, 'whole,': 4490, 'remarkably': 4491, 'look,': 4492, 'primarily': 4493, 'solely': 4494, 'travels': 4495, 'aside,': 4496, 'clues': 4497, 'productions': 4498, 'inevitable': 4499, '/>An': 4500, 'format': 4501, 'Doctor': 4502, 'America,': 4503, 'workers': 4504, 'worry': 4505, \"90's\": 4506, 'introduces': 4507, 'portion': 4508, 'trial': 4509, 'sits': 4510, 'tortured': 4511, 'attraction': 4512, 'Clint': 4513, 'Jesse': 4514, 'material,': 4515, '/>Director': 4516, 'explaining': 4517, 'fancy': 4518, 'enormous': 4519, 'influenced': 4520, 'all-time': 4521, \"characters'\": 4522, 'kicked': 4523, 'recognized': 4524, 'winner': 4525, 'settle': 4526, 'smaller': 4527, 'repeatedly': 4528, 'lessons': 4529, 'Check': 4530, 'soundtrack,': 4531, 'universe': 4532, 'friend.': 4533, 'Lucas': 4534, 'fine.': 4535, 'legs': 4536, 'insulting': 4537, 'represent': 4538, '/>*': 4539, '/>No': 4540, 'creators': 4541, 'acted,': 4542, \"(I'm\": 4543, 'drops': 4544, 'act,': 4545, 'network': 4546, 'Considering': 4547, 'hates': 4548, 'Princess': 4549, 'miscast': 4550, 'letter': 4551, 'Secret': 4552, 'incoherent': 4553, 'golden': 4554, 'nearby': 4555, 'ambitious': 4556, 'improved': 4557, 'upper': 4558, 'Navy': 4559, 'Sally': 4560, '8/10': 4561, 'fu': 4562, 'vast': 4563, 'severe': 4564, 'blatant': 4565, 'Arnold': 4566, 'threatening': 4567, 'table': 4568, 'amazing,': 4569, 'Pat': 4570, 'Would': 4571, 'country,': 4572, 'season.': 4573, 'chased': 4574, 'involvement': 4575, 'developing': 4576, 'Andrews': 4577, 'ANY': 4578, 'failing': 4579, 'course.': 4580, 'endearing': 4581, 'NOTHING': 4582, 'arrive': 4583, 'flaw': 4584, 'Worst': 4585, 'rip-off': 4586, 'strike': 4587, 'horrendous': 4588, 'obscure': 4589, 'morality': 4590, 'storyline.': 4591, 'Rogers': 4592, 'press': 4593, 'mild': 4594, 'situation.': 4595, 'flight': 4596, 'situations.': 4597, 'blah': 4598, 'queen': 4599, 'subtitles': 4600, 'wont': 4601, 'Acting': 4602, 'justify': 4603, 'weight': 4604, 'tad': 4605, 'discussion': 4606, '60': 4607, 'techniques': 4608, 'slap': 4609, 'worried': 4610, 'Dave': 4611, 'documentary.': 4612, 'Foster': 4613, 'hooked': 4614, 'bond': 4615, 'Nevertheless,': 4616, 'possibility': 4617, 'explicit': 4618, \"You're\": 4619, 'Hanks': 4620, 'racism': 4621, '/>Yes,': 4622, 'von': 4623, 'B-movie': 4624, 'blew': 4625, 'argue': 4626, '(an': 4627, 'grab': 4628, 'poster': 4629, 'Poor': 4630, 'rushed': 4631, 'burning': 4632, 'had.': 4633, 'chases': 4634, 'jealous': 4635, 'battles': 4636, 'Elvis': 4637, 'involved,': 4638, \"60's\": 4639, 'marks': 4640, 'exposed': 4641, 'Boll': 4642, 'okay,': 4643, 'sex.': 4644, 'fine,': 4645, 'logical': 4646, 'pace,': 4647, 'loads': 4648, 'view,': 4649, 'Ruth': 4650, 'convoluted': 4651, 'performers': 4652, 'threat': 4653, 'Jeffrey': 4654, 'photographed': 4655, 'pet': 4656, 'park': 4657, 'oddly': 4658, 'conventional': 4659, 'screams': 4660, 'trio': 4661, 'Curtis': 4662, 'aging': 4663, 'evil.': 4664, 'villain,': 4665, 'Fay': 4666, 'killed.': 4667, 'Baby': 4668, 'heroic': 4669, 'suspense.': 4670, 'Wild': 4671, 'spooky': 4672, 'writer,': 4673, 'composed': 4674, 'videos': 4675, 'Uwe': 4676, 'professor': 4677, 'bridge': 4678, 'nightmare': 4679, 'worst.': 4680, 'weakest': 4681, '/>Why': 4682, 'absence': 4683, 'brand': 4684, 'Felix': 4685, 'nine': 4686, '(i.e.': 4687, 'Carl': 4688, 'School': 4689, 'boxing': 4690, 'maintain': 4691, 'terror': 4692, 'many,': 4693, 'CIA': 4694, 'were,': 4695, '2)': 4696, 'waited': 4697, 'drawing': 4698, 'shy': 4699, 'Heston': 4700, 'era.': 4701, 'wicked': 4702, 'revelation': 4703, '18': 4704, 'quality,': 4705, '45': 4706, '\"This': 4707, 'defeat': 4708, 'defend': 4709, 'fears': 4710, 'ourselves': 4711, 'Brando': 4712, 'Real': 4713, 'Karloff': 4714, 'sadistic': 4715, 'newspaper': 4716, 'notion': 4717, 'useless': 4718, 'Comedy': 4719, 'experiment': 4720, 'angle': 4721, 'hunt': 4722, 'hints': 4723, 'parents,': 4724, 'charisma': 4725, 'estate': 4726, 'Lion': 4727, 'Family': 4728, 'suits': 4729, 'react': 4730, 'kid.': 4731, 'USA': 4732, 'Neither': 4733, 'Home': 4734, 'contained': 4735, 'passes': 4736, 'still,': 4737, 'writing.': 4738, 'suspects': 4739, 'emphasis': 4740, 'wore': 4741, 'released.': 4742, '3rd': 4743, 'orders': 4744, 'art,': 4745, 'hearts': 4746, 'month': 4747, 'screen.<br': 4748, 'interaction': 4749, 'times.<br': 4750, 'aliens': 4751, 'arrested': 4752, '3.': 4753, 'harder': 4754, 'International': 4755, 'viewers.': 4756, 'rap': 4757, 'Norman': 4758, 'documentary,': 4759, 'stole': 4760, 'different,': 4761, 'gift': 4762, 'montage': 4763, 'Grace': 4764, 'spoiler': 4765, 'guys.': 4766, 'card': 4767, 'closest': 4768, 'gem.': 4769, 'Return': 4770, 'painted': 4771, 'happiness': 4772, 'nose': 4773, 'sweet,': 4774, 'spin': 4775, \"we'll\": 4776, 'shortly': 4777, 'Spielberg': 4778, 'amounts': 4779, 'indeed,': 4780, 'outrageous': 4781, 'health': 4782, 'delivering': 4783, 'wished': 4784, 'glass': 4785, 'treats': 4786, 'horrors': 4787, 'winds': 4788, 'Hugh': 4789, 'Flynn': 4790, 'chair': 4791, 'exaggerated': 4792, 'anymore': 4793, 'shadow': 4794, 'soundtrack.': 4795, 'interest,': 4796, 'moving,': 4797, 'signs': 4798, 'suitable': 4799, 'this!': 4800, 'noble': 4801, 'intent': 4802, 'offering': 4803, 'movie...': 4804, 'song,': 4805, 'dinner': 4806, 'valuable': 4807, 'Sullivan': 4808, 'Girl': 4809, 'Sutherland': 4810, 'era,': 4811, 'Mother': 4812, 'ignored': 4813, 'thrillers': 4814, 'conspiracy': 4815, 'Carpenter': 4816, 'psychiatrist': 4817, 'Glenn': 4818, 'tales': 4819, 'EVER': 4820, 'note,': 4821, 'response': 4822, \"someone's\": 4823, 'problems.': 4824, 'nervous': 4825, 'darkness': 4826, 'tear': 4827, 'it;': 4828, '/>Most': 4829, 'engage': 4830, 'Alexander': 4831, 'hardcore': 4832, 'raped': 4833, 'fortune': 4834, 'mentioned,': 4835, 'Hope': 4836, 'spots': 4837, 'taught': 4838, '\"It\\'s': 4839, 'Margaret': 4840, 'Warren': 4841, 'Rather': 4842, 'switch': 4843, 'girlfriend,': 4844, 'rank': 4845, 'painting': 4846, 'watch.<br': 4847, 'thumbs': 4848, 'affect': 4849, '(from': 4850, 'Swedish': 4851, 'Hall': 4852, 'Orson': 4853, 'Carrey': 4854, 'ideal': 4855, '..': 4856, 'corporate': 4857, 'mirror': 4858, 'Boy': 4859, 'Nelson': 4860, 'resolution': 4861, 'shorts': 4862, 'relation': 4863, 'disagree': 4864, 'episodes.': 4865, 'Through': 4866, 'demon': 4867, 'link': 4868, 'behave': 4869, 'Mickey': 4870, '24': 4871, 'Apart': 4872, 'headed': 4873, 'situation,': 4874, 'can.': 4875, 'I,': 4876, 'points.': 4877, 'staring': 4878, 'Island': 4879, 'events,': 4880, 'compelled': 4881, 'word,': 4882, 'undoubtedly': 4883, 'you.<br': 4884, 'ticket': 4885, 'also.': 4886, 'from.': 4887, 'daring': 4888, 'business.': 4889, 'reads': 4890, 'silver': 4891, 'considerable': 4892, 'Thanks': 4893, 'depicts': 4894, 'set.': 4895, 'unbelievable.': 4896, 'cheap,': 4897, 'gentle': 4898, 'reflect': 4899, 'honor': 4900, 'writer/director': 4901, 'investigate': 4902, '/>Overall': 4903, 'worthless': 4904, 'engaged': 4905, 'investigation': 4906, 'room.': 4907, 'handed': 4908, 'strip': 4909, 'Widmark': 4910, 'Cole': 4911, \"everyone's\": 4912, 'Titanic': 4913, 'court': 4914, 'joined': 4915, 'bare': 4916, 'masterpiece,': 4917, 'alongside': 4918, 'overlooked': 4919, 'grave': 4920, 'displayed': 4921, 'goal': 4922, 'documentaries': 4923, 'Matthew': 4924, 'Throughout': 4925, \"Disney's\": 4926, 'Perry': 4927, 'melodramatic': 4928, 'next.': 4929, 'Up': 4930, 'individuals': 4931, 'Scottish': 4932, 'chance,': 4933, '80': 4934, 'Law': 4935, 'horses': 4936, 'Maggie': 4937, 'Hard': 4938, 'trite': 4939, '/>Like': 4940, 'crap,': 4941, 'overwhelming': 4942, 'earned': 4943, 'lame.': 4944, 'invisible': 4945, '/>Now,': 4946, 'walls': 4947, 'suck': 4948, 'thick': 4949, 'blows': 4950, 'cousin': 4951, 'destroying': 4952, 'Planet': 4953, 'yet.': 4954, 'exotic': 4955, 'size': 4956, 'magazine': 4957, 'dull.': 4958, '1/2': 4959, 'facing': 4960, 'isolated': 4961, 'Lake': 4962, 'Italy': 4963, 'web': 4964, 'everything,': 4965, 'tends': 4966, 'WWII': 4967, 'Cat': 4968, 'Seeing': 4969, 'one-liners': 4970, \"don't.\": 4971, 'brilliance': 4972, 'agrees': 4973, 'glorious': 4974, 'inspiring': 4975, 'cheating': 4976, 'development.': 4977, 'ABC': 4978, \"family's\": 4979, 'lighting,': 4980, 'less.': 4981, 'vivid': 4982, 'show.<br': 4983, 'thru': 4984, 'it:': 4985, 'China': 4986, 'secretly': 4987, 'nothing,': 4988, 'cabin': 4989, 'timeless': 4990, 'Lots': 4991, 'weapon': 4992, 'proceeds': 4993, 'Civil': 4994, '\"a': 4995, 'advise': 4996, 'Aside': 4997, 'bears': 4998, 'highlights': 4999, 'UNK': 5000}\n",
            "{'the': 0, 'a': 1, 'and': 2, 'of': 3, 'to': 4, 'is': 5, 'in': 6, 'I': 7, 'that': 8, 'this': 9, 'it': 10, '/><br': 11, 'was': 12, 'as': 13, 'with': 14, 'for': 15, 'but': 16, 'The': 17, 'on': 18, 'movie': 19, 'are': 20, 'his': 21, 'film': 22, 'have': 23, 'not': 24, 'be': 25, 'you': 26, 'he': 27, 'by': 28, 'at': 29, 'one': 30, 'an': 31, 'from': 32, 'who': 33, 'like': 34, 'all': 35, 'they': 36, 'has': 37, 'so': 38, 'just': 39, 'about': 40, 'or': 41, 'her': 42, 'out': 43, 'some': 44, 'very': 45, 'more': 46, 'This': 47, 'would': 48, 'what': 49, 'when': 50, 'good': 51, 'only': 52, 'their': 53, 'It': 54, 'if': 55, 'had': 56, 'really': 57, \"it's\": 58, 'even': 59, 'which': 60, 'up': 61, 'can': 62, 'were': 63, 'my': 64, 'see': 65, 'no': 66, 'she': 67, 'than': 68, '-': 69, 'been': 70, 'there': 71, 'into': 72, 'get': 73, 'will': 74, 'story': 75, 'much': 76, 'because': 77, 'other': 78, 'most': 79, 'we': 80, 'time': 81, 'me': 82, 'make': 83, 'could': 84, 'also': 85, 'do': 86, 'how': 87, 'people': 88, 'first': 89, 'its': 90, 'any': 91, '/>The': 92, 'great': 93, \"don't\": 94, 'made': 95, 'think': 96, 'bad': 97, 'him': 98, 'being': 99, 'many': 100, 'never': 101, 'then': 102, 'But': 103, 'two': 104, '<br': 105, 'too': 106, 'little': 107, 'after': 108, 'where': 109, 'way': 110, 'And': 111, 'it.': 112, 'them': 113, 'well': 114, 'your': 115, 'watch': 116, 'does': 117, 'seen': 118, 'movie.': 119, 'know': 120, 'character': 121, \"It's\": 122, 'did': 123, 'characters': 124, 'movies': 125, 'best': 126, 'love': 127, 'ever': 128, 'over': 129, 'still': 130, 'A': 131, 'In': 132, 'should': 133, 'films': 134, 'plot': 135, 'such': 136, 'acting': 137, 'these': 138, 'i': 139, 'off': 140, 'show': 141, 'film.': 142, 'He': 143, 'better': 144, 'say': 145, \"doesn't\": 146, 'through': 147, 'go': 148, 'those': 149, 'If': 150, 'something': 151, 'makes': 152, \"didn't\": 153, 'There': 154, 'scene': 155, 'film,': 156, 'find': 157, \"I'm\": 158, 'back': 159, 'movie,': 160, 'watching': 161, 'few': 162, 'real': 163, 'scenes': 164, 'actually': 165, 'going': 166, 'same': 167, 'life': 168, '/>I': 169, 'lot': 170, 'quite': 171, 'look': 172, 'while': 173, 'want': 174, 'end': 175, '&': 176, 'thing': 177, 'seems': 178, 'every': 179, 'got': 180, 'old': 181, 'why': 182, 'pretty': 183, \"can't\": 184, 'nothing': 185, 'man': 186, 'another': 187, 'actors': 188, 'years': 189, 'between': 190, 'take': 191, 'before': 192, 'give': 193, 'may': 194, 'gets': 195, 'part': 196, 'young': 197, 'thought': 198, \"I've\": 199, 'around': 200, 'it,': 201, 'things': 202, \"isn't\": 203, 'saw': 204, 'without': 205, 'always': 206, 'own': 207, 'work': 208, 'almost': 209, 'must': 210, 'whole': 211, 'cast': 212, 'down': 213, 'might': 214, 'both': 215, 'new': 216, 'though': 217, 'us': 218, 'come': 219, 'least': 220, 'They': 221, 'bit': 222, 'here': 223, 'enough': 224, 'director': 225, 'horror': 226, 'feel': 227, 'big': 228, 'original': 229, 'am': 230, 'probably': 231, 'As': 232, 'fact': 233, 'rather': 234, 'kind': 235, 'far': 236, 'long': 237, 'found': 238, 'last': 239, 'funny': 240, \"that's\": 241, 'anything': 242, 'comes': 243, 'since': 244, 'making': 245, 'trying': 246, '\"The': 247, 'now': 248, 'each': 249, 'action': 250, 'interesting': 251, 'What': 252, 'done': 253, 'You': 254, 'worst': 255, 'She': 256, 'right': 257, 'our': 258, 'looks': 259, 'believe': 260, 'point': 261, 'put': 262, 'goes': 263, 'played': 264, \"he's\": 265, '/>This': 266, 'main': 267, \"wasn't\": 268, 'family': 269, 'role': 270, 'having': 271, 'guy': 272, 'series': 273, 'seem': 274, 'plays': 275, 'script': 276, 'performance': 277, 'hard': 278, 'takes': 279, 'world': 280, 'watched': 281, 'When': 282, 'worth': 283, 'minutes': 284, 'looking': 285, 'music': 286, 'different': 287, 'TV': 288, 'anyone': 289, 'especially': 290, 'shows': 291, 'sure': 292, 'away': 293, 'set': 294, 'woman': 295, 'times': 296, 'someone': 297, 'during': 298, 'time.': 299, 'yet': 300, 'left': 301, 'comedy': 302, 'three': 303, 'John': 304, 'girl': 305, \"there's\": 306, 'simply': 307, 'American': 308, 'fun': 309, 'seeing': 310, 'completely': 311, 'reason': 312, \"you're\": 313, 'play': 314, 'used': 315, 'special': 316, 'Not': 317, 'read': 318, 'need': 319, 'again': 320, 'One': 321, 'true': 322, 'For': 323, 'use': 324, 'sense': 325, 'version': 326, '--': 327, 'everything': 328, 'idea': 329, 'until': 330, 'place': 331, 'given': 332, 'help': 333, 'So': 334, 'nice': 335, 'rest': 336, 'beautiful': 337, 'truly': 338, 'less': 339, 'job': 340, 'money': 341, 'recommend': 342, 'came': 343, 'DVD': 344, 'try': 345, 'We': 346, 'ending': 347, 'once': 348, 'gives': 349, 'tell': 350, 'That': 351, 'said': 352, 'getting': 353, 'shot': 354, 'everyone': 355, 'high': 356, 'second': 357, 'keep': 358, 'My': 359, 'himself': 360, 'excellent': 361, 'couple': 362, 'All': 363, 'actor': 364, '(and': 365, 'enjoy': 366, 'supposed': 367, 'become': 368, 'half': 369, 'playing': 370, 'audience': 371, 'understand': 372, 'all,': 373, 'felt': 374, 'effects': 375, 'poor': 376, 'However,': 377, 'entire': 378, \"couldn't\": 379, 'liked': 380, 'small': 381, 'fan': 382, 'wife': 383, 'early': 384, 'start': 385, 'went': 386, 'doing': 387, 'together': 388, 'against': 389, 'full': 390, 'book': 391, 'remember': 392, 'Even': 393, 'THE': 394, '2': 395, 'let': 396, 'screen': 397, 'Hollywood': 398, 'definitely': 399, 'absolutely': 400, 'time,': 401, 'becomes': 402, 'often': 403, 'is,': 404, 'sort': 405, '10': 406, 'waste': 407, 'later': 408, 'day': 409, 'seemed': 410, 'along': 411, 'At': 412, 'year': 413, '\\x96': 414, 'After': 415, 'His': 416, 'certainly': 417, 'piece': 418, 'next': 419, 'short': 420, '.': 421, 'maybe': 422, 'several': 423, 'human': 424, 'black': 425, 'wanted': 426, 'instead': 427, 'although': 428, 'else': 429, 'kids': 430, 'wonderful': 431, 'camera': 432, 'production': 433, 'course': 434, 'that,': 435, 'performances': 436, 'classic': 437, 'loved': 438, 'them.': 439, 'tries': 440, 'To': 441, 'totally': 442, 'father': 443, 'women': 444, 'men': 445, 'wants': 446, 'able': 447, \"I'd\": 448, 'live': 449, 'called': 450, 'mind': 451, 'home': 452, 'gave': 453, 'hope': 454, 'top': 455, 'line': 456, 'already': 457, 'enjoyed': 458, 'based': 459, 'person': 460, 'video': 461, 'friends': 462, 'turn': 463, 'perfect': 464, 'story,': 465, '(the': 466, 'one.': 467, 'starts': 468, 'New': 469, 'under': 470, \"won't\": 471, 'turns': 472, 'final': 473, 'this.': 474, 'sex': 475, 'While': 476, 'name': 477, 'care': 478, 'problem': 479, 'mean': 480, 'episode': 481, 'me.': 482, 'written': 483, 'him.': 484, 'low': 485, 'moments': 486, 'stupid': 487, 'finally': 488, 'lost': 489, 'lead': 490, 'either': 491, 'lines': 492, 'Michael': 493, 'favorite': 494, 'took': 495, 'behind': 496, 'cannot': 497, 'this,': 498, \"she's\": 499, 'face': 500, 'Of': 501, \"you'll\": 502, 'No': 503, 'head': 504, 'stars': 505, 'story.': 506, 'budget': 507, 'title': 508, 'sound': 509, 'well.': 510, 'good.': 511, 'death': 512, 'fine': 513, 'guess': 514, 'Some': 515, 'school': 516, \"they're\": 517, 'me,': 518, 'all.': 519, \"film's\": 520, 'dialogue': 521, 'night': 522, 'How': 523, 'extremely': 524, 'lack': 525, 'heard': 526, 'style': 527, 'beginning': 528, 'terrible': 529, 'life.': 530, 'itself': 531, 'star': 532, 'feeling': 533, 'perhaps': 534, 'others': 535, 'house': 536, 'works': 537, 'expect': 538, 'boring': 539, \"wouldn't\": 540, 'kill': 541, 'attempt': 542, 'looked': 543, 'decent': 544, 'Mr.': 545, 'course,': 546, 'lives': 547, 'good,': 548, \"There's\": 549, 'quality': 550, 'Then': 551, '/>In': 552, 'late': 553, 'friend': 554, 'With': 555, 'Just': 556, 'well,': 557, 'entertaining': 558, 'particularly': 559, '/>It': 560, 'fight': 561, 'fans': 562, 'wrong': 563, 'thinking': 564, 'complete': 565, 'viewer': 566, 'case': 567, 'highly': 568, 'finds': 569, 'taken': 570, 'directed': 571, 'whose': 572, 'throughout': 573, 'leave': 574, 'writing': 575, 'obviously': 576, 'exactly': 577, '/>If': 578, 'somewhat': 579, 'amazing': 580, 'movies.': 581, 'guys': 582, 'awful': 583, 'evil': 584, 'laugh': 585, 'says': 586, 'shown': 587, 'obvious': 588, 'Why': 589, 'run': 590, '3': 591, 'movies,': 592, 'close': 593, 'told': 594, 'type': 595, 'James': 596, 'boy': 597, 'past': 598, 'parts': 599, 'number': 600, 'wonder': 601, 'films,': 602, 'group': 603, 'however,': 604, 'across': 605, 'opening': 606, 'picture': 607, 'car': 608, 'acting,': 609, 'mother': 610, 'turned': 611, 'save': 612, 'war': 613, 'soon': 614, 'sometimes': 615, 'white': 616, 'coming': 617, 'dead': 618, 'worse': 619, 'known': 620, 'tells': 621, 'despite': 622, 'started': 623, 'direction': 624, 'strong': 625, 'that.': 626, 'local': 627, 'hour': 628, 'killer': 629, 'side': 630, 'stop': 631, 'wish': 632, 'taking': 633, 'single': 634, 'myself': 635, 'knew': 636, 'children': 637, 'huge': 638, 'act': 639, 'except': 640, ',': 641, 'here.': 642, 'it.<br': 643, 'running': 644, 'killed': 645, 'happens': 646, 'female': 647, 'British': 648, 'way,': 649, 'usually': 650, 'including': 651, 'bad.': 652, \"aren't\": 653, 'stories': 654, 'bring': 655, 'fact,': 656, 'girls': 657, 'supporting': 658, 'major': 659, 'due': 660, 'characters,': 661, 'David': 662, '/>There': 663, 'involved': 664, 'dark': 665, 'humor': 666, 'talking': 667, 'Robert': 668, 'town': 669, 'voice': 670, 'musical': 671, 'out.': 672, \"Don't\": 673, 'hit': 674, 'call': 675, 'game': 676, 'end,': 677, 'brilliant': 678, '!': 679, 'falls': 680, 'giving': 681, 'and,': 682, 'actress': 683, 'relationship': 684, 'ends': 685, 'matter': 686, 'son': 687, \"I'll\": 688, 'mostly': 689, 'saying': 690, 'clearly': 691, 'living': 692, 'again.': 693, 'Well,': 694, 'one,': 695, 'easily': 696, 'serious': 697, 'feels': 698, 'police': 699, 'order': 700, 'bad,': 701, 'drama': 702, 'themselves': 703, 'appears': 704, 'example': 705, 'actual': 706, 'needs': 707, 'moment': 708, 'change': 709, 'similar': 710, 'important': 711, 'chance': 712, 'end.': 713, 'nearly': 714, \"haven't\": 715, 'modern': 716, 'is.': 717, 'child': 718, '/>But': 719, 'history': 720, 'cinema': 721, 'comic': 722, 'Although': 723, 'horrible': 724, 'On': 725, 'seen.': 726, 'way.': 727, '/>': 728, 'mention': 729, 'kept': 730, 'art': 731, 'named': 732, 'heart': 733, 'movie.<br': 734, 'released': 735, 'her.': 736, \"That's\": 737, 'interest': 738, 'English': 739, 'happened': 740, 'knows': 741, 'beyond': 742, 'cut': 743, 'plot,': 744, 'here,': 745, 'within': 746, 'usual': 747, 'films.': 748, 'slow': 749, 'kid': 750, 'using': 751, 'again,': 752, 'brought': 753, 'stuff': 754, 'hours': 755, 'simple': 756, 'typical': 757, 'bunch': 758, 'eyes': 759, 'romantic': 760, 'certain': 761, 'tried': 762, 'whether': 763, 'him,': 764, 'showing': 765, 'near': 766, 'film.<br': 767, '/>A': 768, 'fall': 769, 'George': 770, 'upon': 771, 'life,': 772, 'events': 773, 'strange': 774, 'decided': 775, 'score': 776, 'shots': 777, '/>As': 778, 'working': 779, 'among': 780, 'Her': 781, 'four': 782, 'body': 783, 'surprised': 784, 'hear': 785, 'begins': 786, 'Jack': 787, 'yourself': 788, '(I': 789, 'on.': 790, 'sad': 791, 'five': 792, '(as': 793, 'cheap': 794, 'greatest': 795, '/>And': 796, 'Richard': 797, 'became': 798, 'middle': 799, 'Paul': 800, 'Maybe': 801, 'learn': 802, 'happen': 803, 'none': 804, 'song': 805, 'famous': 806, 'annoying': 807, 'stay': 808, 'daughter': 809, 'basically': 810, 'days': 811, 'too.': 812, 'happy': 813, 'sit': 814, 'jokes': 815, 'experience': 816, 'sexual': 817, 'Peter': 818, 'sets': 819, 'murder': 820, 'songs': 821, \"/>It's\": 822, 'sequence': 823, 'attention': 824, '(which': 825, '1': 826, 'funny.': 827, 'lots': 828, 'realize': 829, 'talk': 830, 'buy': 831, 'hate': 832, 'difficult': 833, 'above': 834, 'view': 835, 'violence': 836, 'documentary': 837, 'stand': 838, 'easy': 839, 'From': 840, 'clear': 841, 'funny,': 842, 'light': 843, 'leaves': 844, 'means': 845, 'husband': 846, 'elements': 847, 'alone': 848, 'characters.': 849, 'Now': 850, 'keeps': 851, 'roles': 852, 'brother': 853, 'episodes': 854, 'meets': 855, 'cinematography': 856, 'genre': 857, '5': 858, 'character,': 859, 'word': 860, 'Its': 861, 'flick': 862, 'figure': 863, 'Japanese': 864, 'ones': 865, 'These': 866, 'them,': 867, 'out,': 868, 'possibly': 869, 'poorly': 870, 'hand': 871, 'First': 872, 'doubt': 873, 'silly': 874, 'brings': 875, 'previous': 876, 'nor': 877, 'say,': 878, 'emotional': 879, 'age': 880, 'NOT': 881, 'French': 882, 'move': 883, \"you've\": 884, 'novel': 885, 'killing': 886, 'apparently': 887, 'ten': 888, 'Like': 889, 'Is': 890, 'possible': 891, 'theme': 892, 'on,': 893, \"who's\": 894, 'problems': 895, 'leads': 896, 'reality': 897, 'moving': 898, 'level': 899, 'message': 900, 'reading': 901, 'straight': 902, 'comments': 903, 'talent': 904, 'television': 905, 'towards': 906, 'Most': 907, 'add': 908, 'career': 909, 'better.': 910, '(or': 911, 'deal': 912, 'meant': 913, 'enjoyable': 914, 'overall': 915, 'Tom': 916, 'review': 917, 'write': 918, 'ridiculous': 919, 'Oscar': 920, 'filmed': 921, 'An': 922, 'begin': 923, 'various': 924, 'herself': 925, 'needed': 926, 'gore': 927, 'somehow': 928, 'cool': 929, 'blood': 930, 'work.': 931, 'create': 932, 'though,': 933, 'incredibly': 934, 'leading': 935, 'feature': 936, 'forced': 937, 'watch.': 938, 'manages': 939, 'room': 940, 'personal': 941, 'particular': 942, '4': 943, 'whom': 944, 'eventually': 945, 'appear': 946, 'hell': 947, 'animation': 948, 'future': 949, 'hero': 950, 'Also': 951, 'show.': 952, 'form': 953, \"He's\": 954, 'up.': 955, 'fairly': 956, 'interested': 957, 'male': 958, 'writer': 959, 'effort': 960, 'points': 961, 'third': 962, 'there.': 963, 'scary': 964, 'hilarious': 965, 'meet': 966, 'rent': 967, 'plenty': 968, 'you.': 969, 'older': 970, 'expecting': 971, 'subject': 972, 'her,': 973, 'weak': 974, 'dramatic': 975, 'hardly': 976, 'reviews': 977, 'power': 978, 'viewers': 979, 'team': 980, 'premise': 981, 'scenes,': 982, 'attempts': 983, 'follow': 984, 'political': 985, 'tale': 986, 'features': 987, 'Joe': 988, 'average': 989, 'imagine': 990, 'scene,': 991, 'Also,': 992, 'York': 993, 'decides': 994, 'total': 995, 'worked': 996, 'Disney': 997, 'Lee': 998, 'up,': 999, 'pay': 1000, 'country': 1001, 'crap': 1002, 'gone': 1003, 'fantastic': 1004, 'forget': 1005, 'words': 1006, 'plain': 1007, 'King': 1008, '20': 1009, 'dialog': 1010, 'expected': 1011, '(a': 1012, 'minute': 1013, 'however': 1014, 'class': 1015, 'Who': 1016, 'uses': 1017, 'storyline': 1018, 'front': 1019, 'By': 1020, 'fails': 1021, 'viewing': 1022, 'spent': 1023, 'forward': 1024, 'badly': 1025, 'theater': 1026, '(who': 1027, 'sees': 1028, 'comment': 1029, 'slightly': 1030, 'admit': 1031, 'waiting': 1032, 'deserves': 1033, 'unique': 1034, 'ask': 1035, 'writers': 1036, 'sequences': 1037, 'sounds': 1038, 'open': 1039, 'stage': 1040, 'hold': 1041, 'dance': 1042, 'nature': 1043, 'fast': 1044, 'portrayed': 1045, 'social': 1046, 'soundtrack': 1047, 'crime': 1048, 'footage': 1049, 'times,': 1050, 'atmosphere': 1051, 'realistic': 1052, 'William': 1053, \"what's\": 1054, '...': 1055, 'dull': 1056, 'check': 1057, 'directors': 1058, 'caught': 1059, 'agree': 1060, 'years.': 1061, 'biggest': 1062, 'recent': 1063, 'there,': 1064, 'telling': 1065, 'talented': 1066, 'etc.': 1067, 'quickly': 1068, 'perfectly': 1069, 'wait': 1070, 'visual': 1071, 'spend': 1072, 'amount': 1073, 'wrote': 1074, 'predictable': 1075, 'Dr.': 1076, 'memorable': 1077, 'parents': 1078, 'battle': 1079, 'entirely': 1080, 'sequel': 1081, 'large': 1082, 'Or': 1083, 'outside': 1084, 'ways': 1085, 'powerful': 1086, 'result': 1087, 'plot.': 1088, 'editing': 1089, 'made.': 1090, 'deep': 1091, 'people.': 1092, 'rating': 1093, 'weird': 1094, 'fighting': 1095, 'disappointed': 1096, 'period': 1097, 'question': 1098, 'said,': 1099, 'pure': 1100, 'people,': 1101, 'actors,': 1102, '/>What': 1103, 'work,': 1104, 'earlier': 1105, 'appreciate': 1106, 'inside': 1107, 'showed': 1108, 'kills': 1109, 'show,': 1110, 'lame': 1111, 'material': 1112, 'compared': 1113, \"weren't\": 1114, 'it!': 1115, 'former': 1116, 'Yes,': 1117, 'filmmakers': 1118, 'release': 1119, 'filled': 1120, 'copy': 1121, 'casting': 1122, 'Ben': 1123, 'moves': 1124, 'city': 1125, 'popular': 1126, 'creepy': 1127, 'Man': 1128, 'unless': 1129, 'decide': 1130, 'positive': 1131, 'sister': 1132, 'comedy,': 1133, 'cheesy': 1134, 'scenes.': 1135, 'superb': 1136, 'runs': 1137, 'portrayal': 1138, 'missing': 1139, 'present': 1140, 'screenplay': 1141, 'character.': 1142, 'consider': 1143, 'considered': 1144, 'free': 1145, 'seriously': 1146, 'rich': 1147, 'more.': 1148, 'be.': 1149, 'too,': 1150, 'ideas': 1151, 'credits': 1152, 'recently': 1153, 'entertainment': 1154, 'thriller': 1155, 'follows': 1156, 'fit': 1157, 'general': 1158, 'married': 1159, 'wasted': 1160, 'Great': 1161, 'ago': 1162, 'common': 1163, 'barely': 1164, 'gay': 1165, 'miss': 1166, 'mystery': 1167, 'ended': 1168, 'believable': 1169, 'whatever': 1170, 'Italian': 1171, 'created': 1172, 'please': 1173, 'German': 1174, 'But,': 1175, 'in.': 1176, 'opinion': 1177, 'familiar': 1178, 'likes': 1179, 'situation': 1180, 'eye': 1181, 'Christmas': 1182, 'setting': 1183, 'further': 1184, 'War': 1185, 'bought': 1186, 'great,': 1187, 'involving': 1188, 'focus': 1189, 'break': 1190, 'suddenly': 1191, 'leaving': 1192, 'Jane': 1193, 'Bill': 1194, 'remake': 1195, 'missed': 1196, 'screen.': 1197, \"'The\": 1198, 'sorry': 1199, 'background': 1200, 'following': 1201, 'role.': 1202, 'box': 1203, 'members': 1204, 'Unfortunately,': 1205, 'utterly': 1206, 'hands': 1207, 'successful': 1208, 'acting.': 1209, 'managed': 1210, 'beauty': 1211, 'won': 1212, 'younger': 1213, 'rate': 1214, 'odd': 1215, 'surprise': 1216, 'Best': 1217, 'Well': 1218, '\"I': 1219, 'fantasy': 1220, '(in': 1221, 'basic': 1222, 'glad': 1223, 'starring': 1224, 'crew': 1225, 'Another': 1226, 'you,': 1227, 'avoid': 1228, 'animated': 1229, 'great.': 1230, 'man,': 1231, 'God': 1232, 'considering': 1233, 'Scott': 1234, 'OF': 1235, 'solid': 1236, \"movie's\": 1237, 'hot': 1238, 'crazy': 1239, 'dumb': 1240, 'zombie': 1241, 'potential': 1242, 'thinks': 1243, 'Good': 1244, 'was,': 1245, 'series,': 1246, 'ultimately': 1247, 'Perhaps': 1248, 'laughing': 1249, 'times.': 1250, 'sitting': 1251, 'effect': 1252, 'world.': 1253, '30': 1254, 'bored': 1255, 'chemistry': 1256, '/>My': 1257, 'acted': 1258, 'mentioned': 1259, 'IMDb': 1260, 'slasher': 1261, 'shame': 1262, 'season': 1263, 'US': 1264, 'public': 1265, 'bizarre': 1266, 'directing': 1267, 'suspense': 1268, 'book,': 1269, 'cover': 1270, 'truth': 1271, 'generally': 1272, 'singing': 1273, 'not.': 1274, 'speak': 1275, 'cast,': 1276, 'likely': 1277, 'convincing': 1278, 'explain': 1279, '/>All': 1280, 'space': 1281, 'changed': 1282, 'win': 1283, 'added': 1284, 'cop': 1285, 'intelligent': 1286, 'twist': 1287, 'walk': 1288, 'cinematic': 1289, 'fully': 1290, 'romance': 1291, 'adult': 1292, 'much.': 1293, 'thrown': 1294, 'Oh': 1295, 'monster': 1296, 'off,': 1297, 'business': 1298, 'failed': 1299, 'rare': 1300, 'catch': 1301, 'neither': 1302, 'impossible': 1303, 'return': 1304, '15': 1305, 'Steve': 1306, 'remains': 1307, 'effective': 1308, '/>One': 1309, 'match': 1310, 'sent': 1311, 'incredible': 1312, 'clever': 1313, 'hoping': 1314, 'Every': 1315, 'equally': 1316, 'scene.': 1317, 'development': 1318, 'World': 1319, 'Star': 1320, '10.': 1321, 'historical': 1322, \"we're\": 1323, 'produced': 1324, 'violent': 1325, 'constantly': 1326, 'dancing': 1327, 'lady': 1328, 'aspect': 1329, 'today': 1330, 'series.': 1331, 'credit': 1332, 'ability': 1333, 'longer': 1334, 'success': 1335, 'shooting': 1336, 'off.': 1337, 'boys': 1338, 'script,': 1339, 'years,': 1340, 'escape': 1341, 'Mary': 1342, 'Tony': 1343, 'concept': 1344, 'choice': 1345, 'producers': 1346, 'stick': 1347, 'computer': 1348, 'otherwise': 1349, 'indeed': 1350, 'know,': 1351, 'comedy.': 1352, 'Only': 1353, 'excuse': 1354, 'trouble': 1355, 'knowing': 1356, 'puts': 1357, 'pick': 1358, 'music,': 1359, 'cute': 1360, 'not,': 1361, 'immediately': 1362, 'exciting': 1363, 'do.': 1364, 'contains': 1365, 'was.': 1366, 'Big': 1367, 'mean,': 1368, 'tension': 1369, 'secret': 1370, 'reasons': 1371, 'Because': 1372, 'Very': 1373, 'dream': 1374, 'Sam': 1375, 'So,': 1376, 'cause': 1377, '/>So': 1378, 'suggest': 1379, '(': 1380, 'depth': 1381, 'then,': 1382, 'cult': 1383, 'together.': 1384, 'questions': 1385, 'literally': 1386, 'unlike': 1387, 'Jim': 1388, 'trip': 1389, 'military': 1390, 'flat': 1391, 'lacks': 1392, 'stands': 1393, 'cartoon': 1394, 'laughs': 1395, 'makers': 1396, 'air': 1397, 'enough,': 1398, 'list': 1399, 'surprisingly': 1400, 'society': 1401, 'villain': 1402, 'Do': 1403, 'presented': 1404, 'tough': 1405, 'OK': 1406, 'girlfriend': 1407, 'include': 1408, 'pace': 1409, 'terms': 1410, 'studio': 1411, 'died': 1412, \"hasn't\": 1413, 'respect': 1414, 'Though': 1415, 'college': 1416, 'joke': 1417, 'amusing': 1418, 'best.': 1419, 'minor': 1420, 'Frank': 1421, 'impression': 1422, 'adaptation': 1423, 'unfortunately': 1424, 'nudity': 1425, 'beautifully': 1426, 'images': 1427, 'fake': 1428, 'die': 1429, 'plus': 1430, 'either.': 1431, 'impressive': 1432, 'example,': 1433, '\"the': 1434, 'merely': 1435, 'De': 1436, 'note': 1437, 'thing.': 1438, 'Indian': 1439, 'band': 1440, 'walking': 1441, 'control': 1442, 'state': 1443, 'appeal': 1444, 'America': 1445, 'apart': 1446, 'Film': 1447, 'drawn': 1448, 'Watch': 1449, 'about.': 1450, 'appearance': 1451, 'innocent': 1452, 'proves': 1453, 'Director': 1454, \"you'd\": 1455, 'pointless': 1456, '\"': 1457, 'rock': 1458, 'Christopher': 1459, 'pass': 1460, '8': 1461, 'provides': 1462, 'mess': 1463, 'red': 1464, 'in,': 1465, 'falling': 1466, 'natural': 1467, 'shoot': 1468, 'brief': 1469, 'day.': 1470, 'Chris': 1471, 'fell': 1472, 'sick': 1473, 'loves': 1474, 'becoming': 1475, 'standard': 1476, 'sci-fi': 1477, 'dog': 1478, 'minutes.': 1479, 'silent': 1480, 'share': 1481, 'suppose': 1482, 'subtle': 1483, 'role,': 1484, 'touch': 1485, 'helps': 1486, 'aspects': 1487, 'Night': 1488, 'fellow': 1489, 'appeared': 1490, 'with.': 1491, 'mysterious': 1492, 'mainly': 1493, 'support': 1494, 'reminded': 1495, 'meaning': 1496, 'narrative': 1497, 'IS': 1498, 'Here': 1499, 'pull': 1500, 'Harry': 1501, 'acts': 1502, 'normal': 1503, 'offers': 1504, 'throw': 1505, 'actors.': 1506, 'adds': 1507, 'did.': 1508, 'thoroughly': 1509, 'touching': 1510, 'complex': 1511, 'store': 1512, 'followed': 1513, 'audiences': 1514, 'sweet': 1515, 'fun.': 1516, 'disturbing': 1517, 'drug': 1518, 'seemingly': 1519, 'somewhere': 1520, 'revenge': 1521, 'wondering': 1522, 'putting': 1523, 'central': 1524, 'tired': 1525, 'value': 1526, 'ever.': 1527, 'effects,': 1528, 'director,': 1529, 'held': 1530, 'so,': 1531, 'stuck': 1532, 'Charles': 1533, 'Black': 1534, 'mood': 1535, 'rented': 1536, 'terrific': 1537, 'fear': 1538, 'Van': 1539, 'Despite': 1540, 'serial': 1541, 'surely': 1542, 'South': 1543, 'paid': 1544, 'turning': 1545, 'books': 1546, 'world,': 1547, 'picked': 1548, 'company': 1549, 'onto': 1550, 'gun': 1551, 'delivers': 1552, 'to.': 1553, 'charming': 1554, 'day,': 1555, 'tone': 1556, 'themes': 1557, 'redeeming': 1558, 'action,': 1559, 'minutes,': 1560, 'painful': 1561, 'slowly': 1562, 'Nothing': 1563, 'fire': 1564, 'People': 1565, 'AND': 1566, 'self': 1567, 'language': 1568, 'And,': 1569, 'moral': 1570, 'heavy': 1571, 'science': 1572, 'lived': 1573, 'lovely': 1574, 'finding': 1575, 'office': 1576, 'Charlie': 1577, 'point,': 1578, 'Will': 1579, 'presence': 1580, 'Once': 1581, 'Bruce': 1582, 'led': 1583, 'absolute': 1584, 'yes,': 1585, 'hand,': 1586, 'negative': 1587, 'willing': 1588, 'opportunity': 1589, 'performance.': 1590, 'funniest': 1591, 'element': 1592, 'rated': 1593, 'allowed': 1594, 'man.': 1595, 'twists': 1596, 'doctor': 1597, 'water': 1598, 'Billy': 1599, 'interesting.': 1600, 'changes': 1601, 'cast.': 1602, 'Henry': 1603, 'script.': 1604, 'Both': 1605, 'now,': 1606, 'key': 1607, 'prison': 1608, 'baby': 1609, 'random': 1610, 'force': 1611, 'awful.': 1612, 'party': 1613, 'giant': 1614, 'love,': 1615, 'de': 1616, 'spirit': 1617, 'available': 1618, \"let's\": 1619, \"shouldn't\": 1620, 'values': 1621, 'though.': 1622, 'Two': 1623, 'other.': 1624, 'Christian': 1625, 'later,': 1626, 'away.': 1627, 'filming': 1628, 'carry': 1629, 'Many': 1630, 'Little': 1631, 'track': 1632, 'movie!': 1633, 'laughed': 1634, 'issues': 1635, 'helped': 1636, 'thing,': 1637, 'supposedly': 1638, 'nobody': 1639, 'actor,': 1640, '(not': 1641, 'seen,': 1642, 'includes': 1643, 'describe': 1644, 'time.<br': 1645, \"She's\": 1646, 'Where': 1647, 'bother': 1648, 'magic': 1649, 'Other': 1650, 'thanks': 1651, 'but,': 1652, '7': 1653, 'sexy': 1654, '/>For': 1655, 'soldiers': 1656, '(with': 1657, 'image': 1658, 'mix': 1659, 'asks': 1660, 'before.': 1661, 'fair': 1662, 'Their': 1663, 'drive': 1664, 'wrong.': 1665, 'ready': 1666, 'approach': 1667, 'pieces': 1668, 'names': 1669, 'critics': 1670, 'book.': 1671, \"character's\": 1672, 'boring.': 1673, 'impressed': 1674, 'feelings': 1675, 'Too': 1676, 'interesting,': 1677, 'Movie': 1678, 'humour': 1679, 'martial': 1680, 'audience.': 1681, 'intended': 1682, 'family.': 1683, 'damn': 1684, 'favourite': 1685, 'technical': 1686, 'provide': 1687, 'done.': 1688, 'else.': 1689, 'creative': 1690, 'Oh,': 1691, 'masterpiece': 1692, \"man's\": 1693, 'moved': 1694, 'pathetic': 1695, 'stunning': 1696, 'adventure': 1697, 'chase': 1698, 'gang': 1699, 'tragic': 1700, 'outstanding': 1701, 'Brian': 1702, 'lose': 1703, 'artistic': 1704, 'compelling': 1705, 'Jerry': 1706, 'era': 1707, 'gotten': 1708, 'physical': 1709, 'ending.': 1710, 'Alan': 1711, 'victim': 1712, 'wearing': 1713, 'teenage': 1714, 'Instead': 1715, 'comedic': 1716, 'back.': 1717, 'mental': 1718, 'performance,': 1719, 'allow': 1720, 'inspired': 1721, 'Red': 1722, 'opinion,': 1723, 'hair': 1724, 'attack': 1725, 'House': 1726, 'deeply': 1727, 'street': 1728, 'case,': 1729, 'awesome': 1730, 'wife,': 1731, 'purpose': 1732, 'boring,': 1733, 'developed': 1734, 'reminds': 1735, 'million': 1736, 'sat': 1737, 'fascinating': 1738, 'continue': 1739, 'struggle': 1740, 'journey': 1741, 'project': 1742, 'money.': 1743, 'accept': 1744, 'now.': 1745, 'before,': 1746, 'honestly': 1747, 'situations': 1748, 'train': 1749, 'step': 1750, 'build': 1751, 'naked': 1752, \"80's\": 1753, 'place.': 1754, 'holds': 1755, 'difference': 1756, 'comedies': 1757, 'better,': 1758, 'wanting': 1759, 'do,': 1760, 'Chinese': 1761, 'afraid': 1762, 'Kelly': 1763, 'wonderfully': 1764, 'first,': 1765, 'government': 1766, 'London': 1767, 'member': 1768, 'motion': 1769, 'deliver': 1770, '\"A': 1771, 'made,': 1772, 'search': 1773, 'Ms.': 1774, 'family,': 1775, 'realized': 1776, 'lets': 1777, 'Stewart': 1778, 'himself.': 1779, 'for.': 1780, 'be,': 1781, 'worthy': 1782, 'disappointed.': 1783, 'sense.': 1784, 'rarely': 1785, 'plan': 1786, 'details': 1787, 'compare': 1788, 'honest': 1789, 'offer': 1790, 'answer': 1791, 'culture': 1792, 'direct': 1793, 'notice': 1794, 'cold': 1795, 'western': 1796, 'detective': 1797, 'thus': 1798, 'Mr': 1799, 'places': 1800, 'trash': 1801, 'toward': 1802, 'nowhere': 1803, 'emotions': 1804, 'gorgeous': 1805, 'began': 1806, 'Stephen': 1807, 'thats': 1808, 'OK,': 1809, 'dialogue,': 1810, 'part,': 1811, 'taste': 1812, 'confused': 1813, 'job.': 1814, 'original,': 1815, '/>When': 1816, 'earth': 1817, 'mediocre': 1818, 'Smith': 1819, '9': 1820, 'attractive': 1821, 'edge': 1822, 'latter': 1823, 'marriage': 1824, 'apparent': 1825, 'brain': 1826, 'watch,': 1827, 'Having': 1828, 'flying': 1829, 'intense': 1830, 'zombies': 1831, 'student': 1832, 'introduced': 1833, 'fresh': 1834, 'pictures': 1835, 'Since': 1836, 'However': 1837, 'genius': 1838, 'six': 1839, 'constant': 1840, 'religious': 1841, 'creating': 1842, 'listen': 1843, 'climax': 1844, 'Everything': 1845, \"hadn't\": 1846, 'beat': 1847, 'asked': 1848, 'best,': 1849, 'Martin': 1850, 'spot': 1851, 'pulled': 1852, 'capture': 1853, 'manage': 1854, 'Unfortunately': 1855, 'mad': 1856, 'building': 1857, 'Miss': 1858, 'professional': 1859, 'costumes': 1860, 'bottom': 1861, \"today's\": 1862, 'nasty': 1863, 'B': 1864, 'independent': 1865, 'accent': 1866, 'exception': 1867, 'information': 1868, 'shock': 1869, 'it?': 1870, 'deserve': 1871, 'ghost': 1872, 'extra': 1873, 'Mark': 1874, 'likable': 1875, 'other,': 1876, 'hated': 1877, 'genre.': 1878, 'much,': 1879, 'photography': 1880, 'down.': 1881, 'treated': 1882, '(like': 1883, 'blame': 1884, 'ahead': 1885, 'Russian': 1886, 'woman,': 1887, 'disappointing': 1888, '(if': 1889, 'limited': 1890, 'quick': 1891, 'desperate': 1892, 'animals': 1893, 'students': 1894, 'Rock': 1895, 'master': 1896, 'ago,': 1897, 'numerous': 1898, 'charm': 1899, 'Now,': 1900, 'see.': 1901, 'hidden': 1902, 'personally': 1903, 'scenery': 1904, 'loud': 1905, '90': 1906, 'Those': 1907, 'porn': 1908, 'so.': 1909, 'Did': 1910, 'winning': 1911, 'Tim': 1912, 'superior': 1913, '/>To': 1914, 'Fred': 1915, 'murders': 1916, 'actresses': 1917, 'cinema.': 1918, 'right.': 1919, 'unusual': 1920, 'Yet': 1921, 'terribly': 1922, 'addition': 1923, 'trailer': 1924, 'growing': 1925, 'discover': 1926, 'City': 1927, 'DVD.': 1928, \"/>I'm\": 1929, 'extreme': 1930, 'Love': 1931, 'flick.': 1932, '6': 1933, 'dealing': 1934, 'hopes': 1935, 'sadly': 1936, 'Was': 1937, 'prove': 1938, 'love.': 1939, 'long,': 1940, 'with,': 1941, 'responsible': 1942, 'Ed': 1943, 'phone': 1944, 'smart': 1945, 'finish': 1946, 'loses': 1947, 'Jason': 1948, 'keeping': 1949, 'week': 1950, 'terrible.': 1951, 'Joan': 1952, 'emotion': 1953, 'portray': 1954, 'unbelievable': 1955, '?': 1956, 'mixed': 1957, 'door': 1958, 'returns': 1959, 'Don': 1960, 'rape': 1961, '/>While': 1962, 'Everyone': 1963, 'mistake': 1964, 'deals': 1965, 'pain': 1966, 'girl,': 1967, '100': 1968, 'Americans': 1969, 'impact': 1970, 'genuinely': 1971, 'ride': 1972, 'wild': 1973, 'affair': 1974, 'kinda': 1975, 'road': 1976, '40': 1977, 'relationships': 1978, 'Danny': 1979, 'saved': 1980, 'Batman': 1981, 'noir': 1982, 'fun,': 1983, 'point.': 1984, 'are,': 1985, 'understanding': 1986, 'low-budget': 1987, 'producer': 1988, 'featuring': 1989, 'starting': 1990, 'numbers': 1991, 'camp': 1992, 'original.': 1993, 'dad': 1994, 'together,': 1995, 'psychological': 1996, 'remarkable': 1997, 'Jeff': 1998, 'brothers': 1999, 'them.<br': 2000, 'color': 2001, 'proved': 2002, 'cat': 2003, 'expectations': 2004, 'allows': 2005, 'grew': 2006, 'childhood': 2007, 'oh': 2008, 'current': 2009, 'Arthur': 2010, 'lies': 2011, 'scared': 2012, 'breaks': 2013, 'mind.': 2014, 'traditional': 2015, 'European': 2016, 'finished': 2017, 'Eddie': 2018, 'father,': 2019, 'aware': 2020, 'wrong,': 2021, 'dreams': 2022, 'dying': 2023, 'today.': 2024, \"one's\": 2025, 'sheer': 2026, 'unable': 2027, 'really,': 2028, 'house,': 2029, 'brutal': 2030, 'continues': 2031, 'Anyway,': 2032, 'Still,': 2033, 'hotel': 2034, 'moments,': 2035, 'suspect': 2036, 'discovers': 2037, 'bed': 2038, 'content': 2039, 'gonna': 2040, 'location': 2041, 'scientist': 2042, 'Before': 2043, 'Dead': 2044, 'bar': 2045, 'calls': 2046, 'Bad': 2047, 'shocking': 2048, 'Allen': 2049, 'forgotten': 2050, 'adults': 2051, 'noticed': 2052, 'learned': 2053, 'surprising': 2054, 'wooden': 2055, 'hits': 2056, 'everybody': 2057, 'finest': 2058, 'desire': 2059, 'through.': 2060, 'fan,': 2061, 'Al': 2062, 'lighting': 2063, 'Kevin': 2064, 'opens': 2065, \"70's\": 2066, 'epic': 2067, 'Andy': 2068, 'vampire': 2069, 'soul': 2070, 'regular': 2071, 'happening': 2072, 'originally': 2073, 'short,': 2074, 'Which': 2075, 'boyfriend': 2076, 'Young': 2077, 'loving': 2078, 'performances,': 2079, 'Johnny': 2080, 'speaking': 2081, 'bloody': 2082, 'confusing': 2083, 'victims': 2084, 'CGI': 2085, 'majority': 2086, 'arts': 2087, 'cliché': 2088, 'fail': 2089, 'Gene': 2090, 'ultimate': 2091, 'angry': 2092, 'screen,': 2093, 'music.': 2094, 'pop': 2095, 'THIS': 2096, 'double': 2097, 'anybody': 2098, 'plane': 2099, 'teen': 2100, 'bigger': 2101, 'Adam': 2102, 'driving': 2103, 'bits': 2104, 'necessary': 2105, \"Let's\": 2106, 'visit': 2107, 'Ray': 2108, 'manner': 2109, 'Instead,': 2110, 'to,': 2111, 'caused': 2112, 'island': 2113, 'steal': 2114, 'criminal': 2115, 'event': 2116, 'date': 2117, 'news': 2118, 'creates': 2119, 'seconds': 2120, 'reach': 2121, 'guy,': 2122, 'apartment': 2123, 'ending,': 2124, 'conclusion': 2125, 'super': 2126, 'right,': 2127, '(for': 2128, 'Kate': 2129, 'overly': 2130, 'painfully': 2131, 'watching.': 2132, 'dangerous': 2133, 'garbage': 2134, 'Paris': 2135, 'pleasure': 2136, 'friends,': 2137, 'performances.': 2138, 'met': 2139, 'Williams': 2140, 'West': 2141, 'born': 2142, 'enough.': 2143, 'received': 2144, 'dies': 2145, '(played': 2146, \"they've\": 2147, 'entertaining.': 2148, 'Nick': 2149, 'Jean': 2150, 'genuine': 2151, 'somebody': 2152, 'twice': 2153, 'century': 2154, 'alive': 2155, 'industry': 2156, 'cops': 2157, 'cry': 2158, 'fiction': 2159, \"director's\": 2160, 'faces': 2161, 'captured': 2162, 'involves': 2163, 'relate': 2164, 'collection': 2165, 'discovered': 2166, 'practically': 2167, 'race': 2168, 'personality': 2169, 'jump': 2170, '(The': 2171, 'awful,': 2172, 'believes': 2173, 'Anthony': 2174, 'sign': 2175, '/': 2176, 'Sean': 2177, 'detail': 2178, 'opera': 2179, 'filmmaker': 2180, 'teacher': 2181, 'more,': 2182, 'boss': 2183, 'utter': 2184, 'design': 2185, 'drama,': 2186, 'learns': 2187, 'heads': 2188, 'itself,': 2189, 'done,': 2190, 'Douglas': 2191, 'intriguing': 2192, 'movie?': 2193, 'hospital': 2194, 'animal': 2195, 'White': 2196, 'Spanish': 2197, 'see,': 2198, 'area': 2199, 'Jimmy': 2200, 'issue': 2201, 'energy': 2202, 'delightful': 2203, 'night,': 2204, 'largely': 2205, 'himself,': 2206, 'who,': 2207, 'reviewers': 2208, 'essentially': 2209, 'develop': 2210, 'ago.': 2211, 'capable': 2212, 'described': 2213, 'whilst': 2214, 'actions': 2215, 'asking': 2216, 'effects.': 2217, 'loose': 2218, 'eat': 2219, 'action.': 2220, 'crappy': 2221, 'direction,': 2222, 'fill': 2223, 'forces': 2224, 'commentary': 2225, 'therefore': 2226, 'roles.': 2227, 'laughable': 2228, 'ran': 2229, 'Earth': 2230, 'whenever': 2231, '/>Overall,': 2232, 'tend': 2233, 'job,': 2234, 'system': 2235, 'months': 2236, 'classic.': 2237, 'holes': 2238, ':': 2239, 'unnecessary': 2240, 'Lady': 2241, 'captures': 2242, 'part.': 2243, 'unknown': 2244, 'media': 2245, 'recommended': 2246, 'ship': 2247, 'deserved': 2248, 'history.': 2249, 'clothes': 2250, 'around.': 2251, 'okay': 2252, 'Mike': 2253, 'nicely': 2254, 'Davis': 2255, 'soap': 2256, 'others.': 2257, 'sight': 2258, 'pacing': 2259, 'lacking': 2260, 'picture.': 2261, 'Dick': 2262, 'portrays': 2263, 'fights': 2264, 'pair': 2265, '(but': 2266, 'initial': 2267, 'bright': 2268, 'god': 2269, 'remembered': 2270, 'proper': 2271, 'connection': 2272, 'combination': 2273, '(even': 2274, 'ugly': 2275, 'knowledge': 2276, 'portraying': 2277, 'treat': 2278, 'like,': 2279, 'Gary': 2280, 'money,': 2281, 'food': 2282, 'alien': 2283, 'aside': 2284, 'produce': 2285, 'seven': 2286, 'place,': 2287, 'tears': 2288, 'Sure,': 2289, 'army': 2290, 'tragedy': 2291, 'assume': 2292, 'engaging': 2293, 'Go': 2294, 'death.': 2295, 'walks': 2296, 'over.': 2297, 'justice': 2298, 'normally': 2299, 'flicks': 2300, 'passed': 2301, 'higher': 2302, 'Ann': 2303, 'occasionally': 2304, 'choose': 2305, 'Bob': 2306, 'steals': 2307, 'itself.': 2308, 'grow': 2309, 'creature': 2310, 'Part': 2311, 'home,': 2312, 'thin': 2313, 'anything.': 2314, 'included': 2315, 'ancient': 2316, 'lover': 2317, 'VHS': 2318, 'awkward': 2319, 'Jackson': 2320, 'away,': 2321, 'No,': 2322, 'reason,': 2323, 'bringing': 2324, 'built': 2325, 'crap.': 2326, 'Can': 2327, 'strongly': 2328, 'sell': 2329, 'brilliantly': 2330, 'quiet': 2331, 'trust': 2332, 'Steven': 2333, 'radio': 2334, 'count': 2335, 'stopped': 2336, '(although': 2337, 'study': 2338, 'foreign': 2339, 'folks': 2340, 'long.': 2341, 'losing': 2342, 'record': 2343, 'flaws': 2344, 'stock': 2345, 'relatively': 2346, 'unexpected': 2347, 'intelligence': 2348, 'absurd': 2349, 'Academy': 2350, 'night.': 2351, 'him.<br': 2352, 'send': 2353, 'virtually': 2354, 'below': 2355, 'explanation': 2356, 'kick': 2357, 'twenty': 2358, 'memories': 2359, 'graphic': 2360, 'Eric': 2361, 'cameo': 2362, 'agent': 2363, 'remain': 2364, 'fits': 2365, 'memory': 2366, 'ruined': 2367, 'Anyone': 2368, 'humor,': 2369, 'grown': 2370, 'gratuitous': 2371, 'smile': 2372, 'emotionally': 2373, 'delivered': 2374, 'dressed': 2375, 'gangster': 2376, 'land': 2377, 'gem': 2378, 'meeting': 2379, 'ground': 2380, 'players': 2381, 'eating': 2382, 'empty': 2383, 'vision': 2384, 'hurt': 2385, 'actor.': 2386, 'ordinary': 2387, '****': 2388, 'endless': 2389, 'director.': 2390, 'convinced': 2391, 'standing': 2392, 'soldier': 2393, 'See': 2394, 'serves': 2395, 'lines,': 2396, 'holding': 2397, 'worse.': 2398, 'claim': 2399, 'destroy': 2400, '/>You': 2401, 'violence,': 2402, 'Morgan': 2403, '/>At': 2404, '/>Not': 2405, 'saving': 2406, \"i'm\": 2407, 'joy': 2408, 'mouth': 2409, 'Jennifer': 2410, 'suffers': 2411, 'least,': 2412, 'artist': 2413, 'blind': 2414, 'spoil': 2415, 'curious': 2416, 'pretentious': 2417, 'bland': 2418, 'murdered': 2419, 'cuts': 2420, 'Watching': 2421, 'obsessed': 2422, '50': 2423, 'does.': 2424, 'station': 2425, 'about,': 2426, 'lives.': 2427, \"would've\": 2428, 'Taylor': 2429, 'humans': 2430, 'loss': 2431, 'pilot': 2432, 'themselves.': 2433, 'around,': 2434, 'Barbara': 2435, 'beautiful,': 2436, 'ever,': 2437, 'references': 2438, \"we've\": 2439, 'style,': 2440, 'law': 2441, 'down,': 2442, 'year.': 2443, 'mom': 2444, 'latest': 2445, 'hearing': 2446, 'Jones': 2447, '(though': 2448, '/>Even': 2449, 'Joseph': 2450, 'days.': 2451, 'heroes': 2452, 'broken': 2453, 'travel': 2454, 'conflict': 2455, 'Last': 2456, 'death,': 2457, 'Asian': 2458, 'talks': 2459, 'insult': 2460, 'length': 2461, 'accurate': 2462, 'chose': 2463, 'versions': 2464, 'son,': 2465, 'pleasant': 2466, 'bet': 2467, 'reaction': 2468, 'wide': 2469, 'remotely': 2470, 'strength': 2471, 'stupid,': 2472, 'Canadian': 2473, 'award': 2474, 'well.<br': 2475, '(especially': 2476, 'underrated': 2477, 'budget,': 2478, 'contemporary': 2479, 'Horror': 2480, 'horribly': 2481, 'drunk': 2482, 'Santa': 2483, 'U.S.': 2484, 'today,': 2485, 'marry': 2486, 'viewed': 2487, 'reviewer': 2488, 'mark': 2489, '/>On': 2490, 'chosen': 2491, 'Are': 2492, 'sleep': 2493, 'talents': 2494, 'theatrical': 2495, 'back,': 2496, 'realizes': 2497, 'Without': 2498, 'hide': 2499, 'ruin': 2500, 'anyway.': 2501, 'provided': 2502, 'passion': 2503, 'moments.': 2504, 'screaming': 2505, 'flick,': 2506, 'anywhere': 2507, 'sing': 2508, 'range': 2509, '/>After': 2510, 'occasional': 2511, 'house.': 2512, 'walked': 2513, 'presents': 2514, 'church': 2515, 'imagination': 2516, 'sense,': 2517, 'blonde': 2518, '12': 2519, 'thank': 2520, 'unfortunate': 2521, 'Overall,': 2522, 'Please': 2523, 'spends': 2524, 'scare': 2525, 'reason.': 2526, 'stars.': 2527, 'Being': 2528, 'suffering': 2529, 'partner': 2530, 'types': 2531, '/>However,': 2532, 'cant': 2533, 'excited': 2534, 'of.': 2535, 'cross': 2536, ')': 2537, 'depicted': 2538, 'gags': 2539, 'kinds': 2540, 'VERY': 2541, 'drugs': 2542, 'wall': 2543, 'United': 2544, 'yet,': 2545, 'hanging': 2546, 'edited': 2547, 'downright': 2548, 'sudden': 2549, 'disappointment': 2550, 'till': 2551, 'version,': 2552, 'Australian': 2553, 'which,': 2554, 'terrible,': 2555, 'humorous': 2556, 'process': 2557, 'draw': 2558, 'old,': 2559, 'experience.': 2560, 'summer': 2561, 'production.': 2562, 'Jackie': 2563, 'convince': 2564, 'Superman': 2565, 'planet': 2566, 'episode.': 2567, 'More': 2568, 'suicide': 2569, 'Then,': 2570, 'nominated': 2571, 'heroine': 2572, 'inner': 2573, 'cinema,': 2574, 'Keaton': 2575, 'Lord': 2576, 'revealed': 2577, 'DVD,': 2578, 'pile': 2579, 'are.': 2580, 'own.': 2581, 'Much': 2582, 'Woody': 2583, 'Story': 2584, 'results': 2585, 'vehicle': 2586, 'things.': 2587, 'player': 2588, 'skip': 2589, 'genre,': 2590, 'channel': 2591, 'pulls': 2592, 'struggling': 2593, 'suit': 2594, 'story.<br': 2595, 'rescue': 2596, 'Simon': 2597, 'accidentally': 2598, 'featured': 2599, 'claims': 2600, 'all.<br': 2601, 'individual': 2602, 'explained': 2603, 'visually': 2604, 'offered': 2605, 'Hong': 2606, 'Thomas': 2607, 'torture': 2608, 'unlikely': 2609, 'unfunny': 2610, 'closer': 2611, 'decision': 2612, 'clichés': 2613, '/>Now': 2614, '/>Some': 2615, 'satire': 2616, 'bank': 2617, 'massive': 2618, 'drama.': 2619, 'author': 2620, 'opposite': 2621, \"What's\": 2622, '/>Of': 2623, 'debut': 2624, 'ALL': 2625, 'private': 2626, 'magnificent': 2627, 'Tarzan': 2628, 'Jesus': 2629, 'woods': 2630, 'Albert': 2631, 'repeated': 2632, 'parody': 2633, 'Ford': 2634, 'one.<br': 2635, 'covered': 2636, 'plots': 2637, 'villains': 2638, 'commercial': 2639, 'Any': 2640, 'context': 2641, 'speaks': 2642, 'skills': 2643, 'disaster': 2644, 'know.': 2645, 'lovers': 2646, 'according': 2647, 'episode,': 2648, 'recognize': 2649, 'Roger': 2650, '/>Then': 2651, 'writing,': 2652, 'fat': 2653, 'focuses': 2654, '/>Another': 2655, 'something.': 2656, 'lower': 2657, 'Victor': 2658, 'Walter': 2659, 'source': 2660, 'cinematography,': 2661, 'evidence': 2662, 'naive': 2663, 'stupid.': 2664, 'Matt': 2665, 'multiple': 2666, 'line,': 2667, 'reveals': 2668, 'humor.': 2669, 'worse,': 2670, 'test': 2671, 'None': 2672, 'During': 2673, 'beginning,': 2674, 'lesson': 2675, 'segment': 2676, 'cable': 2677, 'sex,': 2678, 'program': 2679, 'stayed': 2680, 'Dark': 2681, 'owner': 2682, 'shot,': 2683, 'stuff.': 2684, 'paying': 2685, 'audience,': 2686, 'leader': 2687, 'directly': 2688, 'Western': 2689, 'lousy': 2690, 'me.<br': 2691, 'soft': 2692, 'stays': 2693, 'Luke': 2694, 'boy,': 2695, 'About': 2696, 'advice': 2697, 'Captain': 2698, 'community': 2699, 'Kong': 2700, 'Howard': 2701, 'and/or': 2702, 'instead.': 2703, 'think,': 2704, 'spite': 2705, 'dated': 2706, '/>-': 2707, 'friendship': 2708, 'Daniel': 2709, 'gory': 2710, 'of,': 2711, 'clue': 2712, 'Saturday': 2713, 'plans': 2714, 'friends.': 2715, 'fashion': 2716, 'nothing.': 2717, 'survive': 2718, 'explains': 2719, 'standards': 2720, 'club': 2721, 'rise': 2722, 'judge': 2723, 'first.': 2724, 'recall': 2725, '/>That': 2726, 'Prince': 2727, 'favor': 2728, 'High': 2729, 'ridiculous.': 2730, 'figured': 2731, 'theatre': 2732, 'days,': 2733, 'handled': 2734, 'Robin': 2735, 'Freddy': 2736, 'blue': 2737, 'Dan': 2738, 'exist': 2739, 'Irish': 2740, 'children,': 2741, 'influence': 2742, 'formula': 2743, 'cars': 2744, 'football': 2745, '/>We': 2746, 'Louis': 2747, 'vote': 2748, 'shallow': 2749, 'focused': 2750, 'did,': 2751, 'appropriate': 2752, 'touches': 2753, 'behavior': 2754, 'necessarily': 2755, 'handsome': 2756, 'realism': 2757, 'wise': 2758, 'facial': 2759, 'reveal': 2760, 'heavily': 2761, 'bothered': 2762, \"You'll\": 2763, 'Broadway': 2764, 'sympathetic': 2765, 'mind,': 2766, 'possible.': 2767, 'green': 2768, 'cost': 2769, 'hired': 2770, 'this.<br': 2771, 'THAT': 2772, 'else,': 2773, 'treatment': 2774, 'Donald': 2775, 'novel,': 2776, 'TV.': 2777, 'legendary': 2778, 'sounded': 2779, 'very,': 2780, 'home.': 2781, 'machine': 2782, 'amateur': 2783, \"/>Don't\": 2784, 'hilarious.': 2785, 'excellent.': 2786, 'starred': 2787, 'mainstream': 2788, 'contrast': 2789, 'designed': 2790, 'baseball': 2791, 'mess.': 2792, 'shocked': 2793, 'dozen': 2794, 'exact': 2795, 'officer': 2796, 'research': 2797, 'regret': 2798, 'Especially': 2799, 'Each': 2800, 'words,': 2801, 'spectacular': 2802, 'friend,': 2803, 'Patrick': 2804, 'horse': 2805, 'exploitation': 2806, 'sports': 2807, 'throws': 2808, 'significant': 2809, 'buying': 2810, 'dubbed': 2811, \"he'd\": 2812, 'fate': 2813, 'mere': 2814, 'greater': 2815, \"could've\": 2816, 'drop': 2817, 'board': 2818, \"father's\": 2819, \"They're\": 2820, 'African': 2821, 'Mrs.': 2822, 'ladies': 2823, 'attitude': 2824, '(at': 2825, 'depiction': 2826, 'experienced': 2827, 'Let': 2828, 'post': 2829, 'wedding': 2830, 'placed': 2831, 'things,': 2832, 'career.': 2833, 'guns': 2834, 'model': 2835, 'trapped': 2836, 'supernatural': 2837, 'Day': 2838, 'cutting': 2839, 'Have': 2840, 'San': 2841, 'amazed': 2842, 'appealing': 2843, 'Roy': 2844, 'Never': 2845, 'faith': 2846, 'depressing': 2847, 'excellent,': 2848, 'Hitler': 2849, 'morning': 2850, 'picks': 2851, 'reminiscent': 2852, 'warm': 2853, 'lucky': 2854, 'over,': 2855, 'II': 2856, 'Edward': 2857, 'Anne': 2858, 'zero': 2859, 'desperately': 2860, 'embarrassing': 2861, 'entertainment.': 2862, 'accident': 2863, 'wit': 2864, 'voices': 2865, 'Alex': 2866, 'seriously.': 2867, 'irritating': 2868, 'roles,': 2869, 'passing': 2870, 'Nancy': 2871, 'daughter,': 2872, 'so-called': 2873, 'over-the-top': 2874, 'veteran': 2875, '/>With': 2876, 'Cage': 2877, 'enjoying': 2878, 'dry': 2879, 'helping': 2880, 'ring': 2881, 'combined': 2882, 'Life': 2883, 'wear': 2884, 'forever': 2885, '(he': 2886, 'roll': 2887, 'abandoned': 2888, 'this?': 2889, 'Washington': 2890, 'games': 2891, 'previously': 2892, 'committed': 2893, 'protagonist': 2894, 'fictional': 2895, \"people's\": 2896, 'laugh.': 2897, 'display': 2898, 'Thank': 2899, 'fly': 2900, 'TO': 2901, 'dreadful': 2902, 'carries': 2903, 'Sometimes': 2904, 'girl.': 2905, 'others,': 2906, 'mother,': 2907, 'witty': 2908, 'carried': 2909, 'drag': 2910, 'guy.': 2911, 'required': 2912, 'la': 2913, 'real.': 2914, 'fault': 2915, '(including': 2916, 'suffer': 2917, 'efforts': 2918, 'international': 2919, 'identity': 2920, '(it': 2921, 'out.<br': 2922, 'laugh,': 2923, 'gore,': 2924, 'join': 2925, 'youth': 2926, 'war,': 2927, 'convey': 2928, 'serve': 2929, 'prepared': 2930, 'mission': 2931, 'native': 2932, 'version.': 2933, 'locations': 2934, 'blown': 2935, 'IN': 2936, 'decade': 2937, 'Kim': 2938, 'refreshing': 2939, 'instance,': 2940, 'guilty': 2941, 'powers': 2942, 'purely': 2943, 'sake': 2944, 'prefer': 2945, 'children.': 2946, 'round': 2947, 'product': 2948, 'stolen': 2949, 'changing': 2950, 'satisfying': 2951, 'driven': 2952, 'foot': 2953, 'listening': 2954, 'grand': 2955, 'Wayne': 2956, 'women,': 2957, 'paint': 2958, 'face.': 2959, 'renting': 2960, 'that.<br': 2961, 'village': 2962, 'via': 2963, 'perspective': 2964, 'kids,': 2965, 'us.': 2966, 'deeper': 2967, 'nude': 2968, 'technology': 2969, 'prior': 2970, 'seek': 2971, 'clean': 2972, 'beloved': 2973, 'faithful': 2974, 'Old': 2975, 'Marie': 2976, 'film-making': 2977, 'corny': 2978, 'mine': 2979, 'throwing': 2980, 'training': 2981, 'actress,': 2982, 'feet': 2983, 'logic': 2984, 'streets': 2985, 'production,': 2986, 'another.': 2987, '/>So,': 2988, 'reporter': 2989, 'frame': 2990, 'experiences': 2991, 'touched': 2992, 'yes': 2993, 'say.': 2994, 'way.<br': 2995, 'watchable': 2996, 'remind': 2997, 'sympathy': 2998, 'tape': 2999, 'print': 3000, 'Oliver': 3001, 'for,': 3002, 'protect': 3003, 'closing': 3004, 'another,': 3005, 'tiny': 3006, 'Time': 3007, 'carrying': 3008, 'finale': 3009, 'movie:': 3010, 'insane': 3011, 'attempting': 3012, 'center': 3013, 'besides': 3014, 'haunted': 3015, 'slow,': 3016, 'insight': 3017, 'flashbacks': 3018, '80s': 3019, 'Anna': 3020, 'TV,': 3021, 'Sinatra': 3022, 'us,': 3023, 'regarding': 3024, 'halfway': 3025, 'past,': 3026, 'predictable,': 3027, 'stereotypes': 3028, 'visuals': 3029, 'matters': 3030, 'here.<br': 3031, 'raised': 3032, 'monsters': 3033, 'Come': 3034, 'comparison': 3035, 'false': 3036, 'wife.': 3037, 'causes': 3038, '2.': 3039, 'film?': 3040, 'why.': 3041, 'floor': 3042, 'no,': 3043, 'happened.': 3044, 'dark,': 3045, 'surreal': 3046, 'Bond': 3047, 'Ron': 3048, 'relief': 3049, 'Rob': 3050, 'Texas': 3051, 'throughout.': 3052, 'school.': 3053, 'chick': 3054, 'highlight': 3055, 'jumps': 3056, 'critical': 3057, 'kids.': 3058, 'witness': 3059, 'promising': 3060, 'concerned': 3061, 'magical': 3062, 'Columbo': 3063, 'raise': 3064, 'brother,': 3065, 'while,': 3066, 'Okay,': 3067, 'Mexican': 3068, 'facts': 3069, 'revolves': 3070, 'embarrassed': 3071, 'dull,': 3072, 'Get': 3073, 'true.': 3074, 'replaced': 3075, \"/>There's\": 3076, 'haunting': 3077, 'Lynch': 3078, 'variety': 3079, 'Parker': 3080, 'existence': 3081, 'security': 3082, '/>Although': 3083, 'anything,': 3084, 'humanity': 3085, 'performed': 3086, 'disgusting': 3087, 'Avoid': 3088, 'horrible.': 3089, 'understood': 3090, 'Me': 3091, 'wears': 3092, 'entertaining,': 3093, 'costume': 3094, 'harsh': 3095, 'families': 3096, 'crying': 3097, \"children's\": 3098, 'account': 3099, 'Your': 3100, 'contrived': 3101, 'figures': 3102, 'side,': 3103, 'cash': 3104, 'film!': 3105, 'life.<br': 3106, 'sharp': 3107, 'warn': 3108, 'rental': 3109, 'makeup': 3110, 'singer': 3111, 'grade': 3112, 'wasting': 3113, 'cultural': 3114, 'Bourne': 3115, 'Welles': 3116, 'effectively': 3117, '11': 3118, 'Harris': 3119, 'mildly': 3120, 'safe': 3121, 'ignore': 3122, 'amongst': 3123, 'refuses': 3124, 'proud': 3125, 'sorts': 3126, 'glimpse': 3127, 'bear': 3128, 'blow': 3129, 'believed': 3130, 'Japan': 3131, 'later.': 3132, 'Russell': 3133, 'luck': 3134, 'head.': 3135, 'father.': 3136, 'Brad': 3137, 'breaking': 3138, 'woman.': 3139, 'lesbian': 3140, 'thoughts': 3141, 'handle': 3142, 'Does': 3143, 'achieve': 3144, 'Julie': 3145, 'determined': 3146, 'England': 3147, 'Fox': 3148, 'anime': 3149, 'UK': 3150, 'site': 3151, 'asleep': 3152, 'forgot': 3153, 'window': 3154, 'teenagers': 3155, 'calling': 3156, 'adding': 3157, 'sold': 3158, 'amateurish': 3159, 'fans.': 3160, 'make-up': 3161, 'heart.': 3162, 'school,': 3163, 'executed': 3164, 'twisted': 3165, 'warning': 3166, 'pity': 3167, 'mentally': 3168, 'suffered': 3169, 'boat': 3170, 'lawyer': 3171, 'Jon': 3172, 'score,': 3173, 'medical': 3174, 'weeks': 3175, 'game.': 3176, 'Stone': 3177, 'indie': 3178, 'Victoria': 3179, '/>First': 3180, 'speech': 3181, 'NO': 3182, 'Powell': 3183, 'erotic': 3184, 'succeeds': 3185, 'witch': 3186, 'stereotypical': 3187, 'direction.': 3188, 'Dean': 3189, 'amazingly': 3190, 'correct': 3191, 'consists': 3192, 'May': 3193, 'lot.': 3194, 'quest': 3195, 'Shakespeare': 3196, 'Death': 3197, 'Full': 3198, 'encounter': 3199, 'drives': 3200, 'field': 3201, 'priest': 3202, 'laughter': 3203, 'Dennis': 3204, 'complicated': 3205, 'gold': 3206, '25': 3207, 'again.<br': 3208, 'end.<br': 3209, 'horror,': 3210, 'ripped': 3211, 'Had': 3212, 'sensitive': 3213, 'mature': 3214, 'creatures': 3215, 'crowd': 3216, 'Max': 3217, 'history,': 3218, 'Rachel': 3219, 'seasons': 3220, 'Take': 3221, 'camera.': 3222, 'shop': 3223, 'annoying.': 3224, 'Unlike': 3225, 'inept': 3226, 'learning': 3227, 'fallen': 3228, 'Warner': 3229, 'Jewish': 3230, 'Trek': 3231, 'attacked': 3232, 'Still': 3233, \"ain't\": 3234, 'go.': 3235, 'vs.': 3236, 'Three': 3237, 'offensive': 3238, 'Over': 3239, 'brave': 3240, 'DO': 3241, 'Our': 3242, 'served': 3243, 'contain': 3244, 'deadly': 3245, 'dirty': 3246, 'target': 3247, 'men,': 3248, 'locked': 3249, 'stars,': 3250, 'Drew': 3251, 'town.': 3252, 'essential': 3253, 'year,': 3254, 'that?': 3255, 'BBC': 3256, 'Leslie': 3257, 'sum': 3258, 'price': 3259, 'Moore': 3260, 'destroyed': 3261, 'sinister': 3262, 'established': 3263, 'express': 3264, 'level.': 3265, 'separate': 3266, '/>Anyway,': 3267, 'continuity': 3268, 'Sure': 3269, 'grace': 3270, 'related': 3271, 'intellectual': 3272, 'excitement': 3273, 'bodies': 3274, 'war.': 3275, 'abuse': 3276, 'Lost': 3277, 'different.': 3278, 'section': 3279, 'violence.': 3280, 'dislike': 3281, 'MGM': 3282, 'remote': 3283, 'Julia': 3284, 'whatsoever.': 3285, 'mistakes': 3286, 'ones.': 3287, 'written,': 3288, 'views': 3289, 'tied': 3290, 'North': 3291, 'desert': 3292, 'letting': 3293, 'predictable.': 3294, 'reference': 3295, 'struggles': 3296, 'past.': 3297, 'shot.': 3298, '70s': 3299, 'works.': 3300, 'qualities': 3301, 'Sunday': 3302, 'rough': 3303, 'Halloween': 3304, 'myself,': 3305, 'Definitely': 3306, 'seat': 3307, 'clips': 3308, 'something,': 3309, 'introduction': 3310, 'routine': 3311, 'Why?': 3312, 'appreciated': 3313, 'uncle': 3314, 'propaganda': 3315, 'forth': 3316, 'sends': 3317, 'dialogue.': 3318, 'examples': 3319, 'interview': 3320, 'overcome': 3321, 'reputation': 3322, 'title,': 3323, 'failure': 3324, 'ways,': 3325, 'concerns': 3326, 'alone.': 3327, 'Cinderella': 3328, 'classics': 3329, 'notable': 3330, 'path': 3331, 'tedious': 3332, 'teenager': 3333, 'connected': 3334, '14': 3335, 'conversation': 3336, 'face,': 3337, 'hundreds': 3338, 'Someone': 3339, 'regard': 3340, 'on.<br': 3341, 'infamous': 3342, 'frequently': 3343, 'everyday': 3344, 'case.': 3345, 'fans,': 3346, 'skin': 3347, 'Sir': 3348, 'Sidney': 3349, 'happen.': 3350, 'Sarah': 3351, 'crude': 3352, 'perfect.': 3353, 'future.': 3354, 'garbage.': 3355, 'delivery': 3356, 'eight': 3357, 'Larry': 3358, 'dealt': 3359, 'heck': 3360, 'position': 3361, 'believable.': 3362, 'eyes,': 3363, 'hint': 3364, 'ball': 3365, 'degree': 3366, 'saves': 3367, 'picture,': 3368, 'searching': 3369, 'ashamed': 3370, 'interest.': 3371, 'fan.': 3372, 'extraordinary': 3373, 'interpretation': 3374, 'broke': 3375, 'gruesome': 3376, 'generation': 3377, 'Burt': 3378, 'greatly': 3379, 'seeking': 3380, 'checking': 3381, 'quality.': 3382, 'challenge': 3383, 'Ryan': 3384, 'core': 3385, 'Street': 3386, 'perform': 3387, 'bomb': 3388, 'Friday': 3389, 'anymore.': 3390, 'description': 3391, 'wealthy': 3392, 'stops': 3393, 'act.': 3394, 'Something': 3395, 'Overall': 3396, 'sucked': 3397, 'REALLY': 3398, 'statement': 3399, 'Such': 3400, 'rubbish': 3401, 'unrealistic': 3402, 'career,': 3403, 'sequels': 3404, 'flesh': 3405, '(one': 3406, 'play,': 3407, 'Jessica': 3408, 'lesser': 3409, 'dragged': 3410, 'device': 3411, 'pointed': 3412, 'gas': 3413, 'prime': 3414, 'friendly': 3415, 'Gordon': 3416, 'person,': 3417, 'through,': 3418, 'women.': 3419, 'hundred': 3420, 'nonsense': 3421, 'style.': 3422, '/>From': 3423, 'play.': 3424, 'opened': 3425, 'Ned': 3426, 'stories,': 3427, 'arrives': 3428, 'editing,': 3429, 'Wars': 3430, 'angles': 3431, 'wind': 3432, 'successfully': 3433, 'freedom': 3434, 'lonely': 3435, 'thousands': 3436, 'husband,': 3437, 'campy': 3438, 'expert': 3439, 'disappointment.': 3440, \"woman's\": 3441, 'is.<br': 3442, 'gritty': 3443, 'sleeping': 3444, 'Brothers': 3445, 'Anderson': 3446, 'then.': 3447, 'Be': 3448, 'Bette': 3449, 'Murphy': 3450, 'sad,': 3451, 'escapes': 3452, 'poor,': 3453, 'cares': 3454, \"i've\": 3455, 'teach': 3456, 'wishes': 3457, 'horrific': 3458, 'Stanley': 3459, 'develops': 3460, 'sleazy': 3461, 'Ralph': 3462, 'solve': 3463, 'Nazi': 3464, 'surrounding': 3465, 'entertain': 3466, 'Lisa': 3467, 'rip': 3468, 'person.': 3469, '10/10': 3470, 'Zombie': 3471, 'spoof': 3472, 'Michelle': 3473, 'lines.': 3474, 'ill': 3475, 'moment,': 3476, 'gain': 3477, 'material.': 3478, 'honest,': 3479, 'mindless': 3480, 'Susan': 3481, 'Along': 3482, 'amazing.': 3483, 'Claire': 3484, 'birth': 3485, 'clichéd': 3486, 'deaths': 3487, 'suspenseful': 3488, 'breath': 3489, 'cynical': 3490, 'execution': 3491, 'real,': 3492, 'dialog,': 3493, 'ass': 3494, 'Green': 3495, 'Police': 3496, 'horrible,': 3497, 'physically': 3498, 'countless': 3499, 'Things': 3500, 'J.': 3501, 'levels': 3502, 'minds': 3503, 'Probably': 3504, ':)': 3505, 'criticism': 3506, 'old.': 3507, 'increasingly': 3508, 'usual,': 3509, '/>the': 3510, 'child.': 3511, 'Blood': 3512, 'Ted': 3513, 'shines': 3514, 'structure': 3515, 'themselves,': 3516, 'shoots': 3517, 'upset': 3518, '(that': 3519, 'lazy': 3520, 'FBI': 3521, 'plastic': 3522, 'speed': 3523, 'Lewis': 3524, 'par': 3525, 'idea.': 3526, 'sets,': 3527, 'Wood': 3528, 'skill': 3529, 'narration': 3530, 'slapstick': 3531, 'Grant': 3532, 'Maria': 3533, 'Bo': 3534, 'portrait': 3535, 'ensemble': 3536, 'answers': 3537, 'thriller,': 3538, 'uncomfortable': 3539, 'mass': 3540, 'hoped': 3541, 'sure,': 3542, 'suspense,': 3543, 'initially': 3544, 'lights': 3545, 'promise': 3546, 'spending': 3547, 'flashback': 3548, 'associated': 3549, 'drinking': 3550, 'troubled': 3551, 'unfortunately,': 3552, 'belongs': 3553, 'wonderful.': 3554, 'Kurt': 3555, 'Killer': 3556, 'spoken': 3557, 'once,': 3558, 'awards': 3559, 'naturally': 3560, 'child,': 3561, 'market': 3562, 'accents': 3563, 'opinion.': 3564, 'fairy': 3565, 'advantage': 3566, 'Army': 3567, 'Vietnam': 3568, 'own,': 3569, 'fabulous': 3570, 'base': 3571, 'anger': 3572, 'Jr.': 3573, 'Sadly,': 3574, 'adapted': 3575, 'interviews': 3576, 'surrounded': 3577, 'Freeman': 3578, 'stood': 3579, 'either,': 3580, 'factor': 3581, 'YOU': 3582, 'miles': 3583, 'grows': 3584, 'politics': 3585, 'viewer.': 3586, 'inspiration': 3587, 'hang': 3588, 'like.': 3589, 'uninteresting': 3590, 'covers': 3591, 'Elizabeth': 3592, 'praise': 3593, 'storyline,': 3594, 'Give': 3595, 'reality,': 3596, 'attempted': 3597, 'La': 3598, 'expression': 3599, 'yeah,': 3600, 'episodes,': 3601, 'Yes': 3602, 'fare': 3603, 'teens': 3604, 'shows.': 3605, 'caring': 3606, 'entertained': 3607, 'status': 3608, 'allowing': 3609, 'says,': 3610, 'movement': 3611, 'aged': 3612, '(of': 3613, 'enters': 3614, 'religion': 3615, 'pregnant': 3616, \"they'd\": 3617, 'happily': 3618, 'what?': 3619, 'nudity,': 3620, 'bus': 3621, 'pleasantly': 3622, 'term': 3623, 'unconvincing': 3624, 'suggests': 3625, 'right?': 3626, 'talent.': 3627, 'annoyed': 3628, 'cartoons': 3629, 'obsession': 3630, 'classic,': 3631, 'etc': 3632, 'go,': 3633, 'Philip': 3634, 'Germany': 3635, 'Bettie': 3636, 'opposed': 3637, 'believing': 3638, 'chance.': 3639, 'sings': 3640, 'balance': 3641, 'tight': 3642, 'identify': 3643, 'ridiculously': 3644, 'goofy': 3645, 'festival': 3646, 'town,': 3647, 'represents': 3648, 'marvelous': 3649, 'tradition': 3650, 'lying': 3651, 'tribute': 3652, 'Golden': 3653, 'budget.': 3654, 'hiding': 3655, 'there.<br': 3656, 'equal': 3657, 'SO': 3658, 'sequel.': 3659, 'Show': 3660, 'legend': 3661, 'realise': 3662, '1st': 3663, 'Bollywood': 3664, '\"What': 3665, 'stylish': 3666, 'game,': 3667, 'werewolf': 3668, 'extras': 3669, 'frightening': 3670, 'quirky': 3671, 'evening': 3672, 'riding': 3673, 'Here,': 3674, 'bitter': 3675, 'flow': 3676, 'wins': 3677, 'good.<br': 3678, 'masterpiece.': 3679, 'have.': 3680, 'daily': 3681, 'nice,': 3682, 'accepted': 3683, 'mountain': 3684, 'rolling': 3685, 'mob': 3686, 'essence': 3687, 'Alice': 3688, 'prevent': 3689, 'imagery': 3690, 'young,': 3691, 'surprises': 3692, 'thousand': 3693, 'everyone.': 3694, 'substance': 3695, 'steps': 3696, 'rules': 3697, 'Andrew': 3698, 'millions': 3699, 'contact': 3700, 'Queen': 3701, 'bore': 3702, 'torn': 3703, '1.': 3704, 'handful': 3705, 'enter': 3706, 'artists': 3707, 'C.': 3708, 'rival': 3709, 'nuclear': 3710, 'far,': 3711, 'attention.': 3712, 'dollars': 3713, 'tons': 3714, 'welcome': 3715, 'aforementioned': 3716, 'attracted': 3717, 'urban': 3718, 'matter.': 3719, 'Chan': 3720, 'specific': 3721, 'chasing': 3722, 'does,': 3723, '\"I\\'m': 3724, 'horror.': 3725, '/>By': 3726, 'reduced': 3727, 'Rose': 3728, 'striking': 3729, 'ironic': 3730, 'requires': 3731, 'Emma': 3732, 'sexually': 3733, 'hopefully': 3734, 'seriously,': 3735, 'guessing': 3736, 'directorial': 3737, 'danger': 3738, 'oil': 3739, 'cell': 3740, 'beautiful.': 3741, 'raw': 3742, 'split': 3743, 'bound': 3744, 'comical': 3745, '10,': 3746, 'blood,': 3747, 'thriller.': 3748, 'thought,': 3749, 'Europe': 3750, 'Apparently': 3751, 'dollar': 3752, 'strictly': 3753, 'Living': 3754, 'wondered': 3755, 'struck': 3756, 'paced': 3757, 'fourth': 3758, 'Back': 3759, 'release.': 3760, 'really.': 3761, 'neat': 3762, 'risk': 3763, '/>Well,': 3764, 'teeth': 3765, 'dress': 3766, 'underground': 3767, 'States': 3768, 'pushed': 3769, 'adventures': 3770, 'Pacino': 3771, 'strangely': 3772, 'shots,': 3773, 'theory': 3774, 'Pitt': 3775, 'Look': 3776, 'remaining': 3777, 'repeat': 3778, 'talked': 3779, 'problems,': 3780, 'eyes.': 3781, 'task': 3782, 'Almost': 3783, 'theaters': 3784, 'enjoyable.': 3785, 'profound': 3786, 'appearing': 3787, 'funnier': 3788, 'Wilson': 3789, 'Lugosi': 3790, 'moment.': 3791, 'Sci-Fi': 3792, 'problem.': 3793, 'kicks': 3794, 'intensity': 3795, 'Jay': 3796, '(John': 3797, '(except': 3798, 'Movies': 3799, 'release,': 3800, 'pulling': 3801, 'real-life': 3802, 'anyway,': 3803, 'idea,': 3804, 'First,': 3805, 'paper': 3806, 'Caine': 3807, 'rights': 3808, 'basis': 3809, 'metal': 3810, 'courage': 3811, 'chief': 3812, 'entertainment,': 3813, 'video.': 3814, 'rid': 3815, 'Catherine': 3816, 'page': 3817, 'subsequent': 3818, 'commit': 3819, 'screening': 3820, 'heart,': 3821, 'Page': 3822, 'object': 3823, 'circumstances': 3824, 'castle': 3825, 'draws': 3826, 'once.': 3827, 'per': 3828, 'buddy': 3829, \"here's\": 3830, 'slight': 3831, 'fifteen': 3832, 'Finally,': 3833, 'drags': 3834, 'Guy': 3835, 'entry': 3836, 'wanna': 3837, 'her.<br': 3838, 'text': 3839, 'Clark': 3840, 'lot,': 3841, 'yourself.': 3842, 'too.<br': 3843, 'Music': 3844, 'Stan': 3845, 'expensive': 3846, 'performing': 3847, '20th': 3848, 'Really': 3849, 'Sky': 3850, 'sisters': 3851, 'tremendous': 3852, 'anyone.': 3853, 'scary,': 3854, 'recommended.': 3855, 'reasonably': 3856, 'topic': 3857, 'controversial': 3858, 'colors': 3859, 'authentic': 3860, 'killer,': 3861, 'hitting': 3862, 'Hollywood.': 3863, 'IT': 3864, 'sitcom': 3865, 'silly,': 3866, 'Soon': 3867, 'Korean': 3868, 'Award': 3869, 'achieved': 3870, 'unforgettable': 3871, 'Reed': 3872, 'parts,': 3873, 'Uncle': 3874, 'mask': 3875, '(this': 3876, 'Channel': 3877, 'Again,': 3878, 'homeless': 3879, 'mid': 3880, 'murderer': 3881, 'reactions': 3882, 'threw': 3883, \"/>I've\": 3884, 'idiot': 3885, 'watches': 3886, 'film;': 3887, 'television.': 3888, 'dropped': 3889, 'ways.': 3890, 'quote': 3891, 'larger': 3892, 'river': 3893, 'sole': 3894, 'shut': 3895, 'Hitchcock': 3896, 'lacked': 3897, 'sheriff': 3898, 'forgive': 3899, 'criminals': 3900, 'punch': 3901, 'spy': 3902, 'crafted': 3903, 'charismatic': 3904, 'everything.': 3905, 'tense': 3906, 'at.': 3907, 'seeing.': 3908, 'France': 3909, 'matter,': 3910, 'frankly': 3911, 'definite': 3912, 'racist': 3913, 'killer.': 3914, 'line.': 3915, 'obnoxious': 3916, 'hours.': 3917, 'revealing': 3918, 'screenwriter': 3919, 'partly': 3920, 'faced': 3921, 'aimed': 3922, 'movies.<br': 3923, 'graphics': 3924, 'stealing': 3925, 'two.': 3926, 'thrilling': 3927, 'melodrama': 3928, 'sequence,': 3929, 'rule': 3930, 'relevant': 3931, 'sides': 3932, 'Long': 3933, 'jokes,': 3934, 'listed': 3935, 'Hardy': 3936, 'closely': 3937, 'complaint': 3938, 'notorious': 3939, 'men.': 3940, 'retarded': 3941, 'laid': 3942, 'attached': 3943, 'beating': 3944, 'Universal': 3945, 'society.': 3946, '/>Despite': 3947, 'jobs': 3948, 'Science': 3949, 'killers': 3950, 'flawed': 3951, 'wonders': 3952, '13': 3953, 'car,': 3954, 'fitting': 3955, 'kid,': 3956, 'better.<br': 3957, 'providing': 3958, 'choices': 3959, 'murder,': 3960, 'busy': 3961, 'packed': 3962, 'stories.': 3963, 'host': 3964, 'credible': 3965, 'van': 3966, 'eerie': 3967, 'national': 3968, 'picking': 3969, 'ages': 3970, 'Catholic': 3971, 'idiotic': 3972, 'bad.<br': 3973, 'forgettable': 3974, 'guys,': 3975, 'same.': 3976, 'Blair': 3977, 'talent,': 3978, 'whoever': 3979, 'continued': 3980, 'stronger': 3981, 'sword': 3982, 'scary.': 3983, 'age,': 3984, 'NEVER': 3985, 'dead.': 3986, 'sucks': 3987, 'level,': 3988, 'truck': 3989, 'push': 3990, 'u': 3991, 'stunt': 3992, 'Pretty': 3993, 'chilling': 3994, 'Dorothy': 3995, 'gripping': 3996, 'Jeremy': 3997, 'reality.': 3998, 'scenario': 3999, 'terrifying': 4000, 'film:': 4001, 'daughters': 4002, 'enjoys': 4003, 'imaginative': 4004, 'dig': 4005, 'worthwhile': 4006, 'belief': 4007, 'age.': 4008, 'Timothy': 4009, 'scream': 4010, 'film)': 4011, 'fool': 4012, 'head,': 4013, 'displays': 4014, 'receive': 4015, 'Helen': 4016, \"show's\": 4017, 'Yeah,': 4018, 'Los': 4019, 'is:': 4020, 'concerning': 4021, 'accused': 4022, 'perfect,': 4023, 'easier': 4024, 'corrupt': 4025, 'stomach': 4026, 'competent': 4027, 'planning': 4028, 'directing,': 4029, '(to': 4030, 'Out': 4031, 'lovable': 4032, 'elderly': 4033, 'cruel': 4034, 'brilliant,': 4035, 'pleased': 4036, 'decades': 4037, 'parts.': 4038, 'trick': 4039, 'Laurel': 4040, 'Evil': 4041, 'scares': 4042, 'scale': 4043, 'messages': 4044, 'ought': 4045, 'shape': 4046, 'admire': 4047, 'lie': 4048, 'fame': 4049, 'effort.': 4050, 'resembles': 4051, 'cool,': 4052, 'proof': 4053, 'God,': 4054, 'meaningful': 4055, 'romance,': 4056, 'poignant': 4057, 'settings': 4058, 'Besides': 4059, 'Highly': 4060, 'attacks': 4061, 'America.': 4062, 'Tracy': 4063, 'enjoyment': 4064, 'because,': 4065, 'Stanwyck': 4066, 'arm': 4067, 'costs.': 4068, \"world's\": 4069, 'extended': 4070, 'true,': 4071, 'instantly': 4072, 'laughable.': 4073, 'name.': 4074, 'stuff,': 4075, 'hole': 4076, 'explore': 4077, 'returning': 4078, 'Blue': 4079, 'buried': 4080, 'joke.': 4081, 'happen,': 4082, 'help.': 4083, 'weapons': 4084, 'jumping': 4085, 'mother.': 4086, 'blend': 4087, 'start.': 4088, 'pretend': 4089, 'sea': 4090, 'Mel': 4091, 'Lucy': 4092, 'Denzel': 4093, '\"You': 4094, 'reached': 4095, 'short.': 4096, 'shows,': 4097, 'personalities': 4098, 'crash': 4099, 'Ian': 4100, 'sister,': 4101, 'National': 4102, 'wake': 4103, 'Vincent': 4104, 'video,': 4105, 'exists': 4106, 'relies': 4107, 'dogs': 4108, 'Park': 4109, 'string': 4110, 'gotta': 4111, 'segments': 4112, 'Seagal': 4113, 'homage': 4114, 'girls,': 4115, 'Cooper': 4116, 'unintentionally': 4117, 'psycho': 4118, 'cared': 4119, 'junk': 4120, 'hero,': 4121, 'extent': 4122, 'enjoy.': 4123, 'frustrated': 4124, 'patient': 4125, 'carefully': 4126, 'atmosphere,': 4127, '/>He': 4128, '(no': 4129, 'have,': 4130, 'Yet,': 4131, 'encounters': 4132, 'sophisticated': 4133, 'exercise': 4134, 'aired': 4135, 'expressions': 4136, 'think.': 4137, 'camera,': 4138, 'dare': 4139, 'charge': 4140, 'driver': 4141, 'conclusion,': 4142, 'devoted': 4143, 'throughout,': 4144, 'General': 4145, 'precious': 4146, 'service': 4147, 'weekend': 4148, 'spell': 4149, 'distant': 4150, 'cameos': 4151, 'development,': 4152, 'dubbing': 4153, 'sticks': 4154, 'arms': 4155, 'Whether': 4156, 'it)': 4157, 'hire': 4158, '2,': 4159, 'ridiculous,': 4160, 'R': 4161, 'this:': 4162, 'Brooks': 4163, 'thirty': 4164, 'concert': 4165, 'Sandler': 4166, 'Neil': 4167, 'Jamie': 4168, 'Wes': 4169, 'star.': 4170, 'colorful': 4171, '(such': 4172, 'spiritual': 4173, 'while.': 4174, 'albeit': 4175, 'technically': 4176, 'Eva': 4177, 'films.<br': 4178, 'brilliant.': 4179, 'treasure': 4180, 'stretch': 4181, 'Johnson': 4182, 'dude': 4183, 'Africa': 4184, 'musicals': 4185, 'load': 4186, 'hilarious,': 4187, 'ratings': 4188, 'unbelievably': 4189, 'manager': 4190, 'stated': 4191, 'evident': 4192, 'sentimental': 4193, 'appearances': 4194, 'Hollywood,': 4195, 'knock': 4196, 'scientists': 4197, 'kung': 4198, 'rural': 4199, 'Spike': 4200, 'accomplished': 4201, 'robot': 4202, 'laughs,': 4203, 'importance': 4204, 'vampires': 4205, 'ice': 4206, 'atmospheric': 4207, 'gradually': 4208, '1950s': 4209, 'countries': 4210, 'forest': 4211, 'viewing.': 4212, 'mansion': 4213, 'novel.': 4214, 'Meanwhile,': 4215, 'bit,': 4216, 'typically': 4217, 'darker': 4218, 'bit.': 4219, 'MST3K': 4220, 'intention': 4221, 'enemy': 4222, 'drink': 4223, '/>They': 4224, 'involved.': 4225, \"they'll\": 4226, 'shower': 4227, 'cardboard': 4228, 'gross': 4229, 'fired': 4230, 'daughter.': 4231, 'states': 4232, 'theater.': 4233, 'tune': 4234, '\"My': 4235, 'reasons.': 4236, '/>Also': 4237, 'son.': 4238, 'Matthau': 4239, 'exceptional': 4240, 'always,': 4241, 'minimal': 4242, 'laughs.': 4243, 'surface': 4244, 'York,': 4245, 'name,': 4246, 'irony': 4247, 'suited': 4248, 'shall': 4249, 'futuristic': 4250, 'neighborhood': 4251, 'Whatever': 4252, 'it...': 4253, 'poor.': 4254, 'overall,': 4255, 'category': 4256, 'kidnapped': 4257, '*': 4258, 'President': 4259, 'protagonists': 4260, 'Mystery': 4261, 'art.': 4262, 'disbelief': 4263, 'catches': 4264, 'returned': 4265, 'comedian': 4266, 'ludicrous': 4267, 'Plus,': 4268, 'Ghost': 4269, 'care.': 4270, 'Branagh': 4271, 'builds': 4272, 'superbly': 4273, 'breathtaking': 4274, 'Think': 4275, \"he'll\": 4276, 'dead,': 4277, 'deliberately': 4278, 'cases': 4279, 'timing': 4280, 'trilogy': 4281, 'trash.': 4282, 'Festival': 4283, 'causing': 4284, 'financial': 4285, 'star,': 4286, 'holiday': 4287, 'primary': 4288, 'technique': 4289, 'Nightmare': 4290, 'experience,': 4291, 'root': 4292, 'intrigued': 4293, 'Way': 4294, 'notably': 4295, 'tour': 4296, 'Soviet': 4297, 'credibility': 4298, 'chances': 4299, 'Actually,': 4300, 'killed,': 4301, 'Man,': 4302, 'Indeed,': 4303, 'Bobby': 4304, 'noted': 4305, 'murderous': 4306, 'assistant': 4307, 'complain': 4308, 'devoid': 4309, 'properly': 4310, 'actress.': 4311, 'Che': 4312, 'Streisand': 4313, 'Mexico': 4314, 'Men': 4315, 'connect': 4316, 'Jake': 4317, 'king': 4318, 'Roberts': 4319, 'pool': 4320, 'novels': 4321, 'currently': 4322, 'hey,': 4323, 'Laura': 4324, 'Derek': 4325, 'Season': 4326, 'tricks': 4327, 'Quite': 4328, 'LOVE': 4329, 'innocence': 4330, 'hat': 4331, 'simple,': 4332, 'escaped': 4333, 'jail': 4334, 'titles': 4335, 'vague': 4336, '/>Also,': 4337, 'storytelling': 4338, 'realistic.': 4339, 'fond': 4340, 'country.': 4341, 'anyway': 4342, 'affected': 4343, 'colour': 4344, 'Ken': 4345, 'demands': 4346, 'delight': 4347, 'splendid': 4348, 'ad': 4349, 'con': 4350, 'Holmes': 4351, 'Moon': 4352, 'atrocious': 4353, 'slightest': 4354, 'burned': 4355, '100%': 4356, '2nd': 4357, 'relative': 4358, 'ghosts': 4359, 'confusion': 4360, 'describes': 4361, 'menacing': 4362, \"mother's\": 4363, 'reasonable': 4364, 'happens.': 4365, 'look.': 4366, 'summary': 4367, \"Here's\": 4368, 'Hoffman': 4369, 'beaten': 4370, 'beginning.': 4371, 'characters.<br': 4372, 'Later': 4373, 'movie)': 4374, 'Plus': 4375, 'ranks': 4376, 'contract': 4377, 'room,': 4378, 'sequences,': 4379, '/>Just': 4380, 'digital': 4381, 'special.': 4382, 'resemblance': 4383, 'Power': 4384, 'dozens': 4385, 'pack': 4386, 'reaches': 4387, 'dialogs': 4388, 'scripts': 4389, 'sequel,': 4390, 'songs,': 4391, 'selling': 4392, \"Hollywood's\": 4393, 'ON': 4394, 'Cold': 4395, 'guard': 4396, 'jokes.': 4397, 'Could': 4398, 'scientific': 4399, 'particular,': 4400, 'Gothic': 4401, 'occurs': 4402, 'watching,': 4403, 'pays': 4404, 'non': 4405, 'lives,': 4406, 'Chuck': 4407, 'elaborate': 4408, 'along.': 4409, 'afternoon': 4410, 'guest': 4411, 'peace': 4412, 'Brown': 4413, 'lasted': 4414, 'strikes': 4415, 'disappointing.': 4416, 'period.': 4417, 'stranger': 4418, 'gag': 4419, 'rating.': 4420, 'surprise,': 4421, 'California': 4422, 'dynamic': 4423, 'subplot': 4424, 'staying': 4425, 'consistently': 4426, 'urge': 4427, 'title.': 4428, 'also,': 4429, \"We're\": 4430, 'mixture': 4431, 'Next': 4432, 'annoying,': 4433, 'burn': 4434, 'drunken': 4435, 'Greek': 4436, 'funny.<br': 4437, 'guts': 4438, 'dedicated': 4439, 'Unless': 4440, 'Kenneth': 4441, 'originality': 4442, 'everyone,': 4443, 'view.': 4444, 'broad': 4445, 'Lincoln': 4446, 'involve': 4447, 'succeed': 4448, 'psychotic': 4449, 'evil,': 4450, 'whereas': 4451, 'vicious': 4452, 'work.<br': 4453, 'benefit': 4454, 'discuss': 4455, 'Add': 4456, 'OK.': 4457, 'AT': 4458, 'misses': 4459, 'focusing': 4460, 'B.': 4461, 'animation,': 4462, 'Barry': 4463, 'briefly': 4464, 'world.<br': 4465, 'wave': 4466, 'afford': 4467, 'spoilers': 4468, 'comfortable': 4469, \"girl's\": 4470, 'spoiled': 4471, 'truth.': 4472, 'producing': 4473, 'beats': 4474, 'Otherwise,': 4475, '17': 4476, 'presentation': 4477, 'Inspector': 4478, 'Grand': 4479, 'hunting': 4480, 'alone,': 4481, 'acceptable': 4482, 'Lloyd': 4483, 'environment': 4484, 'forever.': 4485, 'titled': 4486, 'stupidity': 4487, 'two,': 4488, 'highest': 4489, 'whole,': 4490, 'remarkably': 4491, 'look,': 4492, 'primarily': 4493, 'solely': 4494, 'travels': 4495, 'aside,': 4496, 'clues': 4497, 'productions': 4498, 'inevitable': 4499, '/>An': 4500, 'format': 4501, 'Doctor': 4502, 'America,': 4503, 'workers': 4504, 'worry': 4505, \"90's\": 4506, 'introduces': 4507, 'portion': 4508, 'trial': 4509, 'sits': 4510, 'tortured': 4511, 'attraction': 4512, 'Clint': 4513, 'Jesse': 4514, 'material,': 4515, '/>Director': 4516, 'explaining': 4517, 'fancy': 4518, 'enormous': 4519, 'influenced': 4520, 'all-time': 4521, \"characters'\": 4522, 'kicked': 4523, 'recognized': 4524, 'winner': 4525, 'settle': 4526, 'smaller': 4527, 'repeatedly': 4528, 'lessons': 4529, 'Check': 4530, 'soundtrack,': 4531, 'universe': 4532, 'friend.': 4533, 'Lucas': 4534, 'fine.': 4535, 'legs': 4536, 'insulting': 4537, 'represent': 4538, '/>*': 4539, '/>No': 4540, 'creators': 4541, 'acted,': 4542, \"(I'm\": 4543, 'drops': 4544, 'act,': 4545, 'network': 4546, 'Considering': 4547, 'hates': 4548, 'Princess': 4549, 'miscast': 4550, 'letter': 4551, 'Secret': 4552, 'incoherent': 4553, 'golden': 4554, 'nearby': 4555, 'ambitious': 4556, 'improved': 4557, 'upper': 4558, 'Navy': 4559, 'Sally': 4560, '8/10': 4561, 'fu': 4562, 'vast': 4563, 'severe': 4564, 'blatant': 4565, 'Arnold': 4566, 'threatening': 4567, 'table': 4568, 'amazing,': 4569, 'Pat': 4570, 'Would': 4571, 'country,': 4572, 'season.': 4573, 'chased': 4574, 'involvement': 4575, 'developing': 4576, 'Andrews': 4577, 'ANY': 4578, 'failing': 4579, 'course.': 4580, 'endearing': 4581, 'NOTHING': 4582, 'arrive': 4583, 'flaw': 4584, 'Worst': 4585, 'rip-off': 4586, 'strike': 4587, 'horrendous': 4588, 'obscure': 4589, 'morality': 4590, 'storyline.': 4591, 'Rogers': 4592, 'press': 4593, 'mild': 4594, 'situation.': 4595, 'flight': 4596, 'situations.': 4597, 'blah': 4598, 'queen': 4599, 'subtitles': 4600, 'wont': 4601, 'Acting': 4602, 'justify': 4603, 'weight': 4604, 'tad': 4605, 'discussion': 4606, '60': 4607, 'techniques': 4608, 'slap': 4609, 'worried': 4610, 'Dave': 4611, 'documentary.': 4612, 'Foster': 4613, 'hooked': 4614, 'bond': 4615, 'Nevertheless,': 4616, 'possibility': 4617, 'explicit': 4618, \"You're\": 4619, 'Hanks': 4620, 'racism': 4621, '/>Yes,': 4622, 'von': 4623, 'B-movie': 4624, 'blew': 4625, 'argue': 4626, '(an': 4627, 'grab': 4628, 'poster': 4629, 'Poor': 4630, 'rushed': 4631, 'burning': 4632, 'had.': 4633, 'chases': 4634, 'jealous': 4635, 'battles': 4636, 'Elvis': 4637, 'involved,': 4638, \"60's\": 4639, 'marks': 4640, 'exposed': 4641, 'Boll': 4642, 'okay,': 4643, 'sex.': 4644, 'fine,': 4645, 'logical': 4646, 'pace,': 4647, 'loads': 4648, 'view,': 4649, 'Ruth': 4650, 'convoluted': 4651, 'performers': 4652, 'threat': 4653, 'Jeffrey': 4654, 'photographed': 4655, 'pet': 4656, 'park': 4657, 'oddly': 4658, 'conventional': 4659, 'screams': 4660, 'trio': 4661, 'Curtis': 4662, 'aging': 4663, 'evil.': 4664, 'villain,': 4665, 'Fay': 4666, 'killed.': 4667, 'Baby': 4668, 'heroic': 4669, 'suspense.': 4670, 'Wild': 4671, 'spooky': 4672, 'writer,': 4673, 'composed': 4674, 'videos': 4675, 'Uwe': 4676, 'professor': 4677, 'bridge': 4678, 'nightmare': 4679, 'worst.': 4680, 'weakest': 4681, '/>Why': 4682, 'absence': 4683, 'brand': 4684, 'Felix': 4685, 'nine': 4686, '(i.e.': 4687, 'Carl': 4688, 'School': 4689, 'boxing': 4690, 'maintain': 4691, 'terror': 4692, 'many,': 4693, 'CIA': 4694, 'were,': 4695, '2)': 4696, 'waited': 4697, 'drawing': 4698, 'shy': 4699, 'Heston': 4700, 'era.': 4701, 'wicked': 4702, 'revelation': 4703, '18': 4704, 'quality,': 4705, '45': 4706, '\"This': 4707, 'defeat': 4708, 'defend': 4709, 'fears': 4710, 'ourselves': 4711, 'Brando': 4712, 'Real': 4713, 'Karloff': 4714, 'sadistic': 4715, 'newspaper': 4716, 'notion': 4717, 'useless': 4718, 'Comedy': 4719, 'experiment': 4720, 'angle': 4721, 'hunt': 4722, 'hints': 4723, 'parents,': 4724, 'charisma': 4725, 'estate': 4726, 'Lion': 4727, 'Family': 4728, 'suits': 4729, 'react': 4730, 'kid.': 4731, 'USA': 4732, 'Neither': 4733, 'Home': 4734, 'contained': 4735, 'passes': 4736, 'still,': 4737, 'writing.': 4738, 'suspects': 4739, 'emphasis': 4740, 'wore': 4741, 'released.': 4742, '3rd': 4743, 'orders': 4744, 'art,': 4745, 'hearts': 4746, 'month': 4747, 'screen.<br': 4748, 'interaction': 4749, 'times.<br': 4750, 'aliens': 4751, 'arrested': 4752, '3.': 4753, 'harder': 4754, 'International': 4755, 'viewers.': 4756, 'rap': 4757, 'Norman': 4758, 'documentary,': 4759, 'stole': 4760, 'different,': 4761, 'gift': 4762, 'montage': 4763, 'Grace': 4764, 'spoiler': 4765, 'guys.': 4766, 'card': 4767, 'closest': 4768, 'gem.': 4769, 'Return': 4770, 'painted': 4771, 'happiness': 4772, 'nose': 4773, 'sweet,': 4774, 'spin': 4775, \"we'll\": 4776, 'shortly': 4777, 'Spielberg': 4778, 'amounts': 4779, 'indeed,': 4780, 'outrageous': 4781, 'health': 4782, 'delivering': 4783, 'wished': 4784, 'glass': 4785, 'treats': 4786, 'horrors': 4787, 'winds': 4788, 'Hugh': 4789, 'Flynn': 4790, 'chair': 4791, 'exaggerated': 4792, 'anymore': 4793, 'shadow': 4794, 'soundtrack.': 4795, 'interest,': 4796, 'moving,': 4797, 'signs': 4798, 'suitable': 4799, 'this!': 4800, 'noble': 4801, 'intent': 4802, 'offering': 4803, 'movie...': 4804, 'song,': 4805, 'dinner': 4806, 'valuable': 4807, 'Sullivan': 4808, 'Girl': 4809, 'Sutherland': 4810, 'era,': 4811, 'Mother': 4812, 'ignored': 4813, 'thrillers': 4814, 'conspiracy': 4815, 'Carpenter': 4816, 'psychiatrist': 4817, 'Glenn': 4818, 'tales': 4819, 'EVER': 4820, 'note,': 4821, 'response': 4822, \"someone's\": 4823, 'problems.': 4824, 'nervous': 4825, 'darkness': 4826, 'tear': 4827, 'it;': 4828, '/>Most': 4829, 'engage': 4830, 'Alexander': 4831, 'hardcore': 4832, 'raped': 4833, 'fortune': 4834, 'mentioned,': 4835, 'Hope': 4836, 'spots': 4837, 'taught': 4838, '\"It\\'s': 4839, 'Margaret': 4840, 'Warren': 4841, 'Rather': 4842, 'switch': 4843, 'girlfriend,': 4844, 'rank': 4845, 'painting': 4846, 'watch.<br': 4847, 'thumbs': 4848, 'affect': 4849, '(from': 4850, 'Swedish': 4851, 'Hall': 4852, 'Orson': 4853, 'Carrey': 4854, 'ideal': 4855, '..': 4856, 'corporate': 4857, 'mirror': 4858, 'Boy': 4859, 'Nelson': 4860, 'resolution': 4861, 'shorts': 4862, 'relation': 4863, 'disagree': 4864, 'episodes.': 4865, 'Through': 4866, 'demon': 4867, 'link': 4868, 'behave': 4869, 'Mickey': 4870, '24': 4871, 'Apart': 4872, 'headed': 4873, 'situation,': 4874, 'can.': 4875, 'I,': 4876, 'points.': 4877, 'staring': 4878, 'Island': 4879, 'events,': 4880, 'compelled': 4881, 'word,': 4882, 'undoubtedly': 4883, 'you.<br': 4884, 'ticket': 4885, 'also.': 4886, 'from.': 4887, 'daring': 4888, 'business.': 4889, 'reads': 4890, 'silver': 4891, 'considerable': 4892, 'Thanks': 4893, 'depicts': 4894, 'set.': 4895, 'unbelievable.': 4896, 'cheap,': 4897, 'gentle': 4898, 'reflect': 4899, 'honor': 4900, 'writer/director': 4901, 'investigate': 4902, '/>Overall': 4903, 'worthless': 4904, 'engaged': 4905, 'investigation': 4906, 'room.': 4907, 'handed': 4908, 'strip': 4909, 'Widmark': 4910, 'Cole': 4911, \"everyone's\": 4912, 'Titanic': 4913, 'court': 4914, 'joined': 4915, 'bare': 4916, 'masterpiece,': 4917, 'alongside': 4918, 'overlooked': 4919, 'grave': 4920, 'displayed': 4921, 'goal': 4922, 'documentaries': 4923, 'Matthew': 4924, 'Throughout': 4925, \"Disney's\": 4926, 'Perry': 4927, 'melodramatic': 4928, 'next.': 4929, 'Up': 4930, 'individuals': 4931, 'Scottish': 4932, 'chance,': 4933, '80': 4934, 'Law': 4935, 'horses': 4936, 'Maggie': 4937, 'Hard': 4938, 'trite': 4939, '/>Like': 4940, 'crap,': 4941, 'overwhelming': 4942, 'earned': 4943, 'lame.': 4944, 'invisible': 4945, '/>Now,': 4946, 'walls': 4947, 'suck': 4948, 'thick': 4949, 'blows': 4950, 'cousin': 4951, 'destroying': 4952, 'Planet': 4953, 'yet.': 4954, 'exotic': 4955, 'size': 4956, 'magazine': 4957, 'dull.': 4958, '1/2': 4959, 'facing': 4960, 'isolated': 4961, 'Lake': 4962, 'Italy': 4963, 'web': 4964, 'everything,': 4965, 'tends': 4966, 'WWII': 4967, 'Cat': 4968, 'Seeing': 4969, 'one-liners': 4970, \"don't.\": 4971, 'brilliance': 4972, 'agrees': 4973, 'glorious': 4974, 'inspiring': 4975, 'cheating': 4976, 'development.': 4977, 'ABC': 4978, \"family's\": 4979, 'lighting,': 4980, 'less.': 4981, 'vivid': 4982, 'show.<br': 4983, 'thru': 4984, 'it:': 4985, 'China': 4986, 'secretly': 4987, 'nothing,': 4988, 'cabin': 4989, 'timeless': 4990, 'Lots': 4991, 'weapon': 4992, 'proceeds': 4993, 'Civil': 4994, '\"a': 4995, 'advise': 4996, 'Aside': 4997, 'bears': 4998, 'highlights': 4999, 'UNK': 5000}\n",
            "(5001, 5001)\n",
            "(5001, 5001)\n"
          ]
        }
      ],
      "source": [
        "vocab_5k, word_counts_5k = vocabulary(corpus, 0, 5000)\n",
        "M5dist = co_occurence_matrix(corpus, vocab_5k, window=5, distance_weighting=True)\n",
        "M20 = co_occurence_matrix(corpus, vocab_5k, window=20, distance_weighting=False)\n",
        "print(M5dist.shape)\n",
        "print(M20.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPYJlbSS550C",
        "outputId": "eb2ad599-db72-45b0-b112-be0cd7ceb9ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "721\n",
            "[3.590e+02 0.000e+00 2.339e+02 ... 2.000e-01 0.000e+00 0.000e+00]\n",
            "[4.355e+03 0.000e+00 2.262e+03 ... 2.000e+00 2.000e+00 0.000e+00]\n"
          ]
        }
      ],
      "source": [
        "print(vocab_5k['cinema'])\n",
        "print(M5dist[429])\n",
        "print(M20[429])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sutoX-GD550C"
      },
      "source": [
        "### Comparaison de vecteurs\n",
        "\n",
        "On peut se servir de ces vecteurs de très grande taille pour une analyse sémantique très basique: par exemple, en cherchant les plus proches voisins d'un mot. Cependant, il faudra faire attention aux distances qu'on utilise, liées à certaines métriques (Euclidiennes, Cosine) ou éventuellement d'autres liées à l'appartenance aux ensembles (Matching, Jaccard). La normalisation des vecteurs peut aussi jouer un rôle. Dans tous les cas, il faut bien faire attention à ne pas sur-interprêter ce type de résultats. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW31f41id0f2",
        "outputId": "c0544e49-a8ec-4eb0-8d08-8b3c5b1f9868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avec un contexte large, sans prendre en compte la distance entre les mots:\n",
            "Plus proches voisins de good selon la distance 'euclidean': \n",
            "[['very', 'what', 'some', 'more', 'even', 'when', 'there', 'out', 'story']]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-53-104c0f54c26a>:9: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return 1 - u.dot(v) / (np.sqrt(u.dot(u)) * np.sqrt(v.dot(v)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plus proches voisins de good selon la distance 'cosine': \n",
            "[['really', 'overall', 'very', 'decent', 'just', 'not', 'great', 'pretty', 'still']]\n",
            "\n",
            "Avec un contexte plus petit, et en réduisant l'impact des paires de mots selon leur distance:\n",
            "Plus proches voisins de good selon la distance 'euclidean': \n",
            "[['just', 'very', 'some', 'really', 'bad', 'even', 'great', 'well', 'what']]\n",
            "Plus proches voisins de good selon la distance 'cosine': \n",
            "[['great', 'well', 'cool', 'bad', 'also', 'decent', 'indeed', 'funny', 'just']]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-53-104c0f54c26a>:9: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return 1 - u.dot(v) / (np.sqrt(u.dot(u)) * np.sqrt(v.dot(v)))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def euclidean(u, v):\n",
        "    return np.linalg.norm(u - v)\n",
        "\n",
        "def length_norm(u):\n",
        "    return u / np.sqrt(u.dot(u))\n",
        "\n",
        "\n",
        "def cosine(u, v):\n",
        "    return 1 - u.dot(v) / (np.sqrt(u.dot(u)) * np.sqrt(v.dot(v)))\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def print_neighbors(distance, voc, co_oc, mot, k=10):\n",
        "    inv_voc = {id: w for w, id in voc.items()}\n",
        "    neigh = NearestNeighbors(algorithm='brute', metric=distance)\n",
        "    co_oc = np.nan_to_num(co_oc)\n",
        "    neigh.fit(co_oc) \n",
        "    dist, ind = neigh.kneighbors([co_oc[voc[mot]]], n_neighbors=k)\n",
        "    print(\"Plus proches voisins de %s selon la distance '%s': \" % (mot, distance.__name__))\n",
        "    print([[inv_voc[i] for i in s[1:]] for s in ind])\n",
        "    \n",
        "print(\"Avec un contexte large, sans prendre en compte la distance entre les mots:\")    \n",
        "print_neighbors(euclidean, vocab_5k, M20, 'good')\n",
        "print_neighbors(cosine, vocab_5k, M20, 'good')\n",
        "print(\"\")\n",
        "print(\"Avec un contexte plus petit, et en réduisant l'impact des paires de mots selon leur distance:\")    \n",
        "print_neighbors(euclidean, vocab_5k, M5dist, 'good')\n",
        "print_neighbors(cosine, vocab_5k, M5dist, 'good')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_tf1w81550C"
      },
      "source": [
        "**Normalisation**: Très simple; il s'agit d'annuler l'influence de la magnitude des comptes sur la représentation.\n",
        "\n",
        "$$\\mathbf{m_{normalized}} = \\left[ \n",
        "   \\frac{m_{1}}{\\sum_{i=1}^{n}m_{i}}, \n",
        "   \\frac{m_{2}}{\\sum_{i=1}^{n}m_{i}}, \n",
        "   \\ldots\n",
        "   \\frac{m_{n}}{\\sum_{i=1}^{n}m_{i}}, \n",
        "\\right]$$\n",
        " \n",
        "**Pointwise Mutual Information**: Il s'agit d'évaluer à quel point la co-occurence des deux termes est *inattendue*. En effet, cette mesure correspond au ratio de la probabilité jointe des deux mots et du produit de leur probabilités individuelles:\n",
        "$$\n",
        "\\text{PMI}(x,y) = \\log \\left( \\frac{P(x,y)}{P(x)P(y)} \\right)\n",
        "$$\n",
        "La probabilité jointe des deux mots correspond au nombre de fois ou on les observe ensemble, divisé par le nombre total de co-occurences du corpus: \n",
        "$$ P(\\mathbf{M},w_{1},w_{2}) = \\frac{M_{w_{1},w_{2}}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n",
        "La probabilité individuelle d'un mot correspond simplement à sa fréquence, que l'on peut calculer en comptant toutes les co-occurences ou ce mot apparaît:\n",
        "$$ P(\\mathbf{M},w) = \\frac{\\sum_{j=1}^{m} M_{w,j}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n",
        "Ainsi,\n",
        "$$ \n",
        "\\text{PMI}(\\mathbf{M},w_{1},w_{2}) = \\log  \\frac{M_{w_{1},w_{2}} \\times \\left( \\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j} \\right)}{\\left( \\sum_{j=1}^{n} M_{w_{1},j} \\right) \\times \\left( \\sum_{i=1}^{n}M_{i,w_{2}} \\right)} \n",
        "$$\n",
        "On calcule ainsi le décalage entre l'observation que l'on a fait dans notre corpus et la fréquence d'apparition de ces termes si on les considère indépendant - c'est à dire qu'on suppose que leur co-occurence est une coïncidence.\n",
        "\n",
        "Le principal problème avec cette mesure est qu'elle n'est pas adaptée au cas où l'on observe aucune co-occurence. Puisque la PMI est censée renvoyer une quantité positive si l'on observe plus de co-occurences que prévu, et négative si l'on en observe moins, on ne peut pas choisir de remplacer $\\log(0)$ par $0$. Une solution couramment utilisée est d'utiliser la **Positive PMI**, qui fixe toutes les valeurs négatives à $0$.\n",
        " \n",
        " $$\\text{PPMI}(\\mathbf{M},w_{1},w_{2}) = \n",
        " \\begin{cases}\n",
        " \\text{PMI}(\\mathbf{M},w_{1},w_{2}) & \\textrm{if } \\text{PMI}(\\mathbf{M},w_{1},w_{2}) > 0 \\\\\n",
        " 0 & \\textrm{otherwise}\n",
        " \\end{cases}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RM0E_z_n550C"
      },
      "outputs": [],
      "source": [
        "def pmi(co_oc, positive=True):\n",
        "    sum_vec = co_oc.sum(axis=0)\n",
        "    sum_tot = sum_vec.sum()\n",
        "    with np.errstate(divide='ignore'):\n",
        "        pmi = np.log((co_oc * sum_tot) / (np.outer(sum_vec, sum_vec)))                   \n",
        "    pmi[np.isnan(pmi)] = 0.0  # log(0) = 0\n",
        "    if positive:\n",
        "        pmi[pmi < 0] = 0.0\n",
        "    return pmi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYcLnrf5550C",
        "outputId": "34d3bfd0-5fbf-4739-dbe1-ea9f189b4e8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-54-0816105b646e>:5: RuntimeWarning: invalid value encountered in true_divide\n",
            "  pmi = np.log((co_oc * sum_tot) / (np.outer(sum_vec, sum_vec)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avec la PPMI:\n",
            "Plus proches voisins de good selon la distance 'euclidean': \n",
            "[['US', 'instance,', 'IMDb', 'Kim', 'IN', '/>My', 'version.', 'out.<br', 'gore,']]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-53-104c0f54c26a>:9: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return 1 - u.dot(v) / (np.sqrt(u.dot(u)) * np.sqrt(v.dot(v)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plus proches voisins de good selon la distance 'cosine': \n",
            "[['great', 'bad', 'decent', 'excellent', 'fine', 'nice', 'solid', 'superb', 'impressive']]\n",
            "Plus proches voisins de good selon la distance 'euclidean': \n",
            "[['but', 'out.<br', 'world.', '30', '(it', '/>My', '(including', 'guy.', 'la']]\n",
            "Plus proches voisins de good selon la distance 'cosine': \n",
            "[['great', 'decent', 'pretty', 'very', 'acting', 'overall', 'but', 'bad', 'excellent']]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-53-104c0f54c26a>:9: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return 1 - u.dot(v) / (np.sqrt(u.dot(u)) * np.sqrt(v.dot(v)))\n"
          ]
        }
      ],
      "source": [
        "PPMI5 = pmi(M5dist)\n",
        "PPMI20 = pmi(M20)\n",
        "\n",
        "print(\"Avec la PPMI:\")    \n",
        "print_neighbors(euclidean, vocab_5k, PPMI5, 'good')\n",
        "print_neighbors(cosine, vocab_5k, PPMI5, 'good')\n",
        "print_neighbors(euclidean, vocab_5k, PPMI20, 'good')\n",
        "print_neighbors(cosine, vocab_5k, PPMI20, 'good')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKBl5YMz550D"
      },
      "source": [
        "**TF-IDF**: Comme on l'a déjà vu, il s'agit du produit de la fréquence du terme (TF) et de sa fréquence inverse dans les documents (IDF). \n",
        "Cette méthode est habituellement utilisée pour extraire l'importance d'un terme $i$ dans un document $j$ relativement au reste du corpus, à partir d'une matrice $termes \\times documents$. Ainsi, pour une matrice $\\mathbf{X}$ de $n$ termes et $d$ documents: \n",
        "\n",
        " $$\\text{TF}(X, i, j) = \\frac{X_{i,j}}{\\sum_{i=1}^{t} X_{i,j}} $$\n",
        " \n",
        " $$\\text{IDF}(X, i) = \\log\\left(\\frac{d}{|\\{j : X_{i,j} > 0\\}|}\\right)$$\n",
        " \n",
        " $$\\text{TF-IDF}(X, i, j) = \\text{TF}(X, i, j) \\cdot \\text{IDF}(X, i)$$\n",
        "\n",
        "\n",
        "On peut l'adapter à notre cas en considérant que le contexte du deuxième mot est le document. Cependant, TF-IDF est généralement plus adaptée aux matrices peu denses, puisque cette mesure pénalisera les termes qui apparaissent dans une grande partie des documents. Ainsi, l'appliquer aux co-occurences des mots les plus fréquents n'est à priori pas optimal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQZmm5gf550D"
      },
      "outputs": [],
      "source": [
        "def tfidf(co_oc):\n",
        "    \"\"\"\n",
        "    Inverse document frequencies applied to our co_oc matrices\n",
        "    \"\"\"\n",
        "    # IDF\n",
        "    d = float(co_oc.shape[1])\n",
        "    in_doc = co_oc.astype(bool).sum(axis=1)\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        idfs = np.log(d / in_doc)\n",
        "    idfs[np.isnan(idfs)] = 0.0  # log(0) = 0\n",
        "    idfs[np.isinf(idfs)] = 0.0\n",
        "    print(idfs)\n",
        "    # TF\n",
        "    sum_vec = co_oc.sum(axis=0)\n",
        "    tfs = co_oc / sum_vec\n",
        "    return (tfs.T * idfs).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7rlxq49550D",
        "outputId": "205998b2-0e22-467c-bbc4-7464f7a5fcee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.47052366 0.         0.47052366 ... 2.46765972 2.51845661 0.        ]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-56-3b8a80f84b8a>:15: RuntimeWarning: invalid value encountered in true_divide\n",
            "  tfs = co_oc / sum_vec\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avec TF-IDF:\n",
            "Plus proches voisins de good selon la distance 'euclidean': \n",
            "[['great', 'bad', 'though', 'excellent', 'think', 'however', 'here', 'did', 'actually']]\n",
            "Plus proches voisins de good selon la distance 'cosine': \n",
            "[['great', 'but', 'and', 'the', 'bad', 'this', 'movie', 'its', 'there']]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-53-104c0f54c26a>:9: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return 1 - u.dot(v) / (np.sqrt(u.dot(u)) * np.sqrt(v.dot(v)))\n"
          ]
        }
      ],
      "source": [
        "TFIDF5 = tfidf(M5dist)\n",
        "\n",
        "print(\"Avec TF-IDF:\")    \n",
        "print_neighbors(euclidean, vocab_5k, TFIDF5, 'good')\n",
        "print_neighbors(cosine, vocab_5k, TFIDF5, 'good')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRlgiYvU550D"
      },
      "source": [
        "### Matrice de co-occurences : Réduction de dimension\n",
        "\n",
        "#### Motivation\n",
        "\n",
        "Il s'agit non seulement de réduire la taille de données (ainsi, on traitera des vecteurs de dimension réduite, plutôt que de travailler avec des vecteurs de la taille du vocabulaire) mais aussi de mettre en évidence des relations de plus haut niveau entre les mots: en réduisant leurs représentations aux dimensions qui *les plus importantes* des données, on se retrouve à *généraliser* certaines propriétés entre les mots.\n",
        "\n",
        "#### Réduction de dimension via SVD \n",
        "\n",
        "Une matrice est une transformation linéaire: y appliquer une SVD, c'est décomposer notre transformation linéaire en un produit de transformations linéaires de différents types. Il va s'agir d'effectuer un changement de base, et de replacer nos données dans un espace ou chacune des coordonnées sont inchangées par la transformation effectuée. Ainsi, on décompose la matrice $\\mathbf{M}$ en trois matrices:\n",
        "\n",
        "$$ \\mathbf{M} = \\mathbf{U} \\mathbf{\\lambda} \\mathbf{V}^{\\text{T}} $$\n",
        "\n",
        "Les matrices $\\mathbf{U}$, $\\mathbf{\\lambda}$, et $\\mathbf{V}$ ont les propriétés suivantes:\n",
        "- $\\mathbf{U}$ et $\\mathbf{V}$ sont des matrices orthogonales ($\\mathbf{U}^{\\text{T}} = \\mathbf{U}^{-1}$ et $\\mathbf{V}^{\\text{T}} = \\mathbf{V}^{-1}$). Elles contiennent les vecteurs propres à gauche et à droite de $\\mathbf{M}$.\n",
        "- $\\mathbf{\\lambda}$ est une matrice diagonale: attention, elle n'est pas forcément carrée. Les coefficients de la diagonale sont les valeurs propres de $\\mathbf{M}$.\n",
        "\n",
        "Ainsi, les dmensions *les plus importantes* correspondent aux plus grandes valeurs propres. Réduire nos données à une dimension $k$ correspond à ne garder que les vecteurs correspondant aux $k$ premières valeurs propres - et cela revient à prendre les $k$ premiers vecteurs de la matrice $U$. \n",
        "On utilise ici ```TruncatedSVD``` du package ```scikit-learn```:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfqAXS43550D",
        "outputId": "aa8e30c2-94b5-4e9f-e67e-6974184c9229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5001, 300)\n",
            "Plus proches voisins de good selon la distance 'euclidean': \n",
            "[['just', 'very', 'some', 'really', 'bad', 'even', 'great', 'well', 'what']]\n",
            "Plus proches voisins de good selon la distance 'cosine': \n",
            "[['great', 'well', 'cool', 'decent', 'pleasant', 'indeed', 'bad', 'also', 'nice']]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-53-104c0f54c26a>:9: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return 1 - u.dot(v) / (np.sqrt(u.dot(u)) * np.sqrt(v.dot(v)))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "svd = TruncatedSVD(n_components=300)\n",
        "SVDEmbeddings = svd.fit_transform(M5dist)\n",
        "print(SVDEmbeddings.shape)\n",
        "SVDEmbeddings[vocab_5k['UNK']]\n",
        "\n",
        "print_neighbors(euclidean, vocab_5k, SVDEmbeddings, 'good')\n",
        "print_neighbors(cosine, vocab_5k, SVDEmbeddings, 'good')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdcMrEpD550D"
      },
      "source": [
        "Note: Lorsque l'on applique cette méthode à la matrice des comptes $\\mathbf{M}$ de dimension $T \\times D$, où $\\mathbf{M}_{t,d}$ contient le nombre d'occurences du mot $t$ dans le document $d$, on obtient la méthode appellée **Latent Semantic Analysis**, pour la détection de composantes latentes (sémantiques) permettant de regrouper les documents.  \n",
        "\n",
        "#### Visualisation en deux dimensions\n",
        "\n",
        "On va maintenant utiliser **l'analyse en composantes principales** (PCA) pour visualiser nos données en 2 dimensions.  Cela revient à appliquer la SVD à la matrice de covariance des données, pour que les directions principales soient indépendantes les unes des autres et maximisent la variance des données.\n",
        "On utilise la classe ```PCA``` du package ```scikit-learn```: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "IlkzdB9Ef8HZ",
        "outputId": "91cdd4e9-5d78-46e8-de21-66a82afb7756"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# Create a PCA with 2 components\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Transform M5dist into Emb using fit_transform\n",
        "Emb = pca.fit_transform(M5dist)\n",
        "\n",
        "# Define the selected words\n",
        "words = ['bad', 'good', 'best', 'worst', 'poor', 'great', 'dialog', 'role', 'actor', 'camera', 'scene', 'film', 'movie', 'award']\n",
        "\n",
        "# Filter out words not in the vocab_5k dictionary\n",
        "ind_words = [vocab_5k[w] for w in words if w in vocab_5k]\n",
        "\n",
        "# Get the embeddings of selected words\n",
        "x_words = [Emb[ind, 0] for ind in ind_words]\n",
        "y_words = [Emb[ind, 1] for ind in ind_words]\n",
        "\n",
        "# Create a scatter plot of the embeddings\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x_words, y_words)\n",
        "\n",
        "# Annotate the plot with the selected words\n",
        "for i, w in enumerate(words):\n",
        "    if w in vocab_5k:\n",
        "        ax.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "KShrEBAc550E",
        "outputId": "34035f9f-1f31-49cb-b417-a3c87adf34fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-60-8ea66fbca11f>:1: RuntimeWarning: invalid value encountered in true_divide\n",
            "  Norm5 = M5dist / np.linalg.norm(M5dist, ord=2, axis=1, keepdims=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "oscar not in vocab_5k\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-8ea66fbca11f>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAGdCAYAAAAIbpn/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABIDklEQVR4nO3deVRV9f7/8ecBFZThIIiChuIEgfM8ppSW2C/TBi3Ta5pDeaM0tdTvzYGsbNCsWzcrK7WyLFNzjJsNmpKJE+aAKIhiSmGiIJggsH9/+PV8OyLmwDmw4fVY66zVnt+fHcvzOnv4fCyGYRiIiIiImJBLaRcgIiIicr0UZERERMS0FGRERETEtBRkRERExLQUZERERMS0FGRERETEtBRkRERExLQUZERERMS0KpV2AVdSWFjI8ePH8fLywmKxlHY5IiIichUMw+DMmTPUrl0bFxfHXjMp00Hm+PHjBAUFlXYZIiIich2OHj3KTTfd5NBjlOkg4+XlBVw4Ed7e3qVcjYiIiFyNrKwsgoKCbN/jjlSmg8zF20ne3t4KMiIiIibjjMdC9LCviIiImJaCjIiIiJiWgoyIiIiYloKMiJMFBwfz+uuvl3YZIiLlgoKMiIiImJaCjIiIiJiWgoxUWGfOnGHQoEF4eHgQGBjInDlziIiIYOzYsQCcOnWKIUOGUL16dapVq0bv3r05ePCg3T6WLl1KkyZNcHNzIzg4mNmzZ9stT09Pp0+fPlStWpX69euzaNEiZzVPRKRCUJCRCmvcuHHExsaycuVK1q1bx8aNG9mxY4dt+dChQ9m2bRsrV65k8+bNGIbBnXfeyfnz5wHYvn07AwYM4MEHH2T37t1Mnz6dKVOmsGDBArt9HD16lB9++IEvv/ySt99+m/T0dGc3VUSk/DLKsMzMTAMwMjMzS7sUMbn8gkLjp6Q/jK92/mr8lPSHcep0plG5cmVjyZIltnVOnz5tVKtWzRgzZoxx4MABAzBiY2Nty//44w+jatWqxhdffGEYhmE89NBDxu233253nKefftoIDw83DMMwEhMTDcCIi4uzLU9ISDAAY86cOQ5srYhI6XLm93eZ7tlXpCTE7EkjetU+0jLP2eZ5nz3G+fPnad++vW2e1WolNDQUgISEBCpVqkSHDh1sy/38/AgNDSUhIcG2Tt++fe2O1aVLF15//XUKCgps+2jTpo1t+c0334yPj48jmikiUiHp1pKUazF70hj9yQ67EANwMjsXgPWJv5dGWSIiUkIUZKTcKig0iF61D+Myy1ytAeBSiecXrKag8MIamZmZHDhwAICwsDDy8/PZsmWLbZuTJ0+SmJhIeHi4bZ3Y2Fi7/cbGxhISEoKrqys333wz+fn5bN++3bY8MTGR06dPl2xDRUQqMAUZKbfiUjKKXIm5yMWtGp5NbyNlzbvM/XQFe/fuZfjw4bi4uGCxWGjcuDF9+/Zl5MiRbNq0iV27djF48GDq1Klju500fvx4vvvuO2bMmMGBAwdYuHAhb731FhMmTAAgNDSUyMhIHn30UbZs2cL27dsZMWIEVatWddo5EBEp7xRkpNxKP3P5EHNR9dtGUKXOzUwY+RA9e/akS5cuhIWF4e7uDsD8+fNp06YNd911F506dcIwDNauXUvlypUBaN26NV988QWLFy+madOmTJ06leeee46hQ4fajjF//nxq165N9+7duffeexk1ahQ1a9Z0WJtFRCoai2EYl7vyXiZkZWVhtVrJzMzE29u7tMsRk9mcfJKB837+2/U+G9mRTg39yMnJoU6dOsyePZvhw4c7oUIRkfLJmd/fuiIj5Vb7+r4EWt2xFLM87/dkKh/+Cb/CU+zYsYNBgwYBFHkTSUREyi4FGSm3XF0sTOtz4cHcS8OMbfqXVbRu1ZKePXuSk5PDxo0bqVGjhjPLFBGRG6BbS1LuXa4fmUCrO9P6hBPZNLAUKxMRKZ+c+f2tDvGk3ItsGsjt4QHEpWSQfuYcNb3caV/fF1eX4m46iYiIWSjISIXg6mKhU0O/0i5DRERKmJ6REREREdNSkBERERHTUpARERER01KQEREREdNSkBERERHTUpARERER01KQEYdav349FouF06dPl3YpIiJSDinIyGXl5eWVdgkiIiJ/S0Gmgjhz5gyDBg3Cw8ODwMBA5syZQ0REBGPHjgUgODiYGTNmMGTIELy9vRk1ahQAmzZt4pZbbqFq1aoEBQXx5JNPkpOTY9vvxx9/TNu2bfHy8iIgIICHHnqI9PR0AA4fPsytt94KQPXq1bFYLAwdOtSp7RYRkfJNQaaCGDduHLGxsaxcuZJ169axceNGduzYYbfOrFmzaNGiBTt37mTKlCkkJycTGRnJfffdxy+//MLnn3/Opk2biIqKsm1z/vx5ZsyYwa5du/jqq684fPiwLawEBQWxdOlSABITE0lLS+ONN95wWptFRKT806CR5VRBoWEbW8jDcp4724Xw6aefcv/99wOQmZlJ7dq1GTlyJK+//jrBwcG0atWK5cuX2/YxYsQIXF1deffdd23zNm3aRPfu3cnJycHd3b3Icbdt20a7du04c+YMnp6erF+/nltvvZVTp07h4+Pj8HaLiEjp06CRckMuHe05L/0Q58+f56y1nm0dq9VKaGio3XZt27a1m961axe//PILixYtss0zDIPCwkJSUlIICwtj+/btTJ8+nV27dnHq1CkKCwsBSE1NJTw83FFNFBERARRkyp2YPWmM/mQHl7vM9q/le6gZeBORTQMvu62Hh4fddHZ2No8++ihPPvlkkXXr1q1LTk4OvXr1olevXixatAh/f39SU1Pp1auXHhYWERGnUJApRwoKDaJX7SsSYipZA8ClErlpB4leVZfbwwPIPpPFgQMH6NatW7H7a926Nfv27aNRo0aXXb57925OnjzJSy+9RFBQEHDh1tJfValS5UJtBQXX3zAREZFi6GHfciQuJcN2O+mvXNyq4dn0Nk798CEpv8Sx+L8/MXz4cFxcXLBYLMXub+LEifz0009ERUURHx/PwYMHWbFihe1h37p161KlShXefPNNDh06xMqVK5kxY4bdPurVq4fFYmH16tWcOHGC7Ozskm20iIhUaAoy5Uj6maIh5qLqt42gSp2bSV8azRND7qVLly6EhYVd9oHdi5o3b86GDRs4cOAAt9xyC61atWLq1KnUrl0bAH9/fxYsWMCSJUsIDw/npZdeYtasWXb7qFOnDtHR0UyaNIlatWrZvfEkIiJyoxz61tKPP/7Iq6++yvbt20lLS2P58uX069fvqrfXW0vXZnPySQbO+/lv1/tsZEeaB7hTp04dZs+ezfDhw51QnYiIVBTO/P526BWZnJwcWrRowX/+8x9HHkb+V/v6vgRa3bnczaK835PJ2beB6vkZVDp1mEGDBgHQt29f5xYpIiJSghz6sG/v3r3p3bu3Iw8hf+HqYmFan3BGf7IDCxR56Dcrbhn7v/0PvT50o02bNmzcuJEaNWqURqkiIiIlQs/IlDORTQOZO7g1AVb7Z1/qhTRhxbcb+fNsDhkZGaxbt45mzZqVUpUiIiIlo0y9fp2bm0tubq5tOisrqxSrMa/IpoHcHh5g69m3ppc77ev74upS/BtKIiIiZlSmgszMmTOJjo4u7TLKBVcXC50a+pV2GSIiIg5Vpm4tTZ48mczMTNvn6NGjpV2SiIiIlGFlKsi4ubnh7e1t9xGRK4uIiGDs2LElus/169djsVg4ffp0ie5XRKSkOfTWUnZ2NklJSbbplJQU4uPj8fX1pW7duo48tIiIiFQADg0y27Zt49Zbb7VNjxs3DoCHH36YBQsWOPLQIiIiUgE49NZSREQEhmEU+SjEiJSs/Px8oqKisFqt1KhRgylTpnCx0+6PP/6Ytm3b4uXlRUBAAA899BDp6el2269du5aQkBCqVq3KrbfeyuHDh0uhFSIi165MPSMjItdn4cKFVKpUibi4ON544w1ee+013n//fQDOnz/PjBkz2LVrF1999RWHDx9m6NChtm2PHj3KvffeS58+fYiPj2fEiBFMmjSplFoiInJtHDrW0o3SWEsify8iIoL09HT27t1rG8180qRJrFy5kn379hVZf9u2bbRr144zZ87g6enJ//zP/7BixQr27t1rW2fSpEm8/PLLnDp1Ch8fH2c1RUTKiXIz1pKIlLyCQoPNySdZEX+MzcknMYCOHTvaQgxAp06dOHjwIAUFBWzfvp0+ffpQt25dvLy86N69OwCpqakAJCQk0KFDB7tjdOrUyWntERG5EWWqQzwRubKYPWlEr9pHWuY527yM1FO4VT972fXPnTtHr1696NWrF4sWLcLf35/U1FR69epFXl6es8oWEXEYXZERMYmYPWmM/mSHXYgByMsvZP3GzcTsSbPN+/nnn2ncuDH79+/n5MmTvPTSS9xyyy3cfPPNRR70DQsLIy4uzm7ezz//7LiGiIiUIAUZERMoKDSIXrWvyIjmF+WfOcGwx55gX8J+PvvsM958803GjBlD3bp1qVKlCm+++SaHDh1i5cqVzJgxw27bxx57jIMHD/L000+TmJjIp59+qjcLRcQ0FGRETCAuJaPIlZi/8mhyG9k5Z2nfvj2PP/44Y8aMYdSoUfj7+7NgwQKWLFlCeHg4L730ErNmzbLbtm7duixdupSvvvqKFi1a8M477/Diiy86ukkiIiVCby2JmMCK+GOMWRz/t+u98WBL+ras4/iCRESuQG8tiYidml7uJbqeiEh5oSAjYgLt6/sSaHXHUsxyCxBodad9fV9nliUiUuoUZERMwNXFwrQ+4QBFwszF6Wl9wnF1KS7qiIiUTwoyIiYR2TSQuYNbE2C1v30UYHVn7uDWRDYNLKXKRERKjzrEEzGRyKaB3B4eQFxKBulnzlHT68LtJF2JEZGKSkFGxGRcXSx0auhX2mWIiJQJurUkIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalIFNG5eXllXYJIiIiZZ6CzFWIiIggKiqKqKgorFYrNWrUYMqUKRiGAcCpU6cYMmQI1atXp1q1avTu3ZuDBw/a7WPp0qU0adIENzc3goODmT17tt3y4OBgZsyYwZAhQ/D29mbUqFFOa5+IiIhZKchcpYULF1KpUiXi4uJ44403eO2113j//fcBGDp0KNu2bWPlypVs3rwZwzC48847OX/+PADbt29nwIABPPjgg+zevZvp06czZcoUFixYYHeMWbNm0aJFC3bu3MmUKVOc3UQRERHTsRgXLyuUQVlZWVitVjIzM/H29i61OiIiIkhPT2fv3r1YLBYAJk2axMqVK1mxYgUhISHExsbSuXNnAE6ePElQUBALFy6kf//+DBo0iBMnTvDNN9/Y9vnMM8+wZs0a9u7dC1y4ItOqVSuWL1/u/AaKiIiUIGd+f+uKzGUUFBpsTj7JivhjbE4+iQF07NjRFmIAOnXqxMGDB9m3bx+VKlWiQ4cOtmV+fn6EhoaSkJAAQEJCAl26dLE7RpcuXTh48CAFBQW2eW3btnVsw0RERMqZSqVdQFkTsyeN6FX7SMs8Z5uXkXoKt+pnHX5sDw8Phx9DRESkPNEVmb+I2ZPG6E922IUYgLz8QtZv3EzMnjTbvJ9//pnGjRsTHh5Ofn4+W7ZssS07efIkiYmJhIeHAxAWFkZsbKzdPmNjYwkJCcHV1dWBLRIRESnfFGT+V0GhQfSqfRT3wFD+mRMMe+wJ9iXs57PPPuPNN99kzJgxNG7cmL59+zJy5Eg2bdrErl27GDx4MHXq1KFv374AjB8/nu+++44ZM2Zw4MABFi5cyFtvvcWECROc10AREZFySLeW/ldcSkaRKzF/5dHkNrJzztK+fXuqVK7EmDFjbK9Iz58/nzFjxnDXXXeRl5dHt27dWLt2LZUrVwagdevWfPHFF0ydOpUZM2YQGBjIc889x9ChQ53RNBERkXJLQeZ/pZ8pPsQAWFxc8e05mjfmz6Nvyzp2y6pXr85HH310xe3vu+8+7rvvvmKXHz58+KprFRERkQuccmvpP//5D8HBwbi7u9OhQwfi4uKccdhrUtPLvUTXExEREcdzeJD5/PPPGTduHNOmTWPHjh20aNGCXr16kZ6e7uhDX5P29X0JtLpjucI6gVZ32tf3dVpNZjR9+nRatmxZ2mWIiEgF4fAg89prrzFy5EiGDRtGeHg477zzDtWqVePDDz909KGviauLhWl9LrxldGmYCXzoJfx6jmJan3BcXa4UdaSkaKwpERG5Gg4NMnl5eWzfvp2ePXv+3wFdXOjZsyebN28usn5ubi5ZWVl2H2eKbBrI3MGtCbDa3z4KsLozd3BrIpsGOrWe0hITE0PXrl3x8fHBz8+Pu+66i+TkZNvyX3/9lYEDB+Lr64uHhwdt27Zly5YtLFiwgOjoaHbt2oXFYsFisdiGYUhNTaVv3754enri7e3NgAED+P333237vHgl5/3336d+/fq4u+sWnoiI/D2HPuz7xx9/UFBQQK1atezm16pVi/379xdZf+bMmURHRzuypL8V2TSQ28MDiEvJIP3MOWp6XbidVJGuxOTk5DBu3DiaN29OdnY2U6dO5Z577iE+Pp6zZ8/SvXt36tSpw8qVKwkICGDHjh0UFhbywAMPsGfPHmJiYvj2228BsFqtFBYW2kLMhg0byM/P5/HHH+eBBx5g/fr1tuMmJSWxdOlSli1bpv51RETkqpSpt5YmT57MuHHjbNNZWVkEBQU5vQ5XFwudGvo5/bhlxaVvV3344Yf4+/uzb98+fvrpJ06cOMHWrVvx9b3wvFCjRo1s63p6elKpUiUCAgJs89atW8fu3btJSUmx/f/86KOPaNKkCVu3bqVdu3bAhSt4H330Ef7+/o5uooiIlBMOvbVUo0YNXF1d7W4hAPz+++92X3QXubm54e3tbfcRx7p0XKmCQoODBw8ycOBAGjRogLe3N8HBwcCF20Px8fG0atXKFmKuRkJCAkFBQXahNDw8HB8fH9t4VAD16tVTiBERkWvi0CsyVapUoU2bNnz33Xf069cPgMLCQr777juioqIceWi5CpcbVyrQ6s6v7z1KeEhD5s2bR+3atSksLKRp06bk5eVRtWpVh9WjsaZERORaOfytpXHjxjFv3jwWLlxIQkICo0ePJicnh2HDhjn60HIFxY0rdey3dH49nMztD42mR48ehIWFcerUKdvy5s2bEx8fT0ZGxmX3W6VKFbsRveHCWFNHjx7l6NGjtnn79u3j9OnTtvGoRERErofDg8wDDzzArFmzmDp1Ki1btiQ+Pp6YmJgiDwCL81xpXCmLuycuVb15+fX/kHjgIN9//73dc0sDBw4kICCAfv36ERsby6FDh1i6dKntLbTg4GBSUlKIj4/njz/+IDc3l549e9KsWTMGDRrEjh07iIuLY8iQIXTv3p22bds6qdUiIlIeOaVn36ioKI4cOUJubi5btmyhQ4cOzjisFONK40pZLC7UuPsZMo8m0rxZM5566ileffVV2/IqVarwzTffULNmTe68806aNWvGSy+9ZHvL6L777iMyMpJbb70Vf39/PvvsMywWCytWrKB69ep069aNnj170qBBAz7//HOntFdERMovi2EYxQ34XOqysrKwWq1kZmbqwd8StCL+GGMWx//tem882LLIuFIiIiJ/x5nf3065IiNli8aVEhGR8kJBpgL6u3GlLGhcKRERMQcFmQroSuNKXZzWuFIiImIGCjIVlMaVEhGR8qBMDVEgzqVxpURExOwUZCq4ij6ulIiImJtuLYmIiIhpKciIiIiIaSnIiIiIiGkpyIiIiIhpKciIiIiIaSnIiIiIiGkpyIiIiIhpKciIiIiIaSnIiIiIiGkpyIiIiIhpKciIiIiIaSnIiIiIiGkpyIiIiIhpKciIiIiIaSnIiIiIiGkpyIiIiIhpKciIiIiIaSnIiIiIiGkpyIiIiIhpKciIiIiIaSnIFCMiIoKxY8cCEBwczOuvv37V2y5YsAAfHx+H1CUiIiL/p1JpF2AGW7duxcPDo7TLEBERkUsoyFwFf3//0i5BRERELkO3loCcnByGDBmCp6cngYGBzJ492275pbeWXnvtNZo1a4aHhwdBQUH885//JDs7+4rHmDt3Lg0bNqRKlSqEhoby8ccf2y3fv38/Xbt2xd3dnfDwcL799lssFgtfffVVSTVTRETEFPLy8q56XQUZ4Omnn2bDhg2sWLGCb775hvXr17Njx45i13dxceHf//43e/fuZeHChXz//fc888wzxa6/fPlyxowZw/jx49mzZw+PPvoow4YN44cffgCgoKCAfv36Ua1aNbZs2cJ7773Hv/71rxJvp4iISHFyc3N58sknqVmzJu7u7nTt2pWtW7cCcOrUKQYNGoS/vz9Vq1alcePGzJ8/37btr7/+ysCBA/H19cXDw4Pu3bvbliUnJ9O3b19q1aqFp6cn7dq149tvv7U7dnBwMDNmzGDIkCF4e3szatSoqy/cKMMyMzMNwMjMzHTYMc6cOWNUqVLF+OKLL2zzTp48aVStWtUYM2aMYRiGUa9ePWPOnDnF7mPJkiWGn5+fbXr+/PmG1Wq1TXfu3NkYOXKk3Tb9+/c37rzzTsMwDOPrr782KlWqZKSlpdmWr1u3zgCM5cuXX3/jRERErtKTTz5p1K5d21i7dq2xd+9e4+GHHzaqV69unDx50nj88ceNli1bGlu3bjVSUlKMdevWGStXrjQM48L3aIMGDYxbbrnF2Lhxo3Hw4EFj/vz5tu/v+Ph445133jF2795tHDhwwHj22WcNd3d348iRI7Zj16tXz/D29jZmzZplJCUlGUlJSVddd4V8Rqag0CAuJYP0M+fI/DWJvLw8OnToYFvu6+tLaGhosdt/++23zJw5k/3795OVlUV+fj7nzp3j7NmzVKtWrcj6CQkJRdJlly5deOONNwBITEwkKCiIgIAA2/L27dvfaDNFRESuSk5ODnPnzmXBggX07t0bgHnz5rFu3To++OADUlNTadWqFW3btgUuXEG56NNPP+XEiRNs3boVX19fAGrWrMmwYcMAaNGiBS1atLCtP2PGDJYvX87KlSuJioqyzb/tttsYP378Ndde4YJMzJ40olftIy3zHAB56YcAWJ/4O0Pq1v3b7Q8fPsxdd93F6NGjeeGFF/D19WXTpk0MHz6cvLy8ywYZERGRsuavP+qzjiVz/vx5unTpYlteuXJl2rdvT0JCAqNHj+a+++5jx44d3HHHHfTr14/OnTsDEB8fT6tWrWwh5lLZ2dlMnz6dNWvWkJaWRn5+Pn/++Sepqal2610MSdeqQj0jE7MnjdGf7LCFGIBKPoHgUokJ/1lGzJ404MK9wAMHDlx2H9u3b6ewsJDZs2fTsWNHQkJCOH78+BWPGxYWRmxsrN282NhYwsPDAQgNDeXo0aP8/vvvtuUX70uKiIiUtJg9aXR9+XsGzvuZMYvjmbj0FwDWJ6Zfdv3evXtz5MgRnnrqKY4fP06PHj2YMGECAFWrVr3isSZMmMDy5ct58cUX2bhxI/Hx8TRr1qzIA73X281JhQkyBYUG0av2YVwy36VKVTyb307GDx8y7vVF7PplN0OHDsXF5fKnplGjRpw/f54333yTQ4cO8fHHH/POO+9c8dhPP/00CxYsYO7cuRw8eJDXXnuNZcuW2f4Ibr/9dho2bMjDDz/ML7/8QmxsLM8++ywAFovlhtsuIiJyUbE/6l0rMeGtL2w/6s+fP8/WrVttP7r9/f15+OGH+eSTT3j99dd57733AGjevDnx8fFkZGRc9nixsbEMHTqUe+65h2bNmhEQEMDhw4dLrD0VJsjEpWTY/U/7q+q3PoJ7UBP2f/Qst/XoSdeuXWnTps1l123RogWvvfYaL7/8Mk2bNmXRokXMnDnzisfu168fb7zxBrNmzaJJkya8++67zJ8/n4iICABcXV356quvyM7Opl27dowYMcL21pK7u/v1N1pEROQviv9R745Xyzs59cOHPDV7Abv37GXkyJGcPXuW4cOHM3XqVFasWEFSUhJ79+5l9erVhIWFATBw4EACAgLo168fsbGxHDp0iBUrVtj23bhxY5YtW0Z8fDy7du3ioYceorCwsMTaZDEM49L2lBlZWVlYrVYyMzPx9va+oX2tiD/GmMXxf7veGw+2pG/LOjd0rJIQGxtL165dSUpKomHDhqVdjoiUY9OnT+err74iPj6+tEsRB9ucfJKB836+7DIjP49T6+eTs28DlQpyadeuLXPmzKFdu3Y8//zzfPrppxw+fJiqVatyyy23MGfOHOrXrw/AkSNHGD9+POvWrSM/P5/Q0FB27txJZmYmGRkZPPLII/z888/UqFGDiRMnsmTJElq2bGnroy04OJixY8fahga6FhUmyFzpf95ffTayI50a+t3Qsa7H8uXL8fT0pHHjxiQlJTFmzBiqV6/Opk2bnF6LiFQs2dnZ5Obm4ufn/H/7xLmc9aO+JL+//06FeWupfX1fAq3u/JZ5rsglNQALEGB1p339yz917Whnzpxh4sSJpKamUqNGDXr27Fmkh2EREUfw9PTE09OztMsQJ6jpdXWPK1ztemVBhXlGxtXFwrQ+Fx5YuvTx2YvT0/qE4+pSOg/XDhkyhAMHDnDu3Dl+/fVXFixYoF9HIhVQREQETzzxBGPHjqV69erUqlWLefPmkZOTw7Bhw/Dy8qJRo0Z8/fXXtm02bNhA+/btcXNzIzAwkEmTJpGfnw/Ae++9R+3atYs8k9C3b18eeeQR4MKtpZYtW9otf//99wkLC8Pd3Z2bb76Zt99+27ENF6e4+KO+uG86CxBYij/qr0eFCTIAkU0DmTu4NQFW+6QZYHVn7uDWRDYNLKXKRET+z8KFC6lRowZxcXE88cQTjB49mv79+9O5c2dbPx7/+Mc/OHv2LMeOHePOO++kXbt27Nq1i7lz5/LBBx/w/PPPA9C/f39OnjxpGxIFICMjg5iYGAYNGnTZ4y9atIipU6fywgsvkJCQwIsvvsiUKVNYuHChU9ovjlPWf9RfjwrzjMxf/bUToJpeF5Knmf6niUj5FRERQUFBARs3bgQujMVmtVq59957+eijjwD47bffCAwMZPPmzaxatYqlS5eSkJBg667h7bffZuLEiWRmZuLi4kK/fv3w8/Pjgw8+AC5cpYmOjubo0aO4uLgUedi3UaNGzJgxg4EDB9rqev7551m7di0//fSTE8+GOMqlncPChSsx0/qEl8iPej0j42CuLpZSeaBXRORSl/6wMrjQL8dFrq6u+Pn50axZM9u8WrVqAZCenk5CQgKdOnWy63OqS5cuZGdn8+uvv1K3bl0GDRrEyJEjefvtt3Fzc2PRokU8+OCDl+0vKycnh+TkZIYPH87IkSNt8/Pz87FarQ44A1IaIpsGcnt4QLn4UV8hg4yISFlwuV/FGamnqB5k3+OpxWKhcuXKdtPAVffF0adPHwzDYM2aNbRr146NGzcyZ86cy66bnZ0NXBhn569j0MGFUCXlR3n5Ua8gIyJSCi72rnrpvf28/EK+T0gnZk/aVV3iDwsLY+nSpRiGYQs4sbGxeHl5cdNNNwEXOta89957WbRoEUlJSYSGhtK6devL7q9WrVrUrl2bQ4cOFfsMjUhZoiAjIuJkxfWu+lfRq/Zxe3jA317q/+c//8nrr7/OE088QVRUFImJiUybNo1x48bZ3ToaNGgQd911F3v37mXw4MFX3Gd0dDRPPvkkVquVyMhIcnNz2bZtG6dOnWLcuHHX0lQRh1OQERFxsisNmXJRWuY54lIy/vbSf506dVi7di1PP/00LVq0wNfXl+HDh9vGa7votttuw9fXl8TERB566KEr7nPEiBFUq1aNV199laeffhoPDw+aNWt2Xb2uijhahXxrSUSkNJltyBSRa+XM7+8K1Y+MiEhZUB57VxUpLQoyIiJOVh57VxUpLQoyIiJOVh57VxUpLQoyIiKlQEOmiJQMvbUkIlJKylPvqiKlRUFGRKQUlZfeVUVKi24tiYiIiGkpyIiIiIhpKciIiIiIaSnIiIiIiGkpyIiIiIhpKciIiIiIaSnIiIiIiGkpyIiIiIhpKciIiIiIaSnIiIiIiGkpyIiIiIhpKciIiIiIaSnIiIiIiGkpyIiIiIhpOSzIvPDCC3Tu3Jlq1arh4+PjqMOIiIhIBeawIJOXl0f//v0ZPXq0ow4hIiIiFVwlR+04OjoagAULFjjqECIiIlLBOSzIXI/c3Fxyc3Nt01lZWaVYjYiIiJR1Zeph35kzZ2K1Wm2foKCg0i5JREREyrBrCjKTJk3CYrFc8bN///7rLmby5MlkZmbaPkePHr3ufYmIiEj5d023lsaPH8/QoUOvuE6DBg2uuxg3Nzfc3Nyue3sRERGpWK4pyPj7++Pv7++oWkRERESuicMe9k1NTSUjI4PU1FQKCgqIj48HoFGjRnh6ejrqsCIiIlKBOCzITJ06lYULF9qmW7VqBcAPP/xARESEow4rIiIiFYjFMAyjtIsoTlZWFlarlczMTLy9vUu7HBEREbkKzvz+LlOvX4u5REREMHbs2NIuQ0REKjAFGRERETEtBRm5rLy8vNIuQURE5G8pyAhw4TZRVFQUY8eOpUaNGvTq1YsNGzbQvn173NzcCAwMZNKkSeTn5xe7j9zcXCZMmECdOnXw8PCgQ4cOrF+/3nmNEBGRCkdBRmwWLlxIlSpViI2NZfr06dx55520a9eOXbt2MXfuXD744AOef/75YrePiopi8+bNLF68mF9++YX+/fsTGRnJwYMHndgKERGpSMrUoJFSuho3bswrr7wCwEcffURQUBBvvfUWFouFm2++mePHjzNx4kSmTp2Ki4t9Bk5NTWX+/PmkpqZSu3ZtACZMmEBMTAzz58/nxRdfdHp7RESk/FOQqaAKCg3iUjJIP3OOml7uGECbNm1syxMSEujUqRMWi8U2r0uXLmRnZ/Prr79St25du/3t3r2bgoICQkJC7Obn5ubi5+fn0LaIiEjFpSBTAcXsSSN61T7SMs/Z5mWknqL6DQw2np2djaurK9u3b8fV1dVumXpyFhERR9EzMhVMzJ40Rn+ywy7EAOTlF/J9Qjoxe9IACAsLY/Pmzfy1v8TY2Fi8vLy46aabiuy3VatWFBQUkJ6eTqNGjew+AQEBjm1UOWAYBqNGjcLX1xeLxYKPj49dHz3BwcG8/vrrpVafiEhZpSBTgRQUGkSv2seVunKOXrWPgkKDf/7znxw9epQnnniC/fv3s2LFCqZNm8a4ceOKPB8DEBISwqBBgxgyZAjLli0jJSWFuLg4Zs6cyZo1axzXqHIiJiaGBQsWsHr1atLS0jhw4AAzZswo7bJERMo83VqqQOJSMopciblUWuY54lIy6NSwDmvXruXpp5+mRYsW+Pr6Mnz4cJ599tlit50/fz7PP/8848eP59ixY9SoUYOOHTty1113lXRTyp3k5GQCAwPp3LlzaZciImIqCjIVSPqZ4kNMwEMvFVmve/fuxMXFFbvNpX3EVK5cmejoaKKjo2+s0Apm6NChtgFWLRYL9erVIzg4mJYtWxZ7O8lisfDOO++watUqvv/+e+rVq8eHH36Iv78/I0aMYOvWrbRo0YKPP/6Yhg0bOrE1IiLOpVtLFUhNL/cSXU9KxhtvvMFzzz3HTTfdRFpaGlu3br2q7WbMmMGQIUOIj4/n5ptv5qGHHuLRRx9l8uTJbNu2DcMwiIqKcnD1IiKlS0GmAmlf35dAqzuWYpZbgECrO+3r+zqzrArParXi5eWFq6srAQEB+Pv7X9V2w4YNY8CAAYSEhDBx4kQOHz7MoEGD6NWrF2FhYYwZM0Y9K4tIuacgU4G4uliY1iccoEiYuTg9rU84ri7FRR0pKQWFBpuTT7Ii/hibk09SaFzpEezLa968ue2/a9WqBUCzZs3s5p07d46srKwbL1hEpIzSMzIVTGTTQOYObl2kH5kAqzvT+oQT2TSwFKurGC7Xjw97DvLn+YJr2k/lypVt/32x48LLzSssLLyBakVEyjYFmQoosmkgt4cH2PXs276+r67EOMHFfnwuvf6S9Wc+Wdl5xOxJU5gUEbkGCjIVlKuLhU4NNXSAM11tPz63h6sDQRGRq6VnZESc5Fr68RERkaujKzIiTnKlfny82/XFu11f23qXvm10+PBhu2njkoeDg4ODi8yLiIgoMk9EpLzRFRkRJ1E/PiIiJU9BRsRJ1I+PiEjJU5ARcRL14yMiUvIUZESc6GI/PgFW+9tHAVZ35g5urVevRUSukR72FXEy9eMjIlJyFGRESoH68RERKRm6tSQiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqalICPlUl5eXmmXICIiTqAgI9essLCQV155hUaNGuHm5kbdunV54YUXAJg4cSIhISFUq1aNBg0aMGXKFM6fP2/bdvr06bRs2ZIPP/yQunXr4unpyT//+U8KCgp45ZVXCAgIoGbNmrb9XXT69GlGjBiBv78/3t7e3HbbbezatavIft9//33q16+Pu7s7ADExMXTt2hUfHx/8/Py46667SE5OdsJZEhERZ6hU2gWI+UyePJl58+YxZ84cunbtSlpaGvv37wfAy8uLBQsWULt2bXbv3s3IkSPx8vLimWeesW2fnJzM119/TUxMDMnJydx///0cOnSIkJAQNmzYwE8//cQjjzxCz5496dChAwD9+/enatWqfP3111itVt5991169OjBgQMH8PX1BSApKYmlS5eybNkyXF1dAcjJyWHcuHE0b96c7Oxspk6dyj333EN8fDwuLsrxIiKmZ5RhmZmZBmBkZmaWdinyv7Kysgw3Nzdj3rx5V7X+q6++arRp08Y2PW3aNKNatWpGVlaWbV6vXr2M4OBgo6CgwDYvNDTUmDlzpmEYhrFx40bD29vbOHfunN2+GzZsaLz77ru2/VauXNlIT0+/Yj0nTpwwAGP37t1XVb+IiFw7Z35/64qMXFFBoUFcSgbpZ85R08sdTiSRm5tLjx49Lrv+559/zr///W+Sk5PJzs4mPz8fb29vu3WCg4Px8vKyTdeqVQtXV1e7KyS1atUiPT0dgF27dpGdnY2fn5/dfv7880+720T16tXD39/fbp2DBw8ydepUtmzZwh9//EFhYSEAqampNG3a9DrOiIiIlCUKMlKsmD1pRK/aR1rmOds865/Hi11/8+bNDBo0iOjoaHr16oXVamXx4sXMnj3bbr3KlSvbTVsslsvOuxg6srOzCQwMZP369UWO6ePjY/tvDw+PIsv79OlDvXr1mDdvHrVr16awsJCmTZvqYWARkXJCQUYuK2ZPGqM/2YFxyfzTlWtgqeTGGx8t5fVpE+yW/fTTT9SrV49//etftnlHjhy54Vpat27Nb7/9RqVKlQgODr7q7U6ePEliYiLz5s3jlltuAWDTpk03XI+IiJQdCjJSREGhQfSqfUVCDIClUhWsHe7jrZefo2U9f265pSsnTpxg7969NG7cmNTUVBYvXky7du1Ys2YNy5cvv+F6evbsSadOnejXrx+vvPIKISEhHD9+nDVr1nDPPffQtm3by25XvXp1/Pz8eO+99wgMDCQ1NZVJkybdcD0iIlJ26LUNKSIuJcPudtKlvLs8iFfbfkx+dgphYWE88MADpKenc/fdd/PUU08RFRVFy5Yt+emnn5gyZcoN12OxWFi7di3dunVj2LBhhISE8OCDD3LkyBFq1apV7HYuLi4sXryY7du307RpU5566ileffXVG65HRETKDothGJf74V0mZGVlYbVayczMLPLAqDjOivhjjFkc/7frvfFgS/q2rOP4gkRExFSc+f2tKzJSRE0v9xJdT0RExFEUZKSI9vV9CbS6YylmuQUItLrTvr6vM8sSEREpQkFGinB1sTCtTzhAkTBzcXpan3BcXYqLOiIiIs6hICOXFdk0kLmDWxNgtb99FGB1Z+7g1kQ2DSylykRERP6PXr+WYkU2DeT28AC7nn3b1/fVlRgRESkzdEVGrsjVxUKnhn70bVmHTg39LhtivvzyS5o1a0bVqlXx8/OjZ8+e5OTkAPDhhx/SpEkT3NzcCAwMJCoqyrbd1Y5o/fHHHxMcHIzVauXBBx/kzJkztnUKCwuZOXMm9evXp2rVqrRo0YIvv/zSgWdERETKEgUZuSFpaWkMHDiQRx55hISEBNavX8+9996LYRjMnTuXxx9/nFGjRrF7925WrlxJo0aNbNv279+f9PR0vv76a7Zv307r1q3p0aMHGRkZtnWSk5P56quvWL16NatXr2bDhg289NJLtuUzZ87ko48+4p133mHv3r089dRTDB48mA0bNjj1PIiISOlQPzJyQ3bs2EGbNm04fPgw9erVs1tWp04dhg0bxvPPP19ku02bNvH//t//Iz09HTc3N9v8Ro0a8cwzzzBq1CimT5/Oq6++ym+//WYbZPKZZ57hxx9/5OeffyY3NxdfX1++/fZbOnXqZNvHiBEjOHv2LJ9++qmDWi0iIlfizO9vPSMj1+yvI2L7ed7EbT160KxZM3r16sUdd9zB/fffz/nz5zl+/Hixo2Rf7YjWl46UHRgYaBsVOykpibNnz3L77bfb7SMvL49WrVqVVHNFRKQMU5CRa3K5EbEDbpvEtIdzyErawZtvvsm//vUvvvvuuyvu52pHtP67UbEB1qxZQ5069j0M//Uqj4iIlF8Oe0bm8OHDDB8+3PYQZsOGDZk2bRp5eXmOOqQ42MURsS8dh+n3rFze2luJTv0fY+fOnVSpUoV169YRHBxcbKD564jWjRo1svvUqFHjquoJDw/Hzc2N1NTUIvsICgq64faKiEjZ57ArMvv376ewsJB3332XRo0asWfPHkaOHElOTg6zZs1y1GHFQYobETv3eCLnjuyianAr/ufjTDJbV+LEiROEhYUxffp0HnvsMWrWrEnv3r05c+YMsbGxPPHEE9c9ovVfeXl5MWHCBJ566ikKCwvp2rUrmZmZxMbG4u3tzcMPP+yYkyEiImWGw4JMZGQkkZGRtukGDRqQmJjI3LlzFWRMqLgRsV2qVOPc0T1kbVtBWu5Znqlbl9mzZ9O7d28Azp07x5w5c5gwYQI1atTg/vvvB/5vROt//etfDBs2jBMnThAQEEC3bt2uOKL1pWbMmIG/vz8zZ87k0KFD+Pj40Lp1a/7nf/6nZBouIiJlmlPfWnr22WeJiYlh27Ztl12em5tLbm6ubTorK4ugoCC9tVQGaERsERG5WuVy9OukpCTefPNNHn300WLXmTlzJlar1fbRcw5lh0bEFhGRsuiag8ykSZOwWCxX/Ozfv99um2PHjhEZGUn//v0ZOXJksfuePHkymZmZts/Ro0evvUXiEBoRW0REyqJrvrV04sQJTp48ecV1GjRoQJUqVQA4fvw4ERERdOzYkQULFuDicvXZSR3ilS0X31oC7B76vRhuNJikiIiAc7+/HfqMzLFjx7j11ltp06YNn3zyCa6urte0vYJM2XO5fmQCre5M6xOuECMiIkA5CTLHjh0jIiKCevXqsXDhQrsQExAQcFX7UJApm/7as69GxBYRkUuViyEK1q1bR1JSEklJSdx00012y8rw8E5yFS6OiC0iIlLaHPbW0tChQzEM47IfERERkZLgtNevRUREREqagoyIiIiYloKMiIiImJaCjIiIiJiWgoyIiIiYloKMiIiImJaCjIiIiJiWgoyIiIiYloKMiIiImJaCjIiIiJiWgoyIiIiYloKMiIiImJaCjIiIiJiWgoyIiIiYloKMiIiImJaCjIiIiJiWgoyIiIiYloKMiIiImJaCjIiIiJiWgoyIiIiYloKMiIiImJaCjIiIiJiWgow4VEREBGPHji3tMkREpJxSkBHTWr9+PRaLhdOnT5d2KSIiUkoUZERERMS0FGTE4fLz84mKisJqtVKjRg2mTJmCYRgA5ObmMmHCBOrUqYOHhwcdOnRg/fr1tm2PHDlCnz59qF69Oh4eHjRp0oS1a9dy+PBhbr31VgCqV6+OxWJh6NChpdA6EREpTZVKuwAp/xYuXMjw4cOJi4tj27ZtjBo1irp16zJy5EiioqLYt28fixcvpnbt2ixfvpzIyEh2795N48aNefzxx8nLy+PHH3/Ew8ODffv24enpSVBQEEuXLuW+++4jMTERb29vqlatWtpNFRERJ1OQEYcLCgpizpw5WCwWQkND2b17N3PmzKFXr17Mnz+f1NRUateuDcCECROIiYlh/vz5vPjii6SmpnLffffRrFkzABo0aGDbr6+vLwA1a9bEx8fH6e0SEZHSpyAjJaqg0CAuJYP0M+eo6eWOAXTs2BGLxWJbp1OnTsyePZvdu3dTUFBASEiI3T5yc3Px8/MD4Mknn2T06NF888039OzZk/vuu4/mzZs7s0kiIlKGKchIiYnZk0b0qn2kZZ6zzctIPYVb9bOXXT87OxtXV1e2b9+Oq6ur3TJPT08ARowYQa9evVizZg3ffPMNM2fOZPbs2TzxxBOOa4iIiJiGHvaVEhGzJ43Rn+ywCzEAefmFrN+4mZg9abZ5P//8M40bN6ZVq1YUFBSQnp5Oo0aN7D4BAQG29YOCgnjsscdYtmwZ48ePZ968eQBUqVIFgIKCAie0UEREyiIFGblhBYUG0av2YRSzPP/MCYY99gT7Evbz2Wef8eabbzJmzBhCQkIYNGgQQ4YMYdmyZaSkpBAXF8fMmTNZs2YNAGPHjuW///0vKSkp7Nixgx9++IGwsDAA6tWrh8ViYfXq1Zw4cYLs7GwntVhERMoKBRm5YXEpGUWuxPyVR5PbyM45S/v27Xn88ccZM2YMo0aNAmD+/PkMGTKE8ePHExoaSr9+/di6dSt169YFLlxtefzxxwkLCyMyMpKQkBDefvttAOrUqUN0dDSTJk2iVq1aREVFOb6xIiJSpliMix16lEFZWVlYrVYyMzPx9vYu7XKkGCvijzFmcfzfrvfGgy3p27KO4wsSEZFS5czvb12RkRtW08u9RNcTERG5WgoycsPa1/cl0OqOpZjlFiDQ6k77+r7OLEtERCoABRm5Ya4uFqb1CQcoEmYuTk/rE46rS3FRR0RE5PooyEiJiGwayNzBrQmw2t8+CrC6M3dwayKbBpZSZSIiUp6pQzwpMZFNA7k9PMCuZ9/29X11JUZERBxGQUZKlKuLhU4N/Uq7DBERqSB0a0lERERMS0FGRERETEtBRkRERExLQUZERERMS0FGRERETEtBRkRERExLQUZERERMS0FGRERETEtBRkRERExLQUZESo3FYuGrr74q7TJExMQUZETEIfLy8kq7BBGpABRkRCqo1atX4+PjQ0FBAQDx8fFYLBYmTZpkW2fEiBEMHjwYgKVLl9KkSRPc3NwIDg5m9uzZdvsLDg5mxowZDBkyBG9vb0aNGkVeXh5RUVEEBgbi7u5OvXr1mDlzpm19gHvuuQeLxWKbFhG5FgoyIhXULbfcwpkzZ9i5cycAGzZsoEaNGqxfv962zoYNG4iIiGD79u0MGDCABx98kN27dzN9+nSmTJnCggUL7PY5a9YsWrRowc6dO5kyZQr//ve/WblyJV988QWJiYksWrTIFli2bt0KwPz580lLS7NNi4hcC41+LVJBFBQaxKVkkH7mHDW93Glf35eWLVuyfv162rZty/r163nqqaeIjo4mOzubzMxMkpKS6N69O9OnT6dHjx5MmTIFgJCQEPbt28err77K0KFDbce47bbbGD9+vG06NTWVxo0b07VrVywWC/Xq1bMt8/f3B8DHx4eAgADnnAQRKXd0RUakAojZk0bXl79n4LyfGbM4noHzfqbry99Tr8mFAGMYBhs3buTee+8lLCyMTZs2sWHDBmrXrk3jxo1JSEigS5cudvvs0qULBw8etN2aAmjbtq3dOkOHDiU+Pp7Q0FCefPJJvvnmG6e0V0QqDl2RESnnYvakMfqTHRiXzP8t8xyHcmqR8+Nidu3aReXKlbn55puJiIhg/fr1nDp1iu7du1/TsTw8POymW7duTUpKCl9//TXffvstAwYMoGfPnnz55Zc32CoRkQt0RUakHCsoNIheta9IiAEwALegJuRkZ/Paa3NsoeVikFm/fj0REREAhIWFERsba7d9bGwsISEhuLq6XrEGb29vHnjgAebNm8fnn3/O0qVLycjIAKBy5cp2V3RERK6VrsiIlGNxKRmkZZ4rdrmLuyeV/YP59NNFvPXWWwB069aNAQMGcP78eVu4GT9+PO3atWPGjBk88MADbN68mbfeeou33377isd/7bXXCAwMpFWrVri4uLBkyRICAgLw8fEBLry59N1339GlSxfc3NyoXr16yTRcRCoMXZERKcfSzxQfYi5yD2pKQUGB7eqLr68v4eHhBAQEEBoaCly4RfTFF1+wePFimjZtytSpU3nuuefsHvS9HC8vL1555RXatm1Lu3btOHz4MGvXrsXF5cI/PbNnz2bdunUEBQXRqlWrG2qriFRMFsMwLnfVuUzIysrCarWSmZmJt7d3aZcjYjqbk08ycN7Pf7veZyM70qmhnxMqEpGKwJnf37oiI1KOta/vS6DVHUsxyy1AoPXCq9giImakICNSjrm6WJjWJxygSJi5OD2tTziuLsVFHRGRsk1BRqSci2wayNzBrQmwutvND7C6M3dwayKbBpZSZSIiN05vLYlUAJFNA7k9PKBIz766EiMiZqcgI1JBuLpY9ECviJQ7urUkIiIipqUgIyIiIqalICMiIiKmpSAjIiIipqUgIyIiIqbl0CBz9913U7duXdzd3QkMDOQf//gHx48fd+QhRUREpAJxaJC59dZb+eKLL0hMTGTp0qUkJydz//33O/KQIiIiUoE4ddDIlStX0q9fP3Jzc6lcufLfrq9BI0VERMzHmd/fTusQLyMjg0WLFtG5c+diQ0xubi65ubm26aysLGeVJyIiIibk8CAzceJE3nrrLc6ePUvHjh1ZvXp1sevOnDmT6OjoIvMVaERERMzj4ve2M276XPOtpUmTJvHyyy9fcZ2EhARuvvlmAP744w8yMjI4cuQI0dHRWK1WVq9ejcVSdIyXS6/IHDt2jPDw8GspT0RERMqIo0ePctNNNzn0GNccZE6cOMHJkyevuE6DBg2oUqVKkfm//vorQUFB/PTTT3Tq1Olvj1VYWEhiYiLh4eEcPXpUz8k4UFZWFkFBQTrPTqBz7Tw6186jc+08ZjjXhmFw5swZateujYuLY3t6ueZbS/7+/vj7+1/XwQoLCwHsrrpciYuLC3Xq1AHA29u7zP4PK090np1H59p5dK6dR+faecr6ubZarU45jsOekdmyZQtbt26la9euVK9eneTkZKZMmULDhg2v6mqMiIiIyN9x2PWeatWqsWzZMnr06EFoaCjDhw+nefPmbNiwATc3N0cdVkRERCoQh12RadasGd9///0N78fNzY1p06Yp/DiYzrPz6Fw7j8618+hcO4/OtT2ndognIiIiUpI0aKSIiIiYloKMiIiImJaCjIiIiJiWgoyIiIiYVpkLMi+88AKdO3emWrVq+Pj4XNU2hmEwdepUAgMDqVq1Kj179uTgwYOOLbQcyMjIYNCgQXh7e+Pj48Pw4cPJzs6+4jbJycncc889+Pv74+3tzYABA/j999+dVLF5Xc+5/u233/jHP/5BQEAAHh4etG7dmqVLlzqpYvO61nN9+PBhLBbLZT9LlixxYuXmcz1/1wCbN2/mtttuw8PDA29vb7p168aff/7phIrN6XrOc0RERJG/58cee8xJFTtXmQsyeXl59O/fn9GjR1/1Nq+88gr//ve/eeedd9iyZQseHh706tWLc+fOObBS8xs0aBB79+5l3bp1rF69mh9//JFRo0YVu35OTg533HEHFouF77//ntjYWPLy8ujTp4+t12a5vGs91wBDhgwhMTGRlStXsnv3bu69914GDBjAzp07nVS1OV3ruQ4KCiItLc3uEx0djaenJ71793Zi5eZzPX/XmzdvJjIykjvuuIO4uDi2bt1KVFSUw7uxN7PrOc8AI0eOtPu7fuWVV5xQbSkwyqj58+cbVqv1b9crLCw0AgICjFdffdU27/Tp04abm5vx2WefObBCc9u3b58BGFu3brXN+/rrrw2LxWIcO3bsstv897//NVxcXIzMzEzbvNOnTxsWi8VYt26dw2s2q+s514ZhGB4eHsZHH31kN8/X19eYN2+ew2o1u+s915dq2bKl8cgjjziixHLjes91hw4djGeffdYZJZYL13ueu3fvbowZM8YJFZY+00fglJQUfvvtN3r27GmbZ7Va6dChA5s3by7Fysq2zZs34+PjQ9u2bW3zevbsiYuLC1u2bLnsNrm5uVgsFrtOmNzd3XFxcWHTpk0Or9msrudcA3Tu3JnPP/+cjIwMCgsLWbx4MefOnSMiIsIJVZvT9Z7rv9q+fTvx8fEMHz7cUWWWC9dzrtPT09myZQs1a9akc+fO1KpVi+7du+vfjyu4kb/pRYsWUaNGDZo2bcrkyZM5e/aso8stFaYPMr/99hsAtWrVsptfq1Yt2zIp6rfffqNmzZp28ypVqoSvr2+x561jx454eHgwceJEzp49S05ODhMmTKCgoIC0tDRnlG1K13OuAb744gvOnz+Pn58fbm5uPProoyxfvpxGjRo5umTTut5z/VcffPABYWFhdO7c2REllhvXc64PHToEwPTp0xk5ciQxMTG0bt2aHj166LnGYlzv3/RDDz3EJ598wg8//MDkyZP5+OOPGTx4sKPLLRVOCTKTJk0q9mG6i5/9+/c7o5Ryz5Hn2t/fnyVLlrBq1So8PT2xWq2cPn2a1q1bV8j7247+u54yZQqnT5/m22+/Zdu2bYwbN44BAwawe/fuEmyFOTjr35A///yTTz/9tEJfjXHkub74LN2jjz7KsGHDaNWqFXPmzCE0NJQPP/ywJJtR5jn6b3rUqFH06tWLZs2aMWjQID766COWL19OcnJyCbaibHDYWEt/NX78eIYOHXrFdRo0aHBd+w4ICADg999/JzAw0Db/999/p2XLlte1TzO72nMdEBBAenq63fz8/HwyMjJs5/Ry7rjjDpKTk/njjz+oVKkSPj4+BAQEXPf/PzNz5LlOTk7mrbfeYs+ePTRp0gSAFi1asHHjRv7zn//wzjvvlEgbzMLRf9cXffnll5w9e5YhQ4bcSLmm5shzffHf6PDwcLv5YWFhpKamXn/RJuSsv+mLOnToAEBSUhINGza85nrLMqcEGX9/f/z9/R2y7/r16xMQEMB3331nCy5ZWVls2bLlmt58Ki+u9lx36tSJ06dPs337dtq0aQPA999/T2Fhoe0P/kpq1Khh2yY9PZ277777xgo3IUee64v3si+90uXq6loh3xBz1t/1Bx98wN133+2wf6/MwJHnOjg4mNq1a5OYmGg3/8CBAxXuDTFn/U1fFB8fD2D3g7/cKO2njS915MgRY+fOnUZ0dLTh6elp7Ny509i5c6dx5swZ2zqhoaHGsmXLbNMvvfSS4ePjY6xYscL45ZdfjL59+xr169c3/vzzz9JogmlERkYarVq1MrZs2WJs2rTJaNy4sTFw4EDb8l9//dUIDQ01tmzZYpv34YcfGps3bzaSkpKMjz/+2PD19TXGjRtXGuWbyrWe67y8PKNRo0bGLbfcYmzZssVISkoyZs2aZVgsFmPNmjWl1QxTuJ6/a8MwjIMHDxoWi8X4+uuvnV2yaV3PuZ4zZ47h7e1tLFmyxDh48KDx7LPPGu7u7kZSUlJpNMEUrvU8JyUlGc8995yxbds2IyUlxVixYoXRoEEDo1u3bqXVBIcqc0Hm4YcfNoAinx9++MG2DmDMnz/fNl1YWGhMmTLFqFWrluHm5mb06NHDSExMdH7xJnPy5Elj4MCBhqenp+Ht7W0MGzbMLjCmpKQUOfcTJ040atWqZVSuXNlo3LixMXv2bKOwsLAUqjeX6znXBw4cMO69916jZs2aRrVq1YzmzZsXeR1birqec20YhjF58mQjKCjIKCgocHLF5nW953rmzJnGTTfdZFSrVs3o1KmTsXHjRidXbi7Xep5TU1ONbt26Gb6+voabm5vRqFEj4+mnn7brOqM8sRiGYZTSxSARERGRG1LxXjURERGRckNBRkRERExLQUZERERMS0FGRERETEtBRkRERExLQUZERERMS0FGRERETEtBRkRERExLQUZERERMS0FGRERETEtBRkRERExLQUZERERM6/8DwoCJpjRXlVwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "Norm5 = M5dist / np.linalg.norm(M5dist, ord=2, axis=1, keepdims=True)\n",
        "\n",
        "pca = PCA(n_components=2, whiten=True)\n",
        "Norm5 = np.nan_to_num(Norm5)\n",
        "Emb = pca.fit_transform(Norm5)\n",
        "\n",
        "words = ['bad', 'good', 'best', 'worst', 'poor', 'great',         'dialog', 'role', 'actor', 'camera', 'scene',         'film', 'movie', 'oscar', 'award']\n",
        "\n",
        "ind_words = []\n",
        "for w in words:\n",
        "    if w in vocab_5k:\n",
        "        ind_words.append(vocab_5k[w])\n",
        "    else:\n",
        "        print(f\"{w} not in vocab_5k\")\n",
        "\n",
        "x_words = [Emb[ind,0] for ind in ind_words]\n",
        "y_words = [Emb[ind,1] for ind in ind_words]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x_words, y_words)\n",
        "\n",
        "for i, w in enumerate(words):\n",
        "    ax.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te0KcWlp550E"
      },
      "source": [
        "## Obtenir une représentation: algorithmes couramment utilisés\n",
        "\n",
        "L'idée, ici, est de définir un ensemble de représentations ${w_{i}}_{i=1}^{V}$, de dimension prédéfinie $d$ (ici, on travaillera avec $d = 300$), pour tous les mots $i$ du vocabulaire $V$ - puis **d'entraîner** ces représentations pour qu'elles correspondent à ce que l'on souhaite. \n",
        "\n",
        "### Glove\n",
        "\n",
        "L'objectif défini par Glove ([Pennington et al. (2014)](http://www.aclweb.org/anthology/D/D14/D14-1162.pdf)) est d'apprendre des vecteurs $w_{i}$ et $w_{k}$ de façon à ce que leur produit scalaire correspondent au logarithme de leur **Pointwise Mutual Information**: \n",
        "\n",
        "\n",
        "$$ w_{i}^\\top w_{k} = (PMI(w_{i}, w_{k}))$$\n",
        "\n",
        "\n",
        "Dans l'article, l'obtention de cet objectif est minutieusement justifié par un raisonnement sur les opérations que l'on veut effectuer avec ces vecteurs et les propriétés qu'ils devraient avoir - notamment, une symétrie entre les lignes et les colonnes (voir l'article pour plus de détails).  \n",
        "L'objectif final obtenu est le suivant, où $M$ est la matrice de co-occurences:\n",
        "\n",
        "\n",
        "$$\\sum_{i, j=1}^{|V|} f\\left(M_{ij}\\right)\n",
        "  \\left(w_i^\\top w_j + b_i + b_j - \\log M_{ij}\\right)^2$$\n",
        "  \n",
        " \n",
        "Ici, $f$ est une fonction de *mise à l'échelle* qui permet de diminuer l'importance des comptes de co-occurences les plus fréquents: \n",
        "\n",
        "\n",
        "$$f(x) \n",
        "\\begin{cases}\n",
        "(x/x_{\\max})^{\\alpha} & \\textrm{if } x < x_{\\max} \\\\\n",
        "1 & \\textrm{otherwise}\n",
        "\\end{cases}$$\n",
        "\n",
        "\n",
        "En général, on choisit $\\alpha=0.75$ et $x_{\\max} = 100$, même si ces paramètres peuvent nécessiter un changement selon les données."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJT4OYPU550E"
      },
      "source": [
        "Le code suivant utilise l'API de gensim pour récupérer des représentations pré-entrainées (Il est normal que le chargement soit long)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsJKmo9O550E"
      },
      "source": [
        "!pip install gensim "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNQtcQqz550E",
        "outputId": "9df5a6f2-e389-4a90-8ef3-6f96a62dcad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "loaded_glove_model = api.load(\"glove-wiki-gigaword-300\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOcC-EoZ550E"
      },
      "source": [
        "On peut extraire la matrice des embeddings ainsi, et vérifier sa taille:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9bth552550E",
        "outputId": "36919697-cd59-43b2-8dc1-ea1a4d6cb71a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(400000, 300)\n"
          ]
        }
      ],
      "source": [
        "loaded_glove_embeddings = loaded_glove_model.vectors\n",
        "print(loaded_glove_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tSsFspd550F"
      },
      "source": [
        "On voit donc qu'il y a $400.000$ mots représentés, et que les embeddings sont de dimension $300$. On définit une fonction qui nous renvoie, à partir du modèle chargé, le vocabulaire et la matrice des embeddings suivant les structures que l'on a utilisé auparavant. On ajoute, ici encore, un mot inconnu ```'UNK'``` au cas où se trouve dans nos données des mots qui ne font pas parti des $400.000$ mots représentés ici. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbeUd3az550F"
      },
      "outputs": [],
      "source": [
        "def get_glove_voc_and_embeddings(glove_model):\n",
        "    voc = {word : index for word, index in enumerate(glove_model.index_to_key)}\n",
        "    voc['UNK'] = len(voc)\n",
        "    embeddings = glove_model.vectors\n",
        "    return voc, embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngD8B-3P550F"
      },
      "outputs": [],
      "source": [
        "loaded_glove_voc, loaded_glove_embeddings = get_glove_voc_and_embeddings(loaded_glove_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qDgeIsa550F"
      },
      "source": [
        "Afin de comparer 'à jeu égal' les représentations chargées ici et celles que l'on a produite, il faudrait utiliser le même vocabulaire. Dans ce but, je réutilise le code qui suit pour créer un vocabulaire de $5000$ mots à partir des données exactement comme hier, et j'ajoute à la fin une fonction qui renvoie la matrices des représentations chargées avec Glove pour ces $5000$ mots seulement, dans le bon ordre. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5_FPEJxmUlj"
      },
      "outputs": [],
      "source": [
        "def get_glove_adapted_embeddings(glove_model, vocab):\n",
        "    # Get the dimensionality of the loaded GloVe model\n",
        "    vector_size = glove_model.vector_size\n",
        "\n",
        "    # Initialize an empty matrix to store the embeddings\n",
        "    embeddings = np.zeros((len(vocab), vector_size))\n",
        "\n",
        "    # Loop over the words in the vocabulary\n",
        "    for word, idx in vocab.items():\n",
        "        if word in glove_model:\n",
        "            # If the word is in the GloVe model, use its embedding\n",
        "            embeddings[idx] = glove_model.get_vecattr(word, \"vector\")\n",
        "        else:\n",
        "            # If the word is not in the GloVe model, randomly initialize its embedding\n",
        "            embeddings[idx] = np.random.normal(scale=0.6, size=(vector_size,))\n",
        "\n",
        "    return embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHntoBuhmrz_"
      },
      "outputs": [],
      "source": [
        "def get_glove_adapted_embeddings(glove_model, word_to_index):\n",
        "    embedding_size = glove_model.vector_size\n",
        "    vocab_size = len(word_to_index)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
        "\n",
        "    for word, idx in word_to_index.items():\n",
        "        try:\n",
        "            embedding_matrix[idx] = glove_model.get_vector(word)\n",
        "        except KeyError:\n",
        "            # Handle the case where the word is not in the vocabulary\n",
        "            pass\n",
        "    return embedding_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unOiEguD550F"
      },
      "outputs": [],
      "source": [
        "GloveEmbeddings = get_glove_adapted_embeddings(loaded_glove_model, vocab_5k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Y3ch4d550F"
      },
      "source": [
        "Cette fonction prend donc en entrée le modèle chargé à l'aide de l'API Gensim, ainsi qu'un vocabulaire que nous avons créé nous même, et renvoie la matrice d'embeddings tiré du modèle chargé, pour les mots notre vocabulaire et dans le bon ordre.\n",
        "Remarque: les mots inconnus sont représentés par le vecteur nul:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx99pmN9550F",
        "outputId": "4bc22f6b-0026-4c10-e9ac-89de1f5d65ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5001, 300)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(GloveEmbeddings.shape)\n",
        "GloveEmbeddings[vocab_5k['UNK']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A83Ddw4J550F",
        "outputId": "11af4086-bbbe-40b5-b9a4-912b96367255"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5001, 300)"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "GloveEmbeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB_ru20q550G",
        "outputId": "381ea480-fd2f-47a9-b4ac-eb2d9c98e539"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5001, 5001)"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "M5dist.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YZxV53s550G",
        "outputId": "2c0275a8-5fbb-43d5-88af-dea2e70c1758"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plus proches voisins de good selon la distance 'euclidean': \n",
            "[['better', 'well', 'always', 'really', 'sure', 'way', 'so', 'but', 'excellent']]\n",
            "Plus proches voisins de good selon la distance 'cosine': \n",
            "[['better', 'really', 'always', 'you', 'well', 'excellent', 'very', 'things', 'think']]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-53-104c0f54c26a>:9: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return 1 - u.dot(v) / (np.sqrt(u.dot(u)) * np.sqrt(v.dot(v)))\n"
          ]
        }
      ],
      "source": [
        "print_neighbors(euclidean, vocab_5k, GloveEmbeddings, 'good')\n",
        "print_neighbors(cosine, vocab_5k, GloveEmbeddings, 'good')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw2oGUC2550G"
      },
      "source": [
        "## Application à l'analyse de sentiments\n",
        "\n",
        "On va maintenant utiliser ces représentations pour l'analyse de sentiments. \n",
        "Le modèle de base, comme hier, sera construit en deux étapes:\n",
        "- Une fonction permettant d'obtenir des représentations vectorielles des critiques, à partir des textes, du vocabulaire, et des représentations vectorielles des mots. Une telle fonction (à compléter ci-dessous) va associer à chaque mot d'une critique son embeddings, et créer la représentation pour l'ensemble de la phrase en sommant ces embeddings.\n",
        "- Un classifieur qui prendra ces représentations en entrée et réalisera une prédiction. Pour le réaliser, on pourra utiliser d'abord la régression logistique ```LogisticRegression``` de ```scikit-learn```  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RAZ-l1g550G"
      },
      "outputs": [],
      "source": [
        "def sentence_representations(texts, vocabulary, embeddings, np_func=np.sum):\n",
        "    \"\"\"\n",
        "    Represent the sentences as a combination of the vector of its words.\n",
        "    Parameters\n",
        "    ----------\n",
        "    texts : a list of sentences   \n",
        "    vocabulary : dict\n",
        "        From words to indexes of vector.\n",
        "    embeddings : Matrix containing word representations\n",
        "    np_func : function (default: np.sum)\n",
        "        A numpy matrix operation that can be applied columnwise, \n",
        "        like `np.mean`, `np.sum`, or `np.prod`. \n",
        "    Returns\n",
        "    -------\n",
        "    np.array, dimension `(len(texts), embeddings.shape[1])`            \n",
        "    \"\"\"\n",
        "    representations = np.zeros((len(texts), embeddings.shape[1]))\n",
        "    for idx, text in enumerate(texts):\n",
        "        words_indexes = [vocabulary[word] for word in text.split() if word in vocabulary]\n",
        "        representations[idx,:] = np_func(embeddings[words_indexes,:], axis = 0)\n",
        "        \n",
        "    return representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojIkVWtg550G",
        "outputId": "0b384514-9a58-45a7-c6aa-09ee4213cd4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8176751592356688\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score de classification: 0.8157245222929935 (std 0.005599347443476836)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# Exemple avec les embeddings obtenus via Glove\n",
        "rep = sentence_representations(corpus, vocab_5k, GloveEmbeddings)\n",
        "clf = LogisticRegression().fit(rep[::2], y[::2])\n",
        "print(clf.score(rep[1::2], y[1::2]))\n",
        "\n",
        "scores = cross_val_score(clf, rep, y, cv=5)\n",
        "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCx7ojAi550G"
      },
      "source": [
        "Vous pouvez maintenant comparer l'ensemble des méthodes, et notamment répondre aux questions suivantes:\n",
        "- Pourquoi peut-on s'attendre à ce que les résultats obtenus avec les embeddings extraits des représentations pré-apprises avec Gl0ve soient bien meilleurs que les autres ? Quel serait le moyen de comparer de manière 'juste' Gl0ve avec les autres méthodes d'apprentissage de représentations ?\n",
        "- Quelle matrice permet d'obtenir les meilleures représentations via SVD ? (Co-occurences, Tf-Idf, PPMI ..)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTBUaiRS550G"
      },
      "source": [
        "### Réponses \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les résultats obtenus avec les embeddings extraits des représentations pré-entraînées avec GloVe peuvent être meilleurs que les autres méthodes pour plusieurs raisons :\n",
        "\n",
        "Pré-entraînement sur de grandes quantités de données : GloVe est pré-entraîné sur de vastes corpus textuels, ce qui lui permet de capturer des relations sémantiques et syntaxiques à partir d'une grande quantité de contextes différents. Cela lui donne une meilleure compréhension des mots et de leur signification.\n",
        "Utilisation de statistiques globales : GloVe utilise des statistiques globales de co-occurrence des mots dans le corpus d'entraînement pour construire les embeddings. Cela permet de prendre en compte les relations entre les mots à l'échelle du corpus, ce qui peut fournir des informations plus riches et significatives sur les relations lexicales.\n",
        "Transfert de connaissances : Les embeddings GloVe pré-entraînés capturent des connaissances générales sur la langue à partir du corpus d'entraînement initial. Ces connaissances peuvent être transférées vers d'autres tâches de traitement du langage naturel, ce qui peut améliorer les performances sur ces tâches spécifiques.\n",
        "\n",
        "\n",
        "En ce qui concerne la matrice permettant d'obtenir les meilleures représentations via SVD, la matrice de co-occurrence est souvent utilisée. La matrice de co-occurrence est construite en comptant les co-occurrences de mots dans un corpus. Cependant, d'autres matrices peuvent également être utilisées, telles que la matrice TF-IDF (Term Frequency-Inverse Document Frequency) ou la matrice PPMI (Positive Pointwise Mutual Information). Chaque matrice a ses avantages et ses inconvénients, et la meilleure matrice dépendra du contexte et de la tâche spécifique. Il est généralement recommandé d'expérimenter différentes matrices et de comparer leurs performances pour déterminer celle qui fonctionne le mieux dans un cas donné. dans notre cas c'est la matrice GloveEmbeddings."
      ],
      "metadata": {
        "id": "0E2hlDrq1i63"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}